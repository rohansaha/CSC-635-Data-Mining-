{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.47070092\n",
      "Iteration 2, loss = 3.07255713\n",
      "Iteration 3, loss = 2.77389770\n",
      "Iteration 4, loss = 2.53635830\n",
      "Iteration 5, loss = 2.34977815\n",
      "Iteration 6, loss = 2.19520145\n",
      "Iteration 7, loss = 2.06200432\n",
      "Iteration 8, loss = 1.94560349\n",
      "Iteration 9, loss = 1.83843881\n",
      "Iteration 10, loss = 1.74505104\n",
      "Iteration 11, loss = 1.66012686\n",
      "Iteration 12, loss = 1.58488179\n",
      "Iteration 13, loss = 1.51777380\n",
      "Iteration 14, loss = 1.45660898\n",
      "Iteration 15, loss = 1.40171292\n",
      "Iteration 16, loss = 1.35126409\n",
      "Iteration 17, loss = 1.30392777\n",
      "Iteration 18, loss = 1.26106636\n",
      "Iteration 19, loss = 1.21961611\n",
      "Iteration 20, loss = 1.18205770\n",
      "Iteration 21, loss = 1.14771612\n",
      "Iteration 22, loss = 1.11632740\n",
      "Iteration 23, loss = 1.08550680\n",
      "Iteration 24, loss = 1.05850925\n",
      "Iteration 25, loss = 1.03268388\n",
      "Iteration 26, loss = 1.00959685\n",
      "Iteration 27, loss = 0.98703056\n",
      "Iteration 28, loss = 0.96688663\n",
      "Iteration 29, loss = 0.94684739\n",
      "Iteration 30, loss = 0.92865023\n",
      "Iteration 31, loss = 0.91128626\n",
      "Iteration 32, loss = 0.89530070\n",
      "Iteration 33, loss = 0.87935770\n",
      "Iteration 34, loss = 0.86486396\n",
      "Iteration 35, loss = 0.85003939\n",
      "Iteration 36, loss = 0.83666178\n",
      "Iteration 37, loss = 0.82228700\n",
      "Iteration 38, loss = 0.80958663\n",
      "Iteration 39, loss = 0.79713852\n",
      "Iteration 40, loss = 0.78627929\n",
      "Iteration 41, loss = 0.77581367\n",
      "Iteration 42, loss = 0.76605064\n",
      "Iteration 43, loss = 0.75687583\n",
      "Iteration 44, loss = 0.74796427\n",
      "Iteration 45, loss = 0.73887245\n",
      "Iteration 46, loss = 0.73036094\n",
      "Iteration 47, loss = 0.72253387\n",
      "Iteration 48, loss = 0.71479588\n",
      "Iteration 49, loss = 0.70690960\n",
      "Iteration 50, loss = 0.70060928\n",
      "Iteration 51, loss = 0.69384244\n",
      "Iteration 52, loss = 0.68663034\n",
      "Iteration 53, loss = 0.68035057\n",
      "Iteration 54, loss = 0.67389097\n",
      "Iteration 55, loss = 0.66858590\n",
      "Iteration 56, loss = 0.66151499\n",
      "Iteration 57, loss = 0.65626428\n",
      "Iteration 58, loss = 0.65022202\n",
      "Iteration 59, loss = 0.64474059\n",
      "Iteration 60, loss = 0.63893216\n",
      "Iteration 61, loss = 0.63522169\n",
      "Iteration 62, loss = 0.62928953\n",
      "Iteration 63, loss = 0.62525309\n",
      "Iteration 64, loss = 0.62022920\n",
      "Iteration 65, loss = 0.61446164\n",
      "Iteration 66, loss = 0.61020583\n",
      "Iteration 67, loss = 0.60655151\n",
      "Iteration 68, loss = 0.60192365\n",
      "Iteration 69, loss = 0.59774531\n",
      "Iteration 70, loss = 0.59402960\n",
      "Iteration 71, loss = 0.59055605\n",
      "Iteration 72, loss = 0.58607006\n",
      "Iteration 73, loss = 0.58298022\n",
      "Iteration 74, loss = 0.57788954\n",
      "Iteration 75, loss = 0.57465698\n",
      "Iteration 76, loss = 0.57117397\n",
      "Iteration 77, loss = 0.56723016\n",
      "Iteration 78, loss = 0.56385572\n",
      "Iteration 79, loss = 0.56130494\n",
      "Iteration 80, loss = 0.55682225\n",
      "Iteration 81, loss = 0.55368896\n",
      "Iteration 82, loss = 0.55056536\n",
      "Iteration 83, loss = 0.54701795\n",
      "Iteration 84, loss = 0.54346393\n",
      "Iteration 85, loss = 0.54118511\n",
      "Iteration 86, loss = 0.53813699\n",
      "Iteration 87, loss = 0.53439353\n",
      "Iteration 88, loss = 0.53173442\n",
      "Iteration 89, loss = 0.52867452\n",
      "Iteration 90, loss = 0.52613821\n",
      "Iteration 91, loss = 0.52263998\n",
      "Iteration 92, loss = 0.52242746\n",
      "Iteration 93, loss = 0.51825354\n",
      "Iteration 94, loss = 0.51374304\n",
      "Iteration 95, loss = 0.51158755\n",
      "Iteration 96, loss = 0.50838907\n",
      "Iteration 97, loss = 0.50615340\n",
      "Iteration 98, loss = 0.50354426\n",
      "Iteration 99, loss = 0.50114635\n",
      "Iteration 100, loss = 0.49931964\n",
      "Iteration 101, loss = 0.49604468\n",
      "Iteration 102, loss = 0.49384605\n",
      "Iteration 103, loss = 0.49179343\n",
      "Iteration 104, loss = 0.48903713\n",
      "Iteration 105, loss = 0.48679674\n",
      "Iteration 106, loss = 0.48312862\n",
      "Iteration 107, loss = 0.48114105\n",
      "Iteration 108, loss = 0.47861817\n",
      "Iteration 109, loss = 0.47672293\n",
      "Iteration 110, loss = 0.47452906\n",
      "Iteration 111, loss = 0.47211586\n",
      "Iteration 112, loss = 0.47023342\n",
      "Iteration 113, loss = 0.46645704\n",
      "Iteration 114, loss = 0.46527017\n",
      "Iteration 115, loss = 0.46261101\n",
      "Iteration 116, loss = 0.46040564\n",
      "Iteration 117, loss = 0.45825082\n",
      "Iteration 118, loss = 0.45551530\n",
      "Iteration 119, loss = 0.45458422\n",
      "Iteration 120, loss = 0.45420211\n",
      "Iteration 121, loss = 0.44999588\n",
      "Iteration 122, loss = 0.44743197\n",
      "Iteration 123, loss = 0.44606015\n",
      "Iteration 124, loss = 0.44407169\n",
      "Iteration 125, loss = 0.44211578\n",
      "Iteration 126, loss = 0.43987842\n",
      "Iteration 127, loss = 0.43727945\n",
      "Iteration 128, loss = 0.43557325\n",
      "Iteration 129, loss = 0.43453922\n",
      "Iteration 130, loss = 0.43317867\n",
      "Iteration 131, loss = 0.42915620\n",
      "Iteration 132, loss = 0.42802561\n",
      "Iteration 133, loss = 0.42514787\n",
      "Iteration 134, loss = 0.42422512\n",
      "Iteration 135, loss = 0.42301505\n",
      "Iteration 136, loss = 0.42016966\n",
      "Iteration 137, loss = 0.42001030\n",
      "Iteration 138, loss = 0.41971048\n",
      "Iteration 139, loss = 0.41433075\n",
      "Iteration 140, loss = 0.41328929\n",
      "Iteration 141, loss = 0.41110446\n",
      "Iteration 142, loss = 0.40894116\n",
      "Iteration 143, loss = 0.40725802\n",
      "Iteration 144, loss = 0.40492839\n",
      "Iteration 145, loss = 0.40401585\n",
      "Iteration 146, loss = 0.40278393\n",
      "Iteration 147, loss = 0.40094561\n",
      "Iteration 148, loss = 0.39842907\n",
      "Iteration 149, loss = 0.39734756\n",
      "Iteration 150, loss = 0.39571452\n",
      "Iteration 151, loss = 0.39455980\n",
      "Iteration 152, loss = 0.39186645\n",
      "Iteration 153, loss = 0.39040645\n",
      "Iteration 154, loss = 0.38909586\n",
      "Iteration 155, loss = 0.38784586\n",
      "Iteration 156, loss = 0.38534032\n",
      "Iteration 157, loss = 0.38396827\n",
      "Iteration 158, loss = 0.38241360\n",
      "Iteration 159, loss = 0.38039279\n",
      "Iteration 160, loss = 0.37976560\n",
      "Iteration 161, loss = 0.37692329\n",
      "Iteration 162, loss = 0.37560478\n",
      "Iteration 163, loss = 0.37431856\n",
      "Iteration 164, loss = 0.37262967\n",
      "Iteration 165, loss = 0.37134170\n",
      "Iteration 166, loss = 0.36974051\n",
      "Iteration 167, loss = 0.36824736\n",
      "Iteration 168, loss = 0.36827613\n",
      "Iteration 169, loss = 0.36659904\n",
      "Iteration 170, loss = 0.36446315\n",
      "Iteration 171, loss = 0.36282705\n",
      "Iteration 172, loss = 0.36305067\n",
      "Iteration 173, loss = 0.36082232\n",
      "Iteration 174, loss = 0.35955286\n",
      "Iteration 175, loss = 0.35667775\n",
      "Iteration 176, loss = 0.35531775\n",
      "Iteration 177, loss = 0.35362947\n",
      "Iteration 178, loss = 0.35393706\n",
      "Iteration 179, loss = 0.35105472\n",
      "Iteration 180, loss = 0.35070745\n",
      "Iteration 181, loss = 0.34820352\n",
      "Iteration 182, loss = 0.34795820\n",
      "Iteration 183, loss = 0.34563237\n",
      "Iteration 184, loss = 0.34442576\n",
      "Iteration 185, loss = 0.34343649\n",
      "Iteration 186, loss = 0.34339262\n",
      "Iteration 187, loss = 0.34266831\n",
      "Iteration 188, loss = 0.34019270\n",
      "Iteration 189, loss = 0.34096009\n",
      "Iteration 190, loss = 0.33805597\n",
      "Iteration 191, loss = 0.33701069\n",
      "Iteration 192, loss = 0.33485133\n",
      "Iteration 193, loss = 0.33510151\n",
      "Iteration 194, loss = 0.33276986\n",
      "Iteration 195, loss = 0.33203771\n",
      "Iteration 196, loss = 0.33026917\n",
      "Iteration 197, loss = 0.32974255\n",
      "Iteration 198, loss = 0.32857304\n",
      "Iteration 199, loss = 0.32737722\n",
      "Iteration 200, loss = 0.32786805\n",
      "Iteration 201, loss = 0.32445947\n",
      "Iteration 202, loss = 0.32462028\n",
      "Iteration 203, loss = 0.32506444\n",
      "Iteration 204, loss = 0.32315018\n",
      "Iteration 205, loss = 0.32122929\n",
      "Iteration 206, loss = 0.32084007\n",
      "Iteration 207, loss = 0.32075923\n",
      "Iteration 208, loss = 0.32093465\n",
      "Iteration 209, loss = 0.31806697\n",
      "Iteration 210, loss = 0.31802066\n",
      "Iteration 211, loss = 0.31536976\n",
      "Iteration 212, loss = 0.31541017\n",
      "Iteration 213, loss = 0.31348592\n",
      "Iteration 214, loss = 0.31290605\n",
      "Iteration 215, loss = 0.31215407\n",
      "Iteration 216, loss = 0.31165617\n",
      "Iteration 217, loss = 0.31023508\n",
      "Iteration 218, loss = 0.31201504\n",
      "Iteration 219, loss = 0.31043138\n",
      "Iteration 220, loss = 0.30784665\n",
      "Iteration 221, loss = 0.30629057\n",
      "Iteration 222, loss = 0.30610762\n",
      "Iteration 223, loss = 0.30547908\n",
      "Iteration 224, loss = 0.30444074\n",
      "Iteration 225, loss = 0.30371680\n",
      "Iteration 226, loss = 0.30359262\n",
      "Iteration 227, loss = 0.30112665\n",
      "Iteration 228, loss = 0.30102739\n",
      "Iteration 229, loss = 0.30009619\n",
      "Iteration 230, loss = 0.30033235\n",
      "Iteration 231, loss = 0.29936581\n",
      "Iteration 232, loss = 0.29971548\n",
      "Iteration 233, loss = 0.29770974\n",
      "Iteration 234, loss = 0.29808277\n",
      "Iteration 235, loss = 0.29599780\n",
      "Iteration 236, loss = 0.29508489\n",
      "Iteration 237, loss = 0.29366480\n",
      "Iteration 238, loss = 0.29306573\n",
      "Iteration 239, loss = 0.29328332\n",
      "Iteration 240, loss = 0.29215816\n",
      "Iteration 241, loss = 0.29095294\n",
      "Iteration 242, loss = 0.29116527\n",
      "Iteration 243, loss = 0.29074268\n",
      "Iteration 244, loss = 0.28968788\n",
      "Iteration 245, loss = 0.28906889\n",
      "Iteration 246, loss = 0.28849624\n",
      "Iteration 247, loss = 0.28732401\n",
      "Iteration 248, loss = 0.28647163\n",
      "Iteration 249, loss = 0.28630287\n",
      "Iteration 250, loss = 0.28553400\n",
      "Iteration 251, loss = 0.28574997\n",
      "Iteration 252, loss = 0.28333575\n",
      "Iteration 253, loss = 0.28396056\n",
      "Iteration 254, loss = 0.28347603\n",
      "Iteration 255, loss = 0.28239456\n",
      "Iteration 256, loss = 0.28231876\n",
      "Iteration 257, loss = 0.28143971\n",
      "Iteration 258, loss = 0.28205557\n",
      "Iteration 259, loss = 0.28255659\n",
      "Iteration 260, loss = 0.28047495\n",
      "Iteration 261, loss = 0.27868013\n",
      "Iteration 262, loss = 0.27957197\n",
      "Iteration 263, loss = 0.27924964\n",
      "Iteration 264, loss = 0.27783103\n",
      "Iteration 265, loss = 0.27804888\n",
      "Iteration 266, loss = 0.27645222\n",
      "Iteration 267, loss = 0.27613322\n",
      "Iteration 268, loss = 0.27555370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 269, loss = 0.27434728\n",
      "Iteration 270, loss = 0.27475037\n",
      "Iteration 271, loss = 0.27374453\n",
      "Iteration 272, loss = 0.27370833\n",
      "Iteration 273, loss = 0.27352503\n",
      "Iteration 274, loss = 0.27387169\n",
      "Iteration 275, loss = 0.27352214\n",
      "Iteration 276, loss = 0.27269694\n",
      "Iteration 277, loss = 0.26985969\n",
      "Iteration 278, loss = 0.27023665\n",
      "Iteration 279, loss = 0.26979363\n",
      "Iteration 280, loss = 0.26952811\n",
      "Iteration 281, loss = 0.26954034\n",
      "Iteration 282, loss = 0.26930847\n",
      "Iteration 283, loss = 0.26792208\n",
      "Iteration 284, loss = 0.26784389\n",
      "Iteration 285, loss = 0.26654467\n",
      "Iteration 286, loss = 0.26896686\n",
      "Iteration 287, loss = 0.26728189\n",
      "Iteration 288, loss = 0.26605764\n",
      "Iteration 289, loss = 0.26567666\n",
      "Iteration 290, loss = 0.26523633\n",
      "Iteration 291, loss = 0.26354797\n",
      "Iteration 292, loss = 0.26398814\n",
      "Iteration 293, loss = 0.26357829\n",
      "Iteration 294, loss = 0.26208594\n",
      "Iteration 295, loss = 0.26318455\n",
      "Iteration 296, loss = 0.26236039\n",
      "Iteration 297, loss = 0.26115519\n",
      "Iteration 298, loss = 0.26333265\n",
      "Iteration 299, loss = 0.26176510\n",
      "Iteration 300, loss = 0.26052798\n",
      "Iteration 301, loss = 0.25924873\n",
      "Iteration 302, loss = 0.25931182\n",
      "Iteration 303, loss = 0.26059196\n",
      "Iteration 304, loss = 0.25836092\n",
      "Iteration 305, loss = 0.25775274\n",
      "Iteration 306, loss = 0.25705841\n",
      "Iteration 307, loss = 0.25699881\n",
      "Iteration 308, loss = 0.25685213\n",
      "Iteration 309, loss = 0.25649847\n",
      "Iteration 310, loss = 0.25507281\n",
      "Iteration 311, loss = 0.25485703\n",
      "Iteration 312, loss = 0.25549650\n",
      "Iteration 313, loss = 0.25459698\n",
      "Iteration 314, loss = 0.25539304\n",
      "Iteration 315, loss = 0.25511056\n",
      "Iteration 316, loss = 0.25260055\n",
      "Iteration 317, loss = 0.25364106\n",
      "Iteration 318, loss = 0.25253588\n",
      "Iteration 319, loss = 0.25383306\n",
      "Iteration 320, loss = 0.25366395\n",
      "Iteration 321, loss = 0.25355341\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Attribute -> # of Divisors\n",
      "Accuracy -> 0.912109375\n",
      "Iteration 1, loss = 1.35870306\n",
      "Iteration 2, loss = 1.09344446\n",
      "Iteration 3, loss = 1.00463111\n",
      "Iteration 4, loss = 0.95370529\n",
      "Iteration 5, loss = 0.91041605\n",
      "Iteration 6, loss = 0.87097396\n",
      "Iteration 7, loss = 0.83557205\n",
      "Iteration 8, loss = 0.80471717\n",
      "Iteration 9, loss = 0.77747071\n",
      "Iteration 10, loss = 0.75201158\n",
      "Iteration 11, loss = 0.72795964\n",
      "Iteration 12, loss = 0.70642563\n",
      "Iteration 13, loss = 0.68561003\n",
      "Iteration 14, loss = 0.66691760\n",
      "Iteration 15, loss = 0.64916128\n",
      "Iteration 16, loss = 0.63232744\n",
      "Iteration 17, loss = 0.61689143\n",
      "Iteration 18, loss = 0.60118076\n",
      "Iteration 19, loss = 0.58732090\n",
      "Iteration 20, loss = 0.57351138\n",
      "Iteration 21, loss = 0.56133275\n",
      "Iteration 22, loss = 0.54993999\n",
      "Iteration 23, loss = 0.53843352\n",
      "Iteration 24, loss = 0.52807312\n",
      "Iteration 25, loss = 0.51761502\n",
      "Iteration 26, loss = 0.50698260\n",
      "Iteration 27, loss = 0.49833413\n",
      "Iteration 28, loss = 0.48878638\n",
      "Iteration 29, loss = 0.48015005\n",
      "Iteration 30, loss = 0.47226179\n",
      "Iteration 31, loss = 0.46478629\n",
      "Iteration 32, loss = 0.45811748\n",
      "Iteration 33, loss = 0.45084119\n",
      "Iteration 34, loss = 0.44500575\n",
      "Iteration 35, loss = 0.43936051\n",
      "Iteration 36, loss = 0.43330987\n",
      "Iteration 37, loss = 0.42713140\n",
      "Iteration 38, loss = 0.42383210\n",
      "Iteration 39, loss = 0.41756753\n",
      "Iteration 40, loss = 0.41159397\n",
      "Iteration 41, loss = 0.40786762\n",
      "Iteration 42, loss = 0.40402534\n",
      "Iteration 43, loss = 0.39804990\n",
      "Iteration 44, loss = 0.39485818\n",
      "Iteration 45, loss = 0.38919981\n",
      "Iteration 46, loss = 0.38511661\n",
      "Iteration 47, loss = 0.38100887\n",
      "Iteration 48, loss = 0.37731818\n",
      "Iteration 49, loss = 0.37349366\n",
      "Iteration 50, loss = 0.37006930\n",
      "Iteration 51, loss = 0.36618644\n",
      "Iteration 52, loss = 0.36334187\n",
      "Iteration 53, loss = 0.35875231\n",
      "Iteration 54, loss = 0.35528092\n",
      "Iteration 55, loss = 0.35145148\n",
      "Iteration 56, loss = 0.34827398\n",
      "Iteration 57, loss = 0.34592127\n",
      "Iteration 58, loss = 0.34204906\n",
      "Iteration 59, loss = 0.33937754\n",
      "Iteration 60, loss = 0.33564814\n",
      "Iteration 61, loss = 0.33251370\n",
      "Iteration 62, loss = 0.32934809\n",
      "Iteration 63, loss = 0.32690449\n",
      "Iteration 64, loss = 0.32471643\n",
      "Iteration 65, loss = 0.32093689\n",
      "Iteration 66, loss = 0.31742932\n",
      "Iteration 67, loss = 0.31522293\n",
      "Iteration 68, loss = 0.31129039\n",
      "Iteration 69, loss = 0.31024770\n",
      "Iteration 70, loss = 0.30694799\n",
      "Iteration 71, loss = 0.30573971\n",
      "Iteration 72, loss = 0.30050025\n",
      "Iteration 73, loss = 0.29873935\n",
      "Iteration 74, loss = 0.29440497\n",
      "Iteration 75, loss = 0.29263097\n",
      "Iteration 76, loss = 0.28978896\n",
      "Iteration 77, loss = 0.28780103\n",
      "Iteration 78, loss = 0.28439514\n",
      "Iteration 79, loss = 0.28075359\n",
      "Iteration 80, loss = 0.27867129\n",
      "Iteration 81, loss = 0.27723653\n",
      "Iteration 82, loss = 0.27325717\n",
      "Iteration 83, loss = 0.27075090\n",
      "Iteration 84, loss = 0.26977461\n",
      "Iteration 85, loss = 0.26628831\n",
      "Iteration 86, loss = 0.26314947\n",
      "Iteration 87, loss = 0.26144382\n",
      "Iteration 88, loss = 0.25846553\n",
      "Iteration 89, loss = 0.25671171\n",
      "Iteration 90, loss = 0.25506149\n",
      "Iteration 91, loss = 0.25140670\n",
      "Iteration 92, loss = 0.24987783\n",
      "Iteration 93, loss = 0.24714451\n",
      "Iteration 94, loss = 0.24440718\n",
      "Iteration 95, loss = 0.24237573\n",
      "Iteration 96, loss = 0.23994704\n",
      "Iteration 97, loss = 0.23852802\n",
      "Iteration 98, loss = 0.23629814\n",
      "Iteration 99, loss = 0.23429739\n",
      "Iteration 100, loss = 0.23155521\n",
      "Iteration 101, loss = 0.23025842\n",
      "Iteration 102, loss = 0.22863321\n",
      "Iteration 103, loss = 0.22543684\n",
      "Iteration 104, loss = 0.22318042\n",
      "Iteration 105, loss = 0.22408637\n",
      "Iteration 106, loss = 0.21916003\n",
      "Iteration 107, loss = 0.21784823\n",
      "Iteration 108, loss = 0.21536546\n",
      "Iteration 109, loss = 0.21327787\n",
      "Iteration 110, loss = 0.21101092\n",
      "Iteration 111, loss = 0.20930657\n",
      "Iteration 112, loss = 0.20787649\n",
      "Iteration 113, loss = 0.20582812\n",
      "Iteration 114, loss = 0.20360691\n",
      "Iteration 115, loss = 0.20165464\n",
      "Iteration 116, loss = 0.20039760\n",
      "Iteration 117, loss = 0.19767153\n",
      "Iteration 118, loss = 0.19647397\n",
      "Iteration 119, loss = 0.19455853\n",
      "Iteration 120, loss = 0.19287373\n",
      "Iteration 121, loss = 0.19167846\n",
      "Iteration 122, loss = 0.18954455\n",
      "Iteration 123, loss = 0.18781184\n",
      "Iteration 124, loss = 0.18526191\n",
      "Iteration 125, loss = 0.18393389\n",
      "Iteration 126, loss = 0.18350304\n",
      "Iteration 127, loss = 0.18043334\n",
      "Iteration 128, loss = 0.17889704\n",
      "Iteration 129, loss = 0.17749390\n",
      "Iteration 130, loss = 0.17728828\n",
      "Iteration 131, loss = 0.17397547\n",
      "Iteration 132, loss = 0.17218622\n",
      "Iteration 133, loss = 0.17069779\n",
      "Iteration 134, loss = 0.16956910\n",
      "Iteration 135, loss = 0.16785650\n",
      "Iteration 136, loss = 0.16715325\n",
      "Iteration 137, loss = 0.16488923\n",
      "Iteration 138, loss = 0.16430874\n",
      "Iteration 139, loss = 0.16292530\n",
      "Iteration 140, loss = 0.16045027\n",
      "Iteration 141, loss = 0.15891027\n",
      "Iteration 142, loss = 0.15832372\n",
      "Iteration 143, loss = 0.15648342\n",
      "Iteration 144, loss = 0.15468503\n",
      "Iteration 145, loss = 0.15371420\n",
      "Iteration 146, loss = 0.15220365\n",
      "Iteration 147, loss = 0.15113386\n",
      "Iteration 148, loss = 0.14949703\n",
      "Iteration 149, loss = 0.14800179\n",
      "Iteration 150, loss = 0.14694677\n",
      "Iteration 151, loss = 0.14728945\n",
      "Iteration 152, loss = 0.14533944\n",
      "Iteration 153, loss = 0.14361981\n",
      "Iteration 154, loss = 0.14228826\n",
      "Iteration 155, loss = 0.14120998\n",
      "Iteration 156, loss = 0.14019080\n",
      "Iteration 157, loss = 0.13778223\n",
      "Iteration 158, loss = 0.13698166\n",
      "Iteration 159, loss = 0.13579816\n",
      "Iteration 160, loss = 0.13463194\n",
      "Iteration 161, loss = 0.13305669\n",
      "Iteration 162, loss = 0.13148157\n",
      "Iteration 163, loss = 0.13093806\n",
      "Iteration 164, loss = 0.13073399\n",
      "Iteration 165, loss = 0.12874161\n",
      "Iteration 166, loss = 0.12755260\n",
      "Iteration 167, loss = 0.12589510\n",
      "Iteration 168, loss = 0.12493892\n",
      "Iteration 169, loss = 0.12381191\n",
      "Iteration 170, loss = 0.12234980\n",
      "Iteration 171, loss = 0.12124709\n",
      "Iteration 172, loss = 0.12068450\n",
      "Iteration 173, loss = 0.11934565\n",
      "Iteration 174, loss = 0.11815762\n",
      "Iteration 175, loss = 0.11771665\n",
      "Iteration 176, loss = 0.11612407\n",
      "Iteration 177, loss = 0.11453940\n",
      "Iteration 178, loss = 0.11381886\n",
      "Iteration 179, loss = 0.11359407\n",
      "Iteration 180, loss = 0.11265123\n",
      "Iteration 181, loss = 0.11076221\n",
      "Iteration 182, loss = 0.10987983\n",
      "Iteration 183, loss = 0.10900307\n",
      "Iteration 184, loss = 0.10815572\n",
      "Iteration 185, loss = 0.10696381\n",
      "Iteration 186, loss = 0.10578184\n",
      "Iteration 187, loss = 0.10503800\n",
      "Iteration 188, loss = 0.10451986\n",
      "Iteration 189, loss = 0.10308636\n",
      "Iteration 190, loss = 0.10147945\n",
      "Iteration 191, loss = 0.10457815\n",
      "Iteration 192, loss = 0.10006984\n",
      "Iteration 193, loss = 0.10007042\n",
      "Iteration 194, loss = 0.09783300\n",
      "Iteration 195, loss = 0.09686104\n",
      "Iteration 196, loss = 0.09635944\n",
      "Iteration 197, loss = 0.09554175\n",
      "Iteration 198, loss = 0.09454277\n",
      "Iteration 199, loss = 0.09355397\n",
      "Iteration 200, loss = 0.09589326\n",
      "Iteration 201, loss = 0.09316805\n",
      "Iteration 202, loss = 0.09219550\n",
      "Iteration 203, loss = 0.09033864\n",
      "Iteration 204, loss = 0.09070712\n",
      "Iteration 205, loss = 0.08848095\n",
      "Iteration 206, loss = 0.08804479\n",
      "Iteration 207, loss = 0.08741953\n",
      "Iteration 208, loss = 0.08590821\n",
      "Iteration 209, loss = 0.08483012\n",
      "Iteration 210, loss = 0.08408706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 211, loss = 0.08411966\n",
      "Iteration 212, loss = 0.08217945\n",
      "Iteration 213, loss = 0.08316234\n",
      "Iteration 214, loss = 0.08157155\n",
      "Iteration 215, loss = 0.08055680\n",
      "Iteration 216, loss = 0.07986802\n",
      "Iteration 217, loss = 0.07984590\n",
      "Iteration 218, loss = 0.07868316\n",
      "Iteration 219, loss = 0.07815934\n",
      "Iteration 220, loss = 0.07724834\n",
      "Iteration 221, loss = 0.07634243\n",
      "Iteration 222, loss = 0.07586549\n",
      "Iteration 223, loss = 0.07502648\n",
      "Iteration 224, loss = 0.07408778\n",
      "Iteration 225, loss = 0.07334558\n",
      "Iteration 226, loss = 0.07330072\n",
      "Iteration 227, loss = 0.07246893\n",
      "Iteration 228, loss = 0.07175626\n",
      "Iteration 229, loss = 0.07129214\n",
      "Iteration 230, loss = 0.07196932\n",
      "Iteration 231, loss = 0.07018330\n",
      "Iteration 232, loss = 0.06979895\n",
      "Iteration 233, loss = 0.06927625\n",
      "Iteration 234, loss = 0.06804528\n",
      "Iteration 235, loss = 0.06802817\n",
      "Iteration 236, loss = 0.06700767\n",
      "Iteration 237, loss = 0.06624043\n",
      "Iteration 238, loss = 0.06620532\n",
      "Iteration 239, loss = 0.06577676\n",
      "Iteration 240, loss = 0.06485902\n",
      "Iteration 241, loss = 0.06430912\n",
      "Iteration 242, loss = 0.06403721\n",
      "Iteration 243, loss = 0.06332433\n",
      "Iteration 244, loss = 0.06261197\n",
      "Iteration 245, loss = 0.06256597\n",
      "Iteration 246, loss = 0.06186870\n",
      "Iteration 247, loss = 0.06160527\n",
      "Iteration 248, loss = 0.06056959\n",
      "Iteration 249, loss = 0.06010231\n",
      "Iteration 250, loss = 0.06011143\n",
      "Iteration 251, loss = 0.05949842\n",
      "Iteration 252, loss = 0.05890920\n",
      "Iteration 253, loss = 0.05841257\n",
      "Iteration 254, loss = 0.05782353\n",
      "Iteration 255, loss = 0.05836186\n",
      "Iteration 256, loss = 0.05726008\n",
      "Iteration 257, loss = 0.05588781\n",
      "Iteration 258, loss = 0.05626123\n",
      "Iteration 259, loss = 0.05763820\n",
      "Iteration 260, loss = 0.05636550\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Attribute -> # of Factors\n",
      "Accuracy -> 0.998046875\n",
      "Iteration 1, loss = 0.49654102\n",
      "Iteration 2, loss = 0.38921874\n",
      "Iteration 3, loss = 0.33457004\n",
      "Iteration 4, loss = 0.31074389\n",
      "Iteration 5, loss = 0.29452660\n",
      "Iteration 6, loss = 0.28020598\n",
      "Iteration 7, loss = 0.26706396\n",
      "Iteration 8, loss = 0.25506475\n",
      "Iteration 9, loss = 0.24369342\n",
      "Iteration 10, loss = 0.23279333\n",
      "Iteration 11, loss = 0.22237800\n",
      "Iteration 12, loss = 0.21235402\n",
      "Iteration 13, loss = 0.20284723\n",
      "Iteration 14, loss = 0.19354315\n",
      "Iteration 15, loss = 0.18462495\n",
      "Iteration 16, loss = 0.17612354\n",
      "Iteration 17, loss = 0.16808891\n",
      "Iteration 18, loss = 0.16020366\n",
      "Iteration 19, loss = 0.15258912\n",
      "Iteration 20, loss = 0.14543555\n",
      "Iteration 21, loss = 0.13856313\n",
      "Iteration 22, loss = 0.13194157\n",
      "Iteration 23, loss = 0.12574162\n",
      "Iteration 24, loss = 0.11981465\n",
      "Iteration 25, loss = 0.11430425\n",
      "Iteration 26, loss = 0.10892701\n",
      "Iteration 27, loss = 0.10406140\n",
      "Iteration 28, loss = 0.09937794\n",
      "Iteration 29, loss = 0.09530179\n",
      "Iteration 30, loss = 0.09117053\n",
      "Iteration 31, loss = 0.08751465\n",
      "Iteration 32, loss = 0.08407395\n",
      "Iteration 33, loss = 0.08066574\n",
      "Iteration 34, loss = 0.07776014\n",
      "Iteration 35, loss = 0.07485698\n",
      "Iteration 36, loss = 0.07229581\n",
      "Iteration 37, loss = 0.06974665\n",
      "Iteration 38, loss = 0.06748211\n",
      "Iteration 39, loss = 0.06541172\n",
      "Iteration 40, loss = 0.06340861\n",
      "Iteration 41, loss = 0.06157929\n",
      "Iteration 42, loss = 0.05982527\n",
      "Iteration 43, loss = 0.05828431\n",
      "Iteration 44, loss = 0.05670828\n",
      "Iteration 45, loss = 0.05534096\n",
      "Iteration 46, loss = 0.05402895\n",
      "Iteration 47, loss = 0.05278758\n",
      "Iteration 48, loss = 0.05167643\n",
      "Iteration 49, loss = 0.05053999\n",
      "Iteration 50, loss = 0.04954853\n",
      "Iteration 51, loss = 0.04856823\n",
      "Iteration 52, loss = 0.04766084\n",
      "Iteration 53, loss = 0.04678630\n",
      "Iteration 54, loss = 0.04599752\n",
      "Iteration 55, loss = 0.04523744\n",
      "Iteration 56, loss = 0.04451499\n",
      "Iteration 57, loss = 0.04383961\n",
      "Iteration 58, loss = 0.04318198\n",
      "Iteration 59, loss = 0.04257032\n",
      "Iteration 60, loss = 0.04202054\n",
      "Iteration 61, loss = 0.04144968\n",
      "Iteration 62, loss = 0.04092642\n",
      "Iteration 63, loss = 0.04044720\n",
      "Iteration 64, loss = 0.03997816\n",
      "Iteration 65, loss = 0.03954632\n",
      "Iteration 66, loss = 0.03911950\n",
      "Iteration 67, loss = 0.03870627\n",
      "Iteration 68, loss = 0.03834915\n",
      "Iteration 69, loss = 0.03795965\n",
      "Iteration 70, loss = 0.03760098\n",
      "Iteration 71, loss = 0.03726957\n",
      "Iteration 72, loss = 0.03695217\n",
      "Iteration 73, loss = 0.03665405\n",
      "Iteration 74, loss = 0.03635074\n",
      "Iteration 75, loss = 0.03605884\n",
      "Iteration 76, loss = 0.03580099\n",
      "Iteration 77, loss = 0.03553772\n",
      "Iteration 78, loss = 0.03527637\n",
      "Iteration 79, loss = 0.03503909\n",
      "Iteration 80, loss = 0.03480622\n",
      "Iteration 81, loss = 0.03458085\n",
      "Iteration 82, loss = 0.03437020\n",
      "Iteration 83, loss = 0.03415896\n",
      "Iteration 84, loss = 0.03395976\n",
      "Iteration 85, loss = 0.03377562\n",
      "Iteration 86, loss = 0.03358640\n",
      "Iteration 87, loss = 0.03339590\n",
      "Iteration 88, loss = 0.03323229\n",
      "Iteration 89, loss = 0.03306967\n",
      "Iteration 90, loss = 0.03289940\n",
      "Iteration 91, loss = 0.03273603\n",
      "Iteration 92, loss = 0.03257481\n",
      "Iteration 93, loss = 0.03243144\n",
      "Iteration 94, loss = 0.03228364\n",
      "Iteration 95, loss = 0.03213768\n",
      "Iteration 96, loss = 0.03200154\n",
      "Iteration 97, loss = 0.03186567\n",
      "Iteration 98, loss = 0.03174181\n",
      "Iteration 99, loss = 0.03160913\n",
      "Iteration 100, loss = 0.03149045\n",
      "Iteration 101, loss = 0.03136004\n",
      "Iteration 102, loss = 0.03124937\n",
      "Iteration 103, loss = 0.03114078\n",
      "Iteration 104, loss = 0.03103593\n",
      "Iteration 105, loss = 0.03090619\n",
      "Iteration 106, loss = 0.03080290\n",
      "Iteration 107, loss = 0.03069282\n",
      "Iteration 108, loss = 0.03060202\n",
      "Iteration 109, loss = 0.03049468\n",
      "Iteration 110, loss = 0.03040313\n",
      "Iteration 111, loss = 0.03030461\n",
      "Iteration 112, loss = 0.03021944\n",
      "Iteration 113, loss = 0.03012036\n",
      "Iteration 114, loss = 0.03002435\n",
      "Iteration 115, loss = 0.02994217\n",
      "Iteration 116, loss = 0.02985189\n",
      "Iteration 117, loss = 0.02976730\n",
      "Iteration 118, loss = 0.02969147\n",
      "Iteration 119, loss = 0.02961960\n",
      "Iteration 120, loss = 0.02951468\n",
      "Iteration 121, loss = 0.02943526\n",
      "Iteration 122, loss = 0.02939015\n",
      "Iteration 123, loss = 0.02927620\n",
      "Iteration 124, loss = 0.02919475\n",
      "Iteration 125, loss = 0.02911592\n",
      "Iteration 126, loss = 0.02903971\n",
      "Iteration 127, loss = 0.02898289\n",
      "Iteration 128, loss = 0.02889179\n",
      "Iteration 129, loss = 0.02883473\n",
      "Iteration 130, loss = 0.02875666\n",
      "Iteration 131, loss = 0.02869057\n",
      "Iteration 132, loss = 0.02863223\n",
      "Iteration 133, loss = 0.02854066\n",
      "Iteration 134, loss = 0.02849314\n",
      "Iteration 135, loss = 0.02840296\n",
      "Iteration 136, loss = 0.02834612\n",
      "Iteration 137, loss = 0.02827407\n",
      "Iteration 138, loss = 0.02823280\n",
      "Iteration 139, loss = 0.02816016\n",
      "Iteration 140, loss = 0.02808993\n",
      "Iteration 141, loss = 0.02802562\n",
      "Iteration 142, loss = 0.02797452\n",
      "Iteration 143, loss = 0.02790621\n",
      "Iteration 144, loss = 0.02784278\n",
      "Iteration 145, loss = 0.02778495\n",
      "Iteration 146, loss = 0.02773913\n",
      "Iteration 147, loss = 0.02767542\n",
      "Iteration 148, loss = 0.02761899\n",
      "Iteration 149, loss = 0.02756643\n",
      "Iteration 150, loss = 0.02750901\n",
      "Iteration 151, loss = 0.02745390\n",
      "Iteration 152, loss = 0.02740064\n",
      "Iteration 153, loss = 0.02734000\n",
      "Iteration 154, loss = 0.02729636\n",
      "Iteration 155, loss = 0.02724405\n",
      "Iteration 156, loss = 0.02718324\n",
      "Iteration 157, loss = 0.02714068\n",
      "Iteration 158, loss = 0.02708202\n",
      "Iteration 159, loss = 0.02702994\n",
      "Iteration 160, loss = 0.02698463\n",
      "Iteration 161, loss = 0.02692574\n",
      "Iteration 162, loss = 0.02689562\n",
      "Iteration 163, loss = 0.02682603\n",
      "Iteration 164, loss = 0.02678831\n",
      "Iteration 165, loss = 0.02673024\n",
      "Iteration 166, loss = 0.02668256\n",
      "Iteration 167, loss = 0.02664296\n",
      "Iteration 168, loss = 0.02659024\n",
      "Iteration 169, loss = 0.02653934\n",
      "Iteration 170, loss = 0.02650761\n",
      "Iteration 171, loss = 0.02645021\n",
      "Iteration 172, loss = 0.02639770\n",
      "Iteration 173, loss = 0.02635265\n",
      "Iteration 174, loss = 0.02631217\n",
      "Iteration 175, loss = 0.02627152\n",
      "Iteration 176, loss = 0.02622787\n",
      "Iteration 177, loss = 0.02617096\n",
      "Iteration 178, loss = 0.02612865\n",
      "Iteration 179, loss = 0.02608529\n",
      "Iteration 180, loss = 0.02605848\n",
      "Iteration 181, loss = 0.02599666\n",
      "Iteration 182, loss = 0.02595536\n",
      "Iteration 183, loss = 0.02590946\n",
      "Iteration 184, loss = 0.02586968\n",
      "Iteration 185, loss = 0.02583809\n",
      "Iteration 186, loss = 0.02579047\n",
      "Iteration 187, loss = 0.02574913\n",
      "Iteration 188, loss = 0.02569925\n",
      "Iteration 189, loss = 0.02566079\n",
      "Iteration 190, loss = 0.02561576\n",
      "Iteration 191, loss = 0.02558458\n",
      "Iteration 192, loss = 0.02553757\n",
      "Iteration 193, loss = 0.02549509\n",
      "Iteration 194, loss = 0.02546583\n",
      "Iteration 195, loss = 0.02541427\n",
      "Iteration 196, loss = 0.02540268\n",
      "Iteration 197, loss = 0.02533351\n",
      "Iteration 198, loss = 0.02529398\n",
      "Iteration 199, loss = 0.02525901\n",
      "Iteration 200, loss = 0.02521422\n",
      "Iteration 201, loss = 0.02517139\n",
      "Iteration 202, loss = 0.02513235\n",
      "Iteration 203, loss = 0.02509596\n",
      "Iteration 204, loss = 0.02506065\n",
      "Iteration 205, loss = 0.02501361\n",
      "Iteration 206, loss = 0.02498852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 207, loss = 0.02494078\n",
      "Iteration 208, loss = 0.02490254\n",
      "Iteration 209, loss = 0.02486634\n",
      "Iteration 210, loss = 0.02482729\n",
      "Iteration 211, loss = 0.02479577\n",
      "Iteration 212, loss = 0.02475170\n",
      "Iteration 213, loss = 0.02471393\n",
      "Iteration 214, loss = 0.02469802\n",
      "Iteration 215, loss = 0.02463875\n",
      "Iteration 216, loss = 0.02460334\n",
      "Iteration 217, loss = 0.02456309\n",
      "Iteration 218, loss = 0.02454017\n",
      "Iteration 219, loss = 0.02449143\n",
      "Iteration 220, loss = 0.02445488\n",
      "Iteration 221, loss = 0.02441683\n",
      "Iteration 222, loss = 0.02438219\n",
      "Iteration 223, loss = 0.02433962\n",
      "Iteration 224, loss = 0.02431636\n",
      "Iteration 225, loss = 0.02427149\n",
      "Iteration 226, loss = 0.02423818\n",
      "Iteration 227, loss = 0.02419575\n",
      "Iteration 228, loss = 0.02415699\n",
      "Iteration 229, loss = 0.02412608\n",
      "Iteration 230, loss = 0.02409153\n",
      "Iteration 231, loss = 0.02405001\n",
      "Iteration 232, loss = 0.02401728\n",
      "Iteration 233, loss = 0.02398653\n",
      "Iteration 234, loss = 0.02394539\n",
      "Iteration 235, loss = 0.02392579\n",
      "Iteration 236, loss = 0.02388031\n",
      "Iteration 237, loss = 0.02385606\n",
      "Iteration 238, loss = 0.02381326\n",
      "Iteration 239, loss = 0.02378028\n",
      "Iteration 240, loss = 0.02374754\n",
      "Iteration 241, loss = 0.02371168\n",
      "Iteration 242, loss = 0.02368428\n",
      "Iteration 243, loss = 0.02365328\n",
      "Iteration 244, loss = 0.02361408\n",
      "Iteration 245, loss = 0.02358119\n",
      "Iteration 246, loss = 0.02354241\n",
      "Iteration 247, loss = 0.02351136\n",
      "Iteration 248, loss = 0.02348233\n",
      "Iteration 249, loss = 0.02345088\n",
      "Iteration 250, loss = 0.02341446\n",
      "Iteration 251, loss = 0.02338793\n",
      "Iteration 252, loss = 0.02334838\n",
      "Iteration 253, loss = 0.02332104\n",
      "Iteration 254, loss = 0.02328262\n",
      "Iteration 255, loss = 0.02324639\n",
      "Iteration 256, loss = 0.02321294\n",
      "Iteration 257, loss = 0.02318388\n",
      "Iteration 258, loss = 0.02315184\n",
      "Iteration 259, loss = 0.02311616\n",
      "Iteration 260, loss = 0.02308402\n",
      "Iteration 261, loss = 0.02305085\n",
      "Iteration 262, loss = 0.02302037\n",
      "Iteration 263, loss = 0.02300459\n",
      "Iteration 264, loss = 0.02296866\n",
      "Iteration 265, loss = 0.02292973\n",
      "Iteration 266, loss = 0.02290720\n",
      "Iteration 267, loss = 0.02287323\n",
      "Iteration 268, loss = 0.02284038\n",
      "Iteration 269, loss = 0.02281262\n",
      "Iteration 270, loss = 0.02278198\n",
      "Iteration 271, loss = 0.02274730\n",
      "Iteration 272, loss = 0.02272286\n",
      "Iteration 273, loss = 0.02270943\n",
      "Iteration 274, loss = 0.02266759\n",
      "Iteration 275, loss = 0.02263434\n",
      "Iteration 276, loss = 0.02261161\n",
      "Iteration 277, loss = 0.02258602\n",
      "Iteration 278, loss = 0.02256073\n",
      "Iteration 279, loss = 0.02253734\n",
      "Iteration 280, loss = 0.02249495\n",
      "Iteration 281, loss = 0.02247553\n",
      "Iteration 282, loss = 0.02243390\n",
      "Iteration 283, loss = 0.02241998\n",
      "Iteration 284, loss = 0.02238607\n",
      "Iteration 285, loss = 0.02234640\n",
      "Iteration 286, loss = 0.02231920\n",
      "Iteration 287, loss = 0.02230207\n",
      "Iteration 288, loss = 0.02226470\n",
      "Iteration 289, loss = 0.02224128\n",
      "Iteration 290, loss = 0.02220506\n",
      "Iteration 291, loss = 0.02218611\n",
      "Iteration 292, loss = 0.02215044\n",
      "Iteration 293, loss = 0.02213096\n",
      "Iteration 294, loss = 0.02209830\n",
      "Iteration 295, loss = 0.02207961\n",
      "Iteration 296, loss = 0.02204921\n",
      "Iteration 297, loss = 0.02201414\n",
      "Iteration 298, loss = 0.02199737\n",
      "Iteration 299, loss = 0.02196131\n",
      "Iteration 300, loss = 0.02193357\n",
      "Iteration 301, loss = 0.02190659\n",
      "Iteration 302, loss = 0.02188199\n",
      "Iteration 303, loss = 0.02185401\n",
      "Iteration 304, loss = 0.02182376\n",
      "Iteration 305, loss = 0.02179814\n",
      "Iteration 306, loss = 0.02177322\n",
      "Iteration 307, loss = 0.02175971\n",
      "Iteration 308, loss = 0.02172097\n",
      "Iteration 309, loss = 0.02170387\n",
      "Iteration 310, loss = 0.02166702\n",
      "Iteration 311, loss = 0.02164161\n",
      "Iteration 312, loss = 0.02162032\n",
      "Iteration 313, loss = 0.02158575\n",
      "Iteration 314, loss = 0.02157658\n",
      "Iteration 315, loss = 0.02154058\n",
      "Iteration 316, loss = 0.02151261\n",
      "Iteration 317, loss = 0.02148649\n",
      "Iteration 318, loss = 0.02147993\n",
      "Iteration 319, loss = 0.02144062\n",
      "Iteration 320, loss = 0.02140709\n",
      "Iteration 321, loss = 0.02138097\n",
      "Iteration 322, loss = 0.02135732\n",
      "Iteration 323, loss = 0.02133497\n",
      "Iteration 324, loss = 0.02130336\n",
      "Iteration 325, loss = 0.02129277\n",
      "Iteration 326, loss = 0.02124787\n",
      "Iteration 327, loss = 0.02124879\n",
      "Iteration 328, loss = 0.02121036\n",
      "Iteration 329, loss = 0.02117380\n",
      "Iteration 330, loss = 0.02116660\n",
      "Iteration 331, loss = 0.02111834\n",
      "Iteration 332, loss = 0.02109624\n",
      "Iteration 333, loss = 0.02107551\n",
      "Iteration 334, loss = 0.02104954\n",
      "Iteration 335, loss = 0.02102006\n",
      "Iteration 336, loss = 0.02099563\n",
      "Iteration 337, loss = 0.02096709\n",
      "Iteration 338, loss = 0.02094519\n",
      "Iteration 339, loss = 0.02091462\n",
      "Iteration 340, loss = 0.02088651\n",
      "Iteration 341, loss = 0.02087538\n",
      "Iteration 342, loss = 0.02084906\n",
      "Iteration 343, loss = 0.02081642\n",
      "Iteration 344, loss = 0.02079814\n",
      "Iteration 345, loss = 0.02077002\n",
      "Iteration 346, loss = 0.02073919\n",
      "Iteration 347, loss = 0.02072234\n",
      "Iteration 348, loss = 0.02069349\n",
      "Iteration 349, loss = 0.02067432\n",
      "Iteration 350, loss = 0.02064380\n",
      "Iteration 351, loss = 0.02061569\n",
      "Iteration 352, loss = 0.02059829\n",
      "Iteration 353, loss = 0.02056946\n",
      "Iteration 354, loss = 0.02054576\n",
      "Iteration 355, loss = 0.02052871\n",
      "Iteration 356, loss = 0.02049449\n",
      "Iteration 357, loss = 0.02047399\n",
      "Iteration 358, loss = 0.02045221\n",
      "Iteration 359, loss = 0.02043104\n",
      "Iteration 360, loss = 0.02039156\n",
      "Iteration 361, loss = 0.02038530\n",
      "Iteration 362, loss = 0.02034206\n",
      "Iteration 363, loss = 0.02031935\n",
      "Iteration 364, loss = 0.02029397\n",
      "Iteration 365, loss = 0.02027008\n",
      "Iteration 366, loss = 0.02023909\n",
      "Iteration 367, loss = 0.02021367\n",
      "Iteration 368, loss = 0.02018972\n",
      "Iteration 369, loss = 0.02017315\n",
      "Iteration 370, loss = 0.02014414\n",
      "Iteration 371, loss = 0.02011853\n",
      "Iteration 372, loss = 0.02009241\n",
      "Iteration 373, loss = 0.02009046\n",
      "Iteration 374, loss = 0.02005247\n",
      "Iteration 375, loss = 0.02002370\n",
      "Iteration 376, loss = 0.01999465\n",
      "Iteration 377, loss = 0.01997460\n",
      "Iteration 378, loss = 0.01995295\n",
      "Iteration 379, loss = 0.01991879\n",
      "Iteration 380, loss = 0.01989157\n",
      "Iteration 381, loss = 0.01987786\n",
      "Iteration 382, loss = 0.01984657\n",
      "Iteration 383, loss = 0.01982139\n",
      "Iteration 384, loss = 0.01979670\n",
      "Iteration 385, loss = 0.01977090\n",
      "Iteration 386, loss = 0.01975754\n",
      "Iteration 387, loss = 0.01972075\n",
      "Iteration 388, loss = 0.01970905\n",
      "Iteration 389, loss = 0.01967712\n",
      "Iteration 390, loss = 0.01965360\n",
      "Iteration 391, loss = 0.01962297\n",
      "Iteration 392, loss = 0.01960790\n",
      "Iteration 393, loss = 0.01957484\n",
      "Iteration 394, loss = 0.01955535\n",
      "Iteration 395, loss = 0.01953077\n",
      "Iteration 396, loss = 0.01950590\n",
      "Iteration 397, loss = 0.01949212\n",
      "Iteration 398, loss = 0.01946175\n",
      "Iteration 399, loss = 0.01943197\n",
      "Iteration 400, loss = 0.01940131\n",
      "Iteration 401, loss = 0.01937898\n",
      "Iteration 402, loss = 0.01935588\n",
      "Iteration 403, loss = 0.01933091\n",
      "Iteration 404, loss = 0.01931544\n",
      "Iteration 405, loss = 0.01928458\n",
      "Iteration 406, loss = 0.01925628\n",
      "Iteration 407, loss = 0.01928280\n",
      "Iteration 408, loss = 0.01922260\n",
      "Iteration 409, loss = 0.01918075\n",
      "Iteration 410, loss = 0.01916213\n",
      "Iteration 411, loss = 0.01913501\n",
      "Iteration 412, loss = 0.01911404\n",
      "Iteration 413, loss = 0.01909998\n",
      "Iteration 414, loss = 0.01907537\n",
      "Iteration 415, loss = 0.01904668\n",
      "Iteration 416, loss = 0.01901975\n",
      "Iteration 417, loss = 0.01899466\n",
      "Iteration 418, loss = 0.01896580\n",
      "Iteration 419, loss = 0.01894784\n",
      "Iteration 420, loss = 0.01892842\n",
      "Iteration 421, loss = 0.01890728\n",
      "Iteration 422, loss = 0.01887160\n",
      "Iteration 423, loss = 0.01885907\n",
      "Iteration 424, loss = 0.01882339\n",
      "Iteration 425, loss = 0.01880525\n",
      "Iteration 426, loss = 0.01878015\n",
      "Iteration 427, loss = 0.01876291\n",
      "Iteration 428, loss = 0.01873204\n",
      "Iteration 429, loss = 0.01870851\n",
      "Iteration 430, loss = 0.01868315\n",
      "Iteration 431, loss = 0.01866366\n",
      "Iteration 432, loss = 0.01864083\n",
      "Iteration 433, loss = 0.01861368\n",
      "Iteration 434, loss = 0.01859279\n",
      "Iteration 435, loss = 0.01857402\n",
      "Iteration 436, loss = 0.01854974\n",
      "Iteration 437, loss = 0.01852308\n",
      "Iteration 438, loss = 0.01853730\n",
      "Iteration 439, loss = 0.01847769\n",
      "Iteration 440, loss = 0.01845070\n",
      "Iteration 441, loss = 0.01842946\n",
      "Iteration 442, loss = 0.01841587\n",
      "Iteration 443, loss = 0.01838130\n",
      "Iteration 444, loss = 0.01836149\n",
      "Iteration 445, loss = 0.01834009\n",
      "Iteration 446, loss = 0.01832622\n",
      "Iteration 447, loss = 0.01829093\n",
      "Iteration 448, loss = 0.01826993\n",
      "Iteration 449, loss = 0.01824624\n",
      "Iteration 450, loss = 0.01822942\n",
      "Iteration 451, loss = 0.01819503\n",
      "Iteration 452, loss = 0.01817375\n",
      "Iteration 453, loss = 0.01814941\n",
      "Iteration 454, loss = 0.01812308\n",
      "Iteration 455, loss = 0.01810333\n",
      "Iteration 456, loss = 0.01808854\n",
      "Iteration 457, loss = 0.01805968\n",
      "Iteration 458, loss = 0.01804222\n",
      "Iteration 459, loss = 0.01800376\n",
      "Iteration 460, loss = 0.01799134\n",
      "Iteration 461, loss = 0.01795292\n",
      "Iteration 462, loss = 0.01793241\n",
      "Iteration 463, loss = 0.01790981\n",
      "Iteration 464, loss = 0.01789062\n",
      "Iteration 465, loss = 0.01785885\n",
      "Iteration 466, loss = 0.01784383\n",
      "Iteration 467, loss = 0.01781535\n",
      "Iteration 468, loss = 0.01779178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 469, loss = 0.01776727\n",
      "Iteration 470, loss = 0.01774678\n",
      "Iteration 471, loss = 0.01771495\n",
      "Iteration 472, loss = 0.01768868\n",
      "Iteration 473, loss = 0.01766617\n",
      "Iteration 474, loss = 0.01763911\n",
      "Iteration 475, loss = 0.01761482\n",
      "Iteration 476, loss = 0.01759097\n",
      "Iteration 477, loss = 0.01756586\n",
      "Iteration 478, loss = 0.01754198\n",
      "Iteration 479, loss = 0.01751513\n",
      "Iteration 480, loss = 0.01751246\n",
      "Iteration 481, loss = 0.01745858\n",
      "Iteration 482, loss = 0.01743213\n",
      "Iteration 483, loss = 0.01740912\n",
      "Iteration 484, loss = 0.01738338\n",
      "Iteration 485, loss = 0.01736512\n",
      "Iteration 486, loss = 0.01733351\n",
      "Iteration 487, loss = 0.01730245\n",
      "Iteration 488, loss = 0.01728436\n",
      "Iteration 489, loss = 0.01725346\n",
      "Iteration 490, loss = 0.01722745\n",
      "Iteration 491, loss = 0.01720492\n",
      "Iteration 492, loss = 0.01717597\n",
      "Iteration 493, loss = 0.01718216\n",
      "Iteration 494, loss = 0.01712461\n",
      "Iteration 495, loss = 0.01710795\n",
      "Iteration 496, loss = 0.01708609\n",
      "Iteration 497, loss = 0.01705143\n",
      "Iteration 498, loss = 0.01703107\n",
      "Iteration 499, loss = 0.01700188\n",
      "Iteration 500, loss = 0.01698413\n",
      "Iteration 501, loss = 0.01696585\n",
      "Iteration 502, loss = 0.01693366\n",
      "Iteration 503, loss = 0.01691158\n",
      "Iteration 504, loss = 0.01687209\n",
      "Iteration 505, loss = 0.01685054\n",
      "Iteration 506, loss = 0.01682634\n",
      "Iteration 507, loss = 0.01680194\n",
      "Iteration 508, loss = 0.01677553\n",
      "Iteration 509, loss = 0.01675191\n",
      "Iteration 510, loss = 0.01673645\n",
      "Iteration 511, loss = 0.01669645\n",
      "Iteration 512, loss = 0.01666872\n",
      "Iteration 513, loss = 0.01665287\n",
      "Iteration 514, loss = 0.01662585\n",
      "Iteration 515, loss = 0.01660017\n",
      "Iteration 516, loss = 0.01657635\n",
      "Iteration 517, loss = 0.01653892\n",
      "Iteration 518, loss = 0.01651909\n",
      "Iteration 519, loss = 0.01648700\n",
      "Iteration 520, loss = 0.01646228\n",
      "Iteration 521, loss = 0.01643894\n",
      "Iteration 522, loss = 0.01640588\n",
      "Iteration 523, loss = 0.01640101\n",
      "Iteration 524, loss = 0.01635854\n",
      "Iteration 525, loss = 0.01634869\n",
      "Iteration 526, loss = 0.01631089\n",
      "Iteration 527, loss = 0.01628746\n",
      "Iteration 528, loss = 0.01626095\n",
      "Iteration 529, loss = 0.01623096\n",
      "Iteration 530, loss = 0.01620660\n",
      "Iteration 531, loss = 0.01618151\n",
      "Iteration 532, loss = 0.01615466\n",
      "Iteration 533, loss = 0.01612475\n",
      "Iteration 534, loss = 0.01611472\n",
      "Iteration 535, loss = 0.01607981\n",
      "Iteration 536, loss = 0.01605430\n",
      "Iteration 537, loss = 0.01603062\n",
      "Iteration 538, loss = 0.01599736\n",
      "Iteration 539, loss = 0.01598727\n",
      "Iteration 540, loss = 0.01595059\n",
      "Iteration 541, loss = 0.01592901\n",
      "Iteration 542, loss = 0.01589808\n",
      "Iteration 543, loss = 0.01586870\n",
      "Iteration 544, loss = 0.01585544\n",
      "Iteration 545, loss = 0.01582407\n",
      "Iteration 546, loss = 0.01579261\n",
      "Iteration 547, loss = 0.01577382\n",
      "Iteration 548, loss = 0.01574381\n",
      "Iteration 549, loss = 0.01572105\n",
      "Iteration 550, loss = 0.01570507\n",
      "Iteration 551, loss = 0.01566960\n",
      "Iteration 552, loss = 0.01564586\n",
      "Iteration 553, loss = 0.01562027\n",
      "Iteration 554, loss = 0.01560109\n",
      "Iteration 555, loss = 0.01557313\n",
      "Iteration 556, loss = 0.01555253\n",
      "Iteration 557, loss = 0.01552943\n",
      "Iteration 558, loss = 0.01550358\n",
      "Iteration 559, loss = 0.01546799\n",
      "Iteration 560, loss = 0.01545586\n",
      "Iteration 561, loss = 0.01541923\n",
      "Iteration 562, loss = 0.01540092\n",
      "Iteration 563, loss = 0.01537038\n",
      "Iteration 564, loss = 0.01534363\n",
      "Iteration 565, loss = 0.01532251\n",
      "Iteration 566, loss = 0.01530510\n",
      "Iteration 567, loss = 0.01527374\n",
      "Iteration 568, loss = 0.01524985\n",
      "Iteration 569, loss = 0.01522575\n",
      "Iteration 570, loss = 0.01520864\n",
      "Iteration 571, loss = 0.01517821\n",
      "Iteration 572, loss = 0.01514563\n",
      "Iteration 573, loss = 0.01511948\n",
      "Iteration 574, loss = 0.01509543\n",
      "Iteration 575, loss = 0.01507856\n",
      "Iteration 576, loss = 0.01505297\n",
      "Iteration 577, loss = 0.01502417\n",
      "Iteration 578, loss = 0.01499735\n",
      "Iteration 579, loss = 0.01497934\n",
      "Iteration 580, loss = 0.01495027\n",
      "Iteration 581, loss = 0.01492378\n",
      "Iteration 582, loss = 0.01490345\n",
      "Iteration 583, loss = 0.01487940\n",
      "Iteration 584, loss = 0.01485226\n",
      "Iteration 585, loss = 0.01483389\n",
      "Iteration 586, loss = 0.01481512\n",
      "Iteration 587, loss = 0.01478105\n",
      "Iteration 588, loss = 0.01476907\n",
      "Iteration 589, loss = 0.01473445\n",
      "Iteration 590, loss = 0.01470248\n",
      "Iteration 591, loss = 0.01467582\n",
      "Iteration 592, loss = 0.01466019\n",
      "Iteration 593, loss = 0.01463108\n",
      "Iteration 594, loss = 0.01460820\n",
      "Iteration 595, loss = 0.01457766\n",
      "Iteration 596, loss = 0.01454414\n",
      "Iteration 597, loss = 0.01452361\n",
      "Iteration 598, loss = 0.01450033\n",
      "Iteration 599, loss = 0.01448534\n",
      "Iteration 600, loss = 0.01444920\n",
      "Iteration 601, loss = 0.01442410\n",
      "Iteration 602, loss = 0.01440302\n",
      "Iteration 603, loss = 0.01437062\n",
      "Iteration 604, loss = 0.01435282\n",
      "Iteration 605, loss = 0.01433042\n",
      "Iteration 606, loss = 0.01431135\n",
      "Iteration 607, loss = 0.01425649\n",
      "Iteration 608, loss = 0.01422535\n",
      "Iteration 609, loss = 0.01420680\n",
      "Iteration 610, loss = 0.01417179\n",
      "Iteration 611, loss = 0.01414148\n",
      "Iteration 612, loss = 0.01411344\n",
      "Iteration 613, loss = 0.01407878\n",
      "Iteration 614, loss = 0.01405490\n",
      "Iteration 615, loss = 0.01402264\n",
      "Iteration 616, loss = 0.01400152\n",
      "Iteration 617, loss = 0.01396363\n",
      "Iteration 618, loss = 0.01393351\n",
      "Iteration 619, loss = 0.01391926\n",
      "Iteration 620, loss = 0.01387345\n",
      "Iteration 621, loss = 0.01384168\n",
      "Iteration 622, loss = 0.01382094\n",
      "Iteration 623, loss = 0.01377890\n",
      "Iteration 624, loss = 0.01378200\n",
      "Iteration 625, loss = 0.01372262\n",
      "Iteration 626, loss = 0.01371781\n",
      "Iteration 627, loss = 0.01364003\n",
      "Iteration 628, loss = 0.01360661\n",
      "Iteration 629, loss = 0.01357223\n",
      "Iteration 630, loss = 0.01354705\n",
      "Iteration 631, loss = 0.01352848\n",
      "Iteration 632, loss = 0.01347551\n",
      "Iteration 633, loss = 0.01344519\n",
      "Iteration 634, loss = 0.01341561\n",
      "Iteration 635, loss = 0.01337417\n",
      "Iteration 636, loss = 0.01333872\n",
      "Iteration 637, loss = 0.01331921\n",
      "Iteration 638, loss = 0.01327807\n",
      "Iteration 639, loss = 0.01327435\n",
      "Iteration 640, loss = 0.01321063\n",
      "Iteration 641, loss = 0.01317511\n",
      "Iteration 642, loss = 0.01313957\n",
      "Iteration 643, loss = 0.01310756\n",
      "Iteration 644, loss = 0.01308491\n",
      "Iteration 645, loss = 0.01304256\n",
      "Iteration 646, loss = 0.01301513\n",
      "Iteration 647, loss = 0.01298309\n",
      "Iteration 648, loss = 0.01294465\n",
      "Iteration 649, loss = 0.01292448\n",
      "Iteration 650, loss = 0.01289982\n",
      "Iteration 651, loss = 0.01285759\n",
      "Iteration 652, loss = 0.01283776\n",
      "Iteration 653, loss = 0.01279579\n",
      "Iteration 654, loss = 0.01278112\n",
      "Iteration 655, loss = 0.01274499\n",
      "Iteration 656, loss = 0.01271427\n",
      "Iteration 657, loss = 0.01267854\n",
      "Iteration 658, loss = 0.01264231\n",
      "Iteration 659, loss = 0.01262725\n",
      "Iteration 660, loss = 0.01259172\n",
      "Iteration 661, loss = 0.01256666\n",
      "Iteration 662, loss = 0.01254186\n",
      "Iteration 663, loss = 0.01249298\n",
      "Iteration 664, loss = 0.01247111\n",
      "Iteration 665, loss = 0.01244075\n",
      "Iteration 666, loss = 0.01241656\n",
      "Iteration 667, loss = 0.01239506\n",
      "Iteration 668, loss = 0.01235131\n",
      "Iteration 669, loss = 0.01232227\n",
      "Iteration 670, loss = 0.01228480\n",
      "Iteration 671, loss = 0.01226685\n",
      "Iteration 672, loss = 0.01222564\n",
      "Iteration 673, loss = 0.01218891\n",
      "Iteration 674, loss = 0.01215431\n",
      "Iteration 675, loss = 0.01214323\n",
      "Iteration 676, loss = 0.01210584\n",
      "Iteration 677, loss = 0.01206302\n",
      "Iteration 678, loss = 0.01204495\n",
      "Iteration 679, loss = 0.01200645\n",
      "Iteration 680, loss = 0.01198393\n",
      "Iteration 681, loss = 0.01195159\n",
      "Iteration 682, loss = 0.01191123\n",
      "Iteration 683, loss = 0.01186827\n",
      "Iteration 684, loss = 0.01186436\n",
      "Iteration 685, loss = 0.01179263\n",
      "Iteration 686, loss = 0.01176943\n",
      "Iteration 687, loss = 0.01174754\n",
      "Iteration 688, loss = 0.01171867\n",
      "Iteration 689, loss = 0.01169438\n",
      "Iteration 690, loss = 0.01165048\n",
      "Iteration 691, loss = 0.01161897\n",
      "Iteration 692, loss = 0.01162363\n",
      "Iteration 693, loss = 0.01156989\n",
      "Iteration 694, loss = 0.01153305\n",
      "Iteration 695, loss = 0.01152250\n",
      "Iteration 696, loss = 0.01146602\n",
      "Iteration 697, loss = 0.01143893\n",
      "Iteration 698, loss = 0.01140282\n",
      "Iteration 699, loss = 0.01139287\n",
      "Iteration 700, loss = 0.01134352\n",
      "Iteration 701, loss = 0.01131713\n",
      "Iteration 702, loss = 0.01129352\n",
      "Iteration 703, loss = 0.01126972\n",
      "Iteration 704, loss = 0.01126623\n",
      "Iteration 705, loss = 0.01120160\n",
      "Iteration 706, loss = 0.01117766\n",
      "Iteration 707, loss = 0.01115031\n",
      "Iteration 708, loss = 0.01116189\n",
      "Iteration 709, loss = 0.01109089\n",
      "Iteration 710, loss = 0.01107413\n",
      "Iteration 711, loss = 0.01104503\n",
      "Iteration 712, loss = 0.01101233\n",
      "Iteration 713, loss = 0.01098809\n",
      "Iteration 714, loss = 0.01097453\n",
      "Iteration 715, loss = 0.01095613\n",
      "Iteration 716, loss = 0.01090528\n",
      "Iteration 717, loss = 0.01087338\n",
      "Iteration 718, loss = 0.01086462\n",
      "Iteration 719, loss = 0.01082379\n",
      "Iteration 720, loss = 0.01079527\n",
      "Iteration 721, loss = 0.01078886\n",
      "Iteration 722, loss = 0.01076322\n",
      "Iteration 723, loss = 0.01073670\n",
      "Iteration 724, loss = 0.01069440\n",
      "Iteration 725, loss = 0.01068210\n",
      "Iteration 726, loss = 0.01064413\n",
      "Iteration 727, loss = 0.01062146\n",
      "Iteration 728, loss = 0.01059691\n",
      "Iteration 729, loss = 0.01056230\n",
      "Iteration 730, loss = 0.01056593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 731, loss = 0.01053019\n",
      "Iteration 732, loss = 0.01049032\n",
      "Iteration 733, loss = 0.01046837\n",
      "Iteration 734, loss = 0.01045341\n",
      "Iteration 735, loss = 0.01041822\n",
      "Iteration 736, loss = 0.01040221\n",
      "Iteration 737, loss = 0.01035583\n",
      "Iteration 738, loss = 0.01035467\n",
      "Iteration 739, loss = 0.01032118\n",
      "Iteration 740, loss = 0.01029007\n",
      "Iteration 741, loss = 0.01026945\n",
      "Iteration 742, loss = 0.01026365\n",
      "Iteration 743, loss = 0.01022424\n",
      "Iteration 744, loss = 0.01022809\n",
      "Iteration 745, loss = 0.01017370\n",
      "Iteration 746, loss = 0.01015673\n",
      "Iteration 747, loss = 0.01012532\n",
      "Iteration 748, loss = 0.01012705\n",
      "Iteration 749, loss = 0.01008221\n",
      "Iteration 750, loss = 0.01006265\n",
      "Iteration 751, loss = 0.01005434\n",
      "Iteration 752, loss = 0.01001827\n",
      "Iteration 753, loss = 0.00998542\n",
      "Iteration 754, loss = 0.00997217\n",
      "Iteration 755, loss = 0.00994063\n",
      "Iteration 756, loss = 0.00993146\n",
      "Iteration 757, loss = 0.00993060\n",
      "Iteration 758, loss = 0.00989388\n",
      "Iteration 759, loss = 0.00986217\n",
      "Iteration 760, loss = 0.00983217\n",
      "Iteration 761, loss = 0.00981365\n",
      "Iteration 762, loss = 0.00980566\n",
      "Iteration 763, loss = 0.00976472\n",
      "Iteration 764, loss = 0.00973544\n",
      "Iteration 765, loss = 0.00972043\n",
      "Iteration 766, loss = 0.00969433\n",
      "Iteration 767, loss = 0.00967821\n",
      "Iteration 768, loss = 0.00966046\n",
      "Iteration 769, loss = 0.00963159\n",
      "Iteration 770, loss = 0.00961856\n",
      "Iteration 771, loss = 0.00958801\n",
      "Iteration 772, loss = 0.00957785\n",
      "Iteration 773, loss = 0.00954728\n",
      "Iteration 774, loss = 0.00952854\n",
      "Iteration 775, loss = 0.00951707\n",
      "Iteration 776, loss = 0.00947598\n",
      "Iteration 777, loss = 0.00946473\n",
      "Iteration 778, loss = 0.00944173\n",
      "Iteration 779, loss = 0.00940028\n",
      "Iteration 780, loss = 0.00938168\n",
      "Iteration 781, loss = 0.00935732\n",
      "Iteration 782, loss = 0.00934126\n",
      "Iteration 783, loss = 0.00931644\n",
      "Iteration 784, loss = 0.00933711\n",
      "Iteration 785, loss = 0.00926228\n",
      "Iteration 786, loss = 0.00924051\n",
      "Iteration 787, loss = 0.00923173\n",
      "Iteration 788, loss = 0.00919432\n",
      "Iteration 789, loss = 0.00921750\n",
      "Iteration 790, loss = 0.00915511\n",
      "Iteration 791, loss = 0.00913084\n",
      "Iteration 792, loss = 0.00911757\n",
      "Iteration 793, loss = 0.00909038\n",
      "Iteration 794, loss = 0.00906624\n",
      "Iteration 795, loss = 0.00905403\n",
      "Iteration 796, loss = 0.00903821\n",
      "Iteration 797, loss = 0.00902651\n",
      "Iteration 798, loss = 0.00899535\n",
      "Iteration 799, loss = 0.00896162\n",
      "Iteration 800, loss = 0.00892446\n",
      "Iteration 801, loss = 0.00891058\n",
      "Iteration 802, loss = 0.00891699\n",
      "Iteration 803, loss = 0.00886799\n",
      "Iteration 804, loss = 0.00885253\n",
      "Iteration 805, loss = 0.00883080\n",
      "Iteration 806, loss = 0.00880305\n",
      "Iteration 807, loss = 0.00879477\n",
      "Iteration 808, loss = 0.00875603\n",
      "Iteration 809, loss = 0.00872814\n",
      "Iteration 810, loss = 0.00873493\n",
      "Iteration 811, loss = 0.00869129\n",
      "Iteration 812, loss = 0.00867622\n",
      "Iteration 813, loss = 0.00865483\n",
      "Iteration 814, loss = 0.00866501\n",
      "Iteration 815, loss = 0.00860831\n",
      "Iteration 816, loss = 0.00857768\n",
      "Iteration 817, loss = 0.00856844\n",
      "Iteration 818, loss = 0.00855532\n",
      "Iteration 819, loss = 0.00852661\n",
      "Iteration 820, loss = 0.00850388\n",
      "Iteration 821, loss = 0.00850058\n",
      "Iteration 822, loss = 0.00846136\n",
      "Iteration 823, loss = 0.00845811\n",
      "Iteration 824, loss = 0.00840993\n",
      "Iteration 825, loss = 0.00840025\n",
      "Iteration 826, loss = 0.00837146\n",
      "Iteration 827, loss = 0.00834754\n",
      "Iteration 828, loss = 0.00835884\n",
      "Iteration 829, loss = 0.00831144\n",
      "Iteration 830, loss = 0.00829322\n",
      "Iteration 831, loss = 0.00826540\n",
      "Iteration 832, loss = 0.00824998\n",
      "Iteration 833, loss = 0.00823078\n",
      "Iteration 834, loss = 0.00821241\n",
      "Iteration 835, loss = 0.00817887\n",
      "Iteration 836, loss = 0.00815754\n",
      "Iteration 837, loss = 0.00814044\n",
      "Iteration 838, loss = 0.00812045\n",
      "Iteration 839, loss = 0.00809295\n",
      "Iteration 840, loss = 0.00807982\n",
      "Iteration 841, loss = 0.00804726\n",
      "Iteration 842, loss = 0.00802609\n",
      "Iteration 843, loss = 0.00801442\n",
      "Iteration 844, loss = 0.00798557\n",
      "Iteration 845, loss = 0.00797414\n",
      "Iteration 846, loss = 0.00794945\n",
      "Iteration 847, loss = 0.00792476\n",
      "Iteration 848, loss = 0.00791345\n",
      "Iteration 849, loss = 0.00789302\n",
      "Iteration 850, loss = 0.00786778\n",
      "Iteration 851, loss = 0.00784248\n",
      "Iteration 852, loss = 0.00782933\n",
      "Iteration 853, loss = 0.00779718\n",
      "Iteration 854, loss = 0.00778036\n",
      "Iteration 855, loss = 0.00776039\n",
      "Iteration 856, loss = 0.00775501\n",
      "Iteration 857, loss = 0.00772928\n",
      "Iteration 858, loss = 0.00769880\n",
      "Iteration 859, loss = 0.00769490\n",
      "Iteration 860, loss = 0.00765467\n",
      "Iteration 861, loss = 0.00764363\n",
      "Iteration 862, loss = 0.00761955\n",
      "Iteration 863, loss = 0.00760654\n",
      "Iteration 864, loss = 0.00761216\n",
      "Iteration 865, loss = 0.00758383\n",
      "Iteration 866, loss = 0.00753598\n",
      "Iteration 867, loss = 0.00751979\n",
      "Iteration 868, loss = 0.00750752\n",
      "Iteration 869, loss = 0.00748591\n",
      "Iteration 870, loss = 0.00745406\n",
      "Iteration 871, loss = 0.00743955\n",
      "Iteration 872, loss = 0.00741735\n",
      "Iteration 873, loss = 0.00738985\n",
      "Iteration 874, loss = 0.00736860\n",
      "Iteration 875, loss = 0.00735144\n",
      "Iteration 876, loss = 0.00735295\n",
      "Iteration 877, loss = 0.00732389\n",
      "Iteration 878, loss = 0.00729181\n",
      "Iteration 879, loss = 0.00728376\n",
      "Iteration 880, loss = 0.00724525\n",
      "Iteration 881, loss = 0.00724172\n",
      "Iteration 882, loss = 0.00721567\n",
      "Iteration 883, loss = 0.00720636\n",
      "Iteration 884, loss = 0.00716522\n",
      "Iteration 885, loss = 0.00716909\n",
      "Iteration 886, loss = 0.00711454\n",
      "Iteration 887, loss = 0.00710214\n",
      "Iteration 888, loss = 0.00708964\n",
      "Iteration 889, loss = 0.00705950\n",
      "Iteration 890, loss = 0.00704961\n",
      "Iteration 891, loss = 0.00702077\n",
      "Iteration 892, loss = 0.00700889\n",
      "Iteration 893, loss = 0.00698682\n",
      "Iteration 894, loss = 0.00698378\n",
      "Iteration 895, loss = 0.00694369\n",
      "Iteration 896, loss = 0.00692623\n",
      "Iteration 897, loss = 0.00690639\n",
      "Iteration 898, loss = 0.00688877\n",
      "Iteration 899, loss = 0.00687767\n",
      "Iteration 900, loss = 0.00688458\n",
      "Iteration 901, loss = 0.00682304\n",
      "Iteration 902, loss = 0.00680488\n",
      "Iteration 903, loss = 0.00678711\n",
      "Iteration 904, loss = 0.00676902\n",
      "Iteration 905, loss = 0.00675876\n",
      "Iteration 906, loss = 0.00672770\n",
      "Iteration 907, loss = 0.00675559\n",
      "Iteration 908, loss = 0.00670024\n",
      "Iteration 909, loss = 0.00667223\n",
      "Iteration 910, loss = 0.00665597\n",
      "Iteration 911, loss = 0.00664037\n",
      "Iteration 912, loss = 0.00661206\n",
      "Iteration 913, loss = 0.00659513\n",
      "Iteration 914, loss = 0.00656917\n",
      "Iteration 915, loss = 0.00654586\n",
      "Iteration 916, loss = 0.00654077\n",
      "Iteration 917, loss = 0.00651400\n",
      "Iteration 918, loss = 0.00649669\n",
      "Iteration 919, loss = 0.00647888\n",
      "Iteration 920, loss = 0.00647114\n",
      "Iteration 921, loss = 0.00645447\n",
      "Iteration 922, loss = 0.00642434\n",
      "Iteration 923, loss = 0.00641673\n",
      "Iteration 924, loss = 0.00637504\n",
      "Iteration 925, loss = 0.00636175\n",
      "Iteration 926, loss = 0.00634490\n",
      "Iteration 927, loss = 0.00632341\n",
      "Iteration 928, loss = 0.00631011\n",
      "Iteration 929, loss = 0.00628388\n",
      "Iteration 930, loss = 0.00626508\n",
      "Iteration 931, loss = 0.00626502\n",
      "Iteration 932, loss = 0.00623965\n",
      "Iteration 933, loss = 0.00621143\n",
      "Iteration 934, loss = 0.00619308\n",
      "Iteration 935, loss = 0.00616762\n",
      "Iteration 936, loss = 0.00616465\n",
      "Iteration 937, loss = 0.00613368\n",
      "Iteration 938, loss = 0.00612189\n",
      "Iteration 939, loss = 0.00609295\n",
      "Iteration 940, loss = 0.00608692\n",
      "Iteration 941, loss = 0.00605343\n",
      "Iteration 942, loss = 0.00603782\n",
      "Iteration 943, loss = 0.00602008\n",
      "Iteration 944, loss = 0.00599964\n",
      "Iteration 945, loss = 0.00598870\n",
      "Iteration 946, loss = 0.00597299\n",
      "Iteration 947, loss = 0.00595433\n",
      "Iteration 948, loss = 0.00592957\n",
      "Iteration 949, loss = 0.00591132\n",
      "Iteration 950, loss = 0.00589702\n",
      "Iteration 951, loss = 0.00588568\n",
      "Iteration 952, loss = 0.00586192\n",
      "Iteration 953, loss = 0.00585070\n",
      "Iteration 954, loss = 0.00581904\n",
      "Iteration 955, loss = 0.00582988\n",
      "Iteration 956, loss = 0.00578602\n",
      "Iteration 957, loss = 0.00576441\n",
      "Iteration 958, loss = 0.00574161\n",
      "Iteration 959, loss = 0.00572303\n",
      "Iteration 960, loss = 0.00571013\n",
      "Iteration 961, loss = 0.00569539\n",
      "Iteration 962, loss = 0.00567574\n",
      "Iteration 963, loss = 0.00565136\n",
      "Iteration 964, loss = 0.00563641\n",
      "Iteration 965, loss = 0.00563132\n",
      "Iteration 966, loss = 0.00560706\n",
      "Iteration 967, loss = 0.00558760\n",
      "Iteration 968, loss = 0.00556864\n",
      "Iteration 969, loss = 0.00554408\n",
      "Iteration 970, loss = 0.00553053\n",
      "Iteration 971, loss = 0.00551220\n",
      "Iteration 972, loss = 0.00549128\n",
      "Iteration 973, loss = 0.00547567\n",
      "Iteration 974, loss = 0.00547171\n",
      "Iteration 975, loss = 0.00543493\n",
      "Iteration 976, loss = 0.00541979\n",
      "Iteration 977, loss = 0.00540573\n",
      "Iteration 978, loss = 0.00538487\n",
      "Iteration 979, loss = 0.00536869\n",
      "Iteration 980, loss = 0.00534820\n",
      "Iteration 981, loss = 0.00533926\n",
      "Iteration 982, loss = 0.00534746\n",
      "Iteration 983, loss = 0.00528935\n",
      "Iteration 984, loss = 0.00527069\n",
      "Iteration 985, loss = 0.00526080\n",
      "Iteration 986, loss = 0.00524089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 987, loss = 0.00521870\n",
      "Iteration 988, loss = 0.00519010\n",
      "Iteration 989, loss = 0.00518624\n",
      "Iteration 990, loss = 0.00516273\n",
      "Iteration 991, loss = 0.00515026\n",
      "Iteration 992, loss = 0.00514053\n",
      "Iteration 993, loss = 0.00511323\n",
      "Iteration 994, loss = 0.00509252\n",
      "Iteration 995, loss = 0.00509200\n",
      "Iteration 996, loss = 0.00505217\n",
      "Iteration 997, loss = 0.00504245\n",
      "Iteration 998, loss = 0.00502853\n",
      "Iteration 999, loss = 0.00502793\n",
      "Iteration 1000, loss = 0.00498141\n",
      "Attribute -> Fermat\n",
      "Accuracy -> 0.994140625\n",
      "Iteration 1, loss = 0.45983075\n",
      "Iteration 2, loss = 0.31965682\n",
      "Iteration 3, loss = 0.24808782\n",
      "Iteration 4, loss = 0.22102585\n",
      "Iteration 5, loss = 0.20971125\n",
      "Iteration 6, loss = 0.20408299\n",
      "Iteration 7, loss = 0.19940844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.19519653\n",
      "Iteration 9, loss = 0.19123457\n",
      "Iteration 10, loss = 0.18740224\n",
      "Iteration 11, loss = 0.18382097\n",
      "Iteration 12, loss = 0.18045276\n",
      "Iteration 13, loss = 0.17727893\n",
      "Iteration 14, loss = 0.17419581\n",
      "Iteration 15, loss = 0.17124624\n",
      "Iteration 16, loss = 0.16836301\n",
      "Iteration 17, loss = 0.16556393\n",
      "Iteration 18, loss = 0.16302155\n",
      "Iteration 19, loss = 0.16042212\n",
      "Iteration 20, loss = 0.15799897\n",
      "Iteration 21, loss = 0.15560245\n",
      "Iteration 22, loss = 0.15332352\n",
      "Iteration 23, loss = 0.15103728\n",
      "Iteration 24, loss = 0.14884628\n",
      "Iteration 25, loss = 0.14675496\n",
      "Iteration 26, loss = 0.14464214\n",
      "Iteration 27, loss = 0.14253362\n",
      "Iteration 28, loss = 0.14055130\n",
      "Iteration 29, loss = 0.13863563\n",
      "Iteration 30, loss = 0.13674216\n",
      "Iteration 31, loss = 0.13491818\n",
      "Iteration 32, loss = 0.13307005\n",
      "Iteration 33, loss = 0.13134489\n",
      "Iteration 34, loss = 0.12970026\n",
      "Iteration 35, loss = 0.12803020\n",
      "Iteration 36, loss = 0.12651807\n",
      "Iteration 37, loss = 0.12487135\n",
      "Iteration 38, loss = 0.12343945\n",
      "Iteration 39, loss = 0.12192173\n",
      "Iteration 40, loss = 0.12053303\n",
      "Iteration 41, loss = 0.11914088\n",
      "Iteration 42, loss = 0.11775365\n",
      "Iteration 43, loss = 0.11643146\n",
      "Iteration 44, loss = 0.11519866\n",
      "Iteration 45, loss = 0.11388354\n",
      "Iteration 46, loss = 0.11265397\n",
      "Iteration 47, loss = 0.11149700\n",
      "Iteration 48, loss = 0.11030655\n",
      "Iteration 49, loss = 0.10919895\n",
      "Iteration 50, loss = 0.10810300\n",
      "Iteration 51, loss = 0.10705516\n",
      "Iteration 52, loss = 0.10601700\n",
      "Iteration 53, loss = 0.10495896\n",
      "Iteration 54, loss = 0.10396650\n",
      "Iteration 55, loss = 0.10300214\n",
      "Iteration 56, loss = 0.10205517\n",
      "Iteration 57, loss = 0.10110011\n",
      "Iteration 58, loss = 0.10019964\n",
      "Iteration 59, loss = 0.09930369\n",
      "Iteration 60, loss = 0.09842773\n",
      "Iteration 61, loss = 0.09756371\n",
      "Iteration 62, loss = 0.09668839\n",
      "Iteration 63, loss = 0.09586166\n",
      "Iteration 64, loss = 0.09504607\n",
      "Iteration 65, loss = 0.09425859\n",
      "Iteration 66, loss = 0.09350201\n",
      "Iteration 67, loss = 0.09271139\n",
      "Iteration 68, loss = 0.09198469\n",
      "Iteration 69, loss = 0.09126007\n",
      "Iteration 70, loss = 0.09057008\n",
      "Iteration 71, loss = 0.08985516\n",
      "Iteration 72, loss = 0.08919319\n",
      "Iteration 73, loss = 0.08848447\n",
      "Iteration 74, loss = 0.08782952\n",
      "Iteration 75, loss = 0.08713154\n",
      "Iteration 76, loss = 0.08650394\n",
      "Iteration 77, loss = 0.08583025\n",
      "Iteration 78, loss = 0.08519143\n",
      "Iteration 79, loss = 0.08459560\n",
      "Iteration 80, loss = 0.08393881\n",
      "Iteration 81, loss = 0.08330240\n",
      "Iteration 82, loss = 0.08274062\n",
      "Iteration 83, loss = 0.08207175\n",
      "Iteration 84, loss = 0.08145650\n",
      "Iteration 85, loss = 0.08086376\n",
      "Iteration 86, loss = 0.08030829\n",
      "Iteration 87, loss = 0.07964549\n",
      "Iteration 88, loss = 0.07905275\n",
      "Iteration 89, loss = 0.07846278\n",
      "Iteration 90, loss = 0.07787038\n",
      "Iteration 91, loss = 0.07725278\n",
      "Iteration 92, loss = 0.07667561\n",
      "Iteration 93, loss = 0.07605089\n",
      "Iteration 94, loss = 0.07543235\n",
      "Iteration 95, loss = 0.07479810\n",
      "Iteration 96, loss = 0.07418611\n",
      "Iteration 97, loss = 0.07355270\n",
      "Iteration 98, loss = 0.07295516\n",
      "Iteration 99, loss = 0.07233154\n",
      "Iteration 100, loss = 0.07172784\n",
      "Iteration 101, loss = 0.07110182\n",
      "Iteration 102, loss = 0.07051468\n",
      "Iteration 103, loss = 0.06988870\n",
      "Iteration 104, loss = 0.06927757\n",
      "Iteration 105, loss = 0.06865164\n",
      "Iteration 106, loss = 0.06804615\n",
      "Iteration 107, loss = 0.06744355\n",
      "Iteration 108, loss = 0.06683600\n",
      "Iteration 109, loss = 0.06625219\n",
      "Iteration 110, loss = 0.06566344\n",
      "Iteration 111, loss = 0.06509591\n",
      "Iteration 112, loss = 0.06452707\n",
      "Iteration 113, loss = 0.06392178\n",
      "Iteration 114, loss = 0.06338032\n",
      "Iteration 115, loss = 0.06274938\n",
      "Iteration 116, loss = 0.06220968\n",
      "Iteration 117, loss = 0.06156225\n",
      "Iteration 118, loss = 0.06097972\n",
      "Iteration 119, loss = 0.06034117\n",
      "Iteration 120, loss = 0.05971007\n",
      "Iteration 121, loss = 0.05910638\n",
      "Iteration 122, loss = 0.05851730\n",
      "Iteration 123, loss = 0.05787297\n",
      "Iteration 124, loss = 0.05726218\n",
      "Iteration 125, loss = 0.05662896\n",
      "Iteration 126, loss = 0.05603901\n",
      "Iteration 127, loss = 0.05543221\n",
      "Iteration 128, loss = 0.05488038\n",
      "Iteration 129, loss = 0.05428013\n",
      "Iteration 130, loss = 0.05369307\n",
      "Iteration 131, loss = 0.05316432\n",
      "Iteration 132, loss = 0.05256151\n",
      "Iteration 133, loss = 0.05199636\n",
      "Iteration 134, loss = 0.05142839\n",
      "Iteration 135, loss = 0.05083755\n",
      "Iteration 136, loss = 0.05028993\n",
      "Iteration 137, loss = 0.04974791\n",
      "Iteration 138, loss = 0.04920966\n",
      "Iteration 139, loss = 0.04865362\n",
      "Iteration 140, loss = 0.04810474\n",
      "Iteration 141, loss = 0.04761476\n",
      "Iteration 142, loss = 0.04704330\n",
      "Iteration 143, loss = 0.04653726\n",
      "Iteration 144, loss = 0.04601389\n",
      "Iteration 145, loss = 0.04551283\n",
      "Iteration 146, loss = 0.04496451\n",
      "Iteration 147, loss = 0.04445428\n",
      "Iteration 148, loss = 0.04397022\n",
      "Iteration 149, loss = 0.04345156\n",
      "Iteration 150, loss = 0.04297302\n",
      "Iteration 151, loss = 0.04247103\n",
      "Iteration 152, loss = 0.04197703\n",
      "Iteration 153, loss = 0.04149664\n",
      "Iteration 154, loss = 0.04102114\n",
      "Iteration 155, loss = 0.04053535\n",
      "Iteration 156, loss = 0.04006775\n",
      "Iteration 157, loss = 0.03958893\n",
      "Iteration 158, loss = 0.03912080\n",
      "Iteration 159, loss = 0.03867909\n",
      "Iteration 160, loss = 0.03818896\n",
      "Iteration 161, loss = 0.03774762\n",
      "Iteration 162, loss = 0.03728612\n",
      "Iteration 163, loss = 0.03683016\n",
      "Iteration 164, loss = 0.03637837\n",
      "Iteration 165, loss = 0.03598170\n",
      "Iteration 166, loss = 0.03551182\n",
      "Iteration 167, loss = 0.03508695\n",
      "Iteration 168, loss = 0.03466926\n",
      "Iteration 169, loss = 0.03425146\n",
      "Iteration 170, loss = 0.03385504\n",
      "Iteration 171, loss = 0.03344133\n",
      "Iteration 172, loss = 0.03305066\n",
      "Iteration 173, loss = 0.03269850\n",
      "Iteration 174, loss = 0.03226455\n",
      "Iteration 175, loss = 0.03186489\n",
      "Iteration 176, loss = 0.03149516\n",
      "Iteration 177, loss = 0.03111591\n",
      "Iteration 178, loss = 0.03079744\n",
      "Iteration 179, loss = 0.03038221\n",
      "Iteration 180, loss = 0.03002147\n",
      "Iteration 181, loss = 0.02967746\n",
      "Iteration 182, loss = 0.02932494\n",
      "Iteration 183, loss = 0.02897313\n",
      "Iteration 184, loss = 0.02862353\n",
      "Iteration 185, loss = 0.02828177\n",
      "Iteration 186, loss = 0.02795761\n",
      "Iteration 187, loss = 0.02762299\n",
      "Iteration 188, loss = 0.02731525\n",
      "Iteration 189, loss = 0.02698150\n",
      "Iteration 190, loss = 0.02666467\n",
      "Iteration 191, loss = 0.02634553\n",
      "Iteration 192, loss = 0.02604035\n",
      "Iteration 193, loss = 0.02571333\n",
      "Iteration 194, loss = 0.02542106\n",
      "Iteration 195, loss = 0.02512770\n",
      "Iteration 196, loss = 0.02481611\n",
      "Iteration 197, loss = 0.02453228\n",
      "Iteration 198, loss = 0.02424939\n",
      "Iteration 199, loss = 0.02395483\n",
      "Iteration 200, loss = 0.02367194\n",
      "Iteration 201, loss = 0.02338664\n",
      "Iteration 202, loss = 0.02311537\n",
      "Iteration 203, loss = 0.02285819\n",
      "Iteration 204, loss = 0.02257646\n",
      "Iteration 205, loss = 0.02230590\n",
      "Iteration 206, loss = 0.02205728\n",
      "Iteration 207, loss = 0.02179131\n",
      "Iteration 208, loss = 0.02154635\n",
      "Iteration 209, loss = 0.02129628\n",
      "Iteration 210, loss = 0.02104292\n",
      "Iteration 211, loss = 0.02079579\n",
      "Iteration 212, loss = 0.02056868\n",
      "Iteration 213, loss = 0.02032734\n",
      "Iteration 214, loss = 0.02009731\n",
      "Iteration 215, loss = 0.01987049\n",
      "Iteration 216, loss = 0.01963193\n",
      "Iteration 217, loss = 0.01941308\n",
      "Iteration 218, loss = 0.01919731\n",
      "Iteration 219, loss = 0.01899111\n",
      "Iteration 220, loss = 0.01876080\n",
      "Iteration 221, loss = 0.01854981\n",
      "Iteration 222, loss = 0.01834478\n",
      "Iteration 223, loss = 0.01813198\n",
      "Iteration 224, loss = 0.01792617\n",
      "Iteration 225, loss = 0.01775160\n",
      "Iteration 226, loss = 0.01754003\n",
      "Iteration 227, loss = 0.01733552\n",
      "Iteration 228, loss = 0.01714046\n",
      "Iteration 229, loss = 0.01694404\n",
      "Iteration 230, loss = 0.01676171\n",
      "Iteration 231, loss = 0.01658112\n",
      "Iteration 232, loss = 0.01639140\n",
      "Iteration 233, loss = 0.01620498\n",
      "Iteration 234, loss = 0.01603115\n",
      "Iteration 235, loss = 0.01585788\n",
      "Iteration 236, loss = 0.01568220\n",
      "Iteration 237, loss = 0.01550834\n",
      "Iteration 238, loss = 0.01534536\n",
      "Iteration 239, loss = 0.01517844\n",
      "Iteration 240, loss = 0.01501516\n",
      "Iteration 241, loss = 0.01484968\n",
      "Iteration 242, loss = 0.01468736\n",
      "Iteration 243, loss = 0.01453291\n",
      "Iteration 244, loss = 0.01438100\n",
      "Iteration 245, loss = 0.01423001\n",
      "Iteration 246, loss = 0.01408278\n",
      "Iteration 247, loss = 0.01392892\n",
      "Iteration 248, loss = 0.01378638\n",
      "Iteration 249, loss = 0.01364660\n",
      "Iteration 250, loss = 0.01349856\n",
      "Iteration 251, loss = 0.01336111\n",
      "Iteration 252, loss = 0.01322387\n",
      "Iteration 253, loss = 0.01308800\n",
      "Iteration 254, loss = 0.01295509\n",
      "Iteration 255, loss = 0.01283031\n",
      "Iteration 256, loss = 0.01269129\n",
      "Iteration 257, loss = 0.01256526\n",
      "Iteration 258, loss = 0.01244182\n",
      "Iteration 259, loss = 0.01231654\n",
      "Iteration 260, loss = 0.01219052\n",
      "Iteration 261, loss = 0.01207014\n",
      "Iteration 262, loss = 0.01194902\n",
      "Iteration 263, loss = 0.01183117\n",
      "Iteration 264, loss = 0.01171737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 265, loss = 0.01160115\n",
      "Iteration 266, loss = 0.01148950\n",
      "Iteration 267, loss = 0.01137814\n",
      "Iteration 268, loss = 0.01126804\n",
      "Iteration 269, loss = 0.01115847\n",
      "Iteration 270, loss = 0.01105530\n",
      "Iteration 271, loss = 0.01094730\n",
      "Iteration 272, loss = 0.01084718\n",
      "Iteration 273, loss = 0.01074042\n",
      "Iteration 274, loss = 0.01063993\n",
      "Iteration 275, loss = 0.01053615\n",
      "Iteration 276, loss = 0.01044149\n",
      "Iteration 277, loss = 0.01034034\n",
      "Iteration 278, loss = 0.01024433\n",
      "Iteration 279, loss = 0.01014936\n",
      "Iteration 280, loss = 0.01005673\n",
      "Iteration 281, loss = 0.00995677\n",
      "Iteration 282, loss = 0.00986812\n",
      "Iteration 283, loss = 0.00977681\n",
      "Iteration 284, loss = 0.00968427\n",
      "Iteration 285, loss = 0.00959805\n",
      "Iteration 286, loss = 0.00951456\n",
      "Iteration 287, loss = 0.00942305\n",
      "Iteration 288, loss = 0.00933699\n",
      "Iteration 289, loss = 0.00925658\n",
      "Iteration 290, loss = 0.00917081\n",
      "Iteration 291, loss = 0.00908926\n",
      "Iteration 292, loss = 0.00900396\n",
      "Iteration 293, loss = 0.00892350\n",
      "Iteration 294, loss = 0.00884212\n",
      "Iteration 295, loss = 0.00876573\n",
      "Iteration 296, loss = 0.00868914\n",
      "Iteration 297, loss = 0.00861187\n",
      "Iteration 298, loss = 0.00853626\n",
      "Iteration 299, loss = 0.00846182\n",
      "Iteration 300, loss = 0.00838676\n",
      "Iteration 301, loss = 0.00831560\n",
      "Iteration 302, loss = 0.00824605\n",
      "Iteration 303, loss = 0.00817802\n",
      "Iteration 304, loss = 0.00810416\n",
      "Iteration 305, loss = 0.00803803\n",
      "Iteration 306, loss = 0.00796693\n",
      "Iteration 307, loss = 0.00789982\n",
      "Iteration 308, loss = 0.00783549\n",
      "Iteration 309, loss = 0.00776776\n",
      "Iteration 310, loss = 0.00770253\n",
      "Iteration 311, loss = 0.00764141\n",
      "Iteration 312, loss = 0.00757716\n",
      "Iteration 313, loss = 0.00751416\n",
      "Iteration 314, loss = 0.00745898\n",
      "Iteration 315, loss = 0.00739075\n",
      "Iteration 316, loss = 0.00733229\n",
      "Iteration 317, loss = 0.00727327\n",
      "Iteration 318, loss = 0.00721458\n",
      "Iteration 319, loss = 0.00715810\n",
      "Iteration 320, loss = 0.00710159\n",
      "Iteration 321, loss = 0.00704706\n",
      "Iteration 322, loss = 0.00699193\n",
      "Iteration 323, loss = 0.00693662\n",
      "Iteration 324, loss = 0.00688207\n",
      "Iteration 325, loss = 0.00683062\n",
      "Iteration 326, loss = 0.00677550\n",
      "Iteration 327, loss = 0.00672561\n",
      "Iteration 328, loss = 0.00667325\n",
      "Iteration 329, loss = 0.00662402\n",
      "Iteration 330, loss = 0.00657358\n",
      "Iteration 331, loss = 0.00652452\n",
      "Iteration 332, loss = 0.00647567\n",
      "Iteration 333, loss = 0.00642998\n",
      "Iteration 334, loss = 0.00638153\n",
      "Iteration 335, loss = 0.00633420\n",
      "Iteration 336, loss = 0.00628631\n",
      "Iteration 337, loss = 0.00623901\n",
      "Iteration 338, loss = 0.00619547\n",
      "Iteration 339, loss = 0.00614955\n",
      "Iteration 340, loss = 0.00610708\n",
      "Iteration 341, loss = 0.00606463\n",
      "Iteration 342, loss = 0.00601782\n",
      "Iteration 343, loss = 0.00597514\n",
      "Iteration 344, loss = 0.00593248\n",
      "Iteration 345, loss = 0.00589141\n",
      "Iteration 346, loss = 0.00584842\n",
      "Iteration 347, loss = 0.00580752\n",
      "Iteration 348, loss = 0.00576640\n",
      "Iteration 349, loss = 0.00572635\n",
      "Iteration 350, loss = 0.00568747\n",
      "Iteration 351, loss = 0.00564708\n",
      "Iteration 352, loss = 0.00560945\n",
      "Iteration 353, loss = 0.00557164\n",
      "Iteration 354, loss = 0.00553262\n",
      "Iteration 355, loss = 0.00549345\n",
      "Iteration 356, loss = 0.00545818\n",
      "Iteration 357, loss = 0.00542240\n",
      "Iteration 358, loss = 0.00538424\n",
      "Iteration 359, loss = 0.00534866\n",
      "Iteration 360, loss = 0.00531419\n",
      "Iteration 361, loss = 0.00527784\n",
      "Iteration 362, loss = 0.00524349\n",
      "Iteration 363, loss = 0.00520961\n",
      "Iteration 364, loss = 0.00517619\n",
      "Iteration 365, loss = 0.00514186\n",
      "Iteration 366, loss = 0.00510799\n",
      "Iteration 367, loss = 0.00507503\n",
      "Iteration 368, loss = 0.00504362\n",
      "Iteration 369, loss = 0.00501083\n",
      "Iteration 370, loss = 0.00497972\n",
      "Iteration 371, loss = 0.00494698\n",
      "Iteration 372, loss = 0.00491573\n",
      "Iteration 373, loss = 0.00488732\n",
      "Iteration 374, loss = 0.00485444\n",
      "Iteration 375, loss = 0.00482483\n",
      "Iteration 376, loss = 0.00479456\n",
      "Iteration 377, loss = 0.00476531\n",
      "Iteration 378, loss = 0.00473573\n",
      "Iteration 379, loss = 0.00470683\n",
      "Iteration 380, loss = 0.00467854\n",
      "Iteration 381, loss = 0.00464969\n",
      "Iteration 382, loss = 0.00462204\n",
      "Iteration 383, loss = 0.00459478\n",
      "Iteration 384, loss = 0.00456659\n",
      "Iteration 385, loss = 0.00453940\n",
      "Iteration 386, loss = 0.00451227\n",
      "Iteration 387, loss = 0.00448548\n",
      "Iteration 388, loss = 0.00445934\n",
      "Iteration 389, loss = 0.00443265\n",
      "Iteration 390, loss = 0.00440783\n",
      "Iteration 391, loss = 0.00438012\n",
      "Iteration 392, loss = 0.00435519\n",
      "Iteration 393, loss = 0.00432984\n",
      "Iteration 394, loss = 0.00430391\n",
      "Iteration 395, loss = 0.00428203\n",
      "Iteration 396, loss = 0.00425541\n",
      "Iteration 397, loss = 0.00423062\n",
      "Iteration 398, loss = 0.00420786\n",
      "Iteration 399, loss = 0.00418262\n",
      "Iteration 400, loss = 0.00415897\n",
      "Iteration 401, loss = 0.00413538\n",
      "Iteration 402, loss = 0.00411271\n",
      "Iteration 403, loss = 0.00409077\n",
      "Iteration 404, loss = 0.00406563\n",
      "Iteration 405, loss = 0.00404335\n",
      "Iteration 406, loss = 0.00402085\n",
      "Iteration 407, loss = 0.00399900\n",
      "Iteration 408, loss = 0.00397665\n",
      "Iteration 409, loss = 0.00395523\n",
      "Iteration 410, loss = 0.00393360\n",
      "Iteration 411, loss = 0.00391209\n",
      "Iteration 412, loss = 0.00389074\n",
      "Iteration 413, loss = 0.00387022\n",
      "Iteration 414, loss = 0.00384863\n",
      "Iteration 415, loss = 0.00382808\n",
      "Iteration 416, loss = 0.00380788\n",
      "Iteration 417, loss = 0.00378712\n",
      "Iteration 418, loss = 0.00376781\n",
      "Iteration 419, loss = 0.00374742\n",
      "Iteration 420, loss = 0.00372884\n",
      "Iteration 421, loss = 0.00370803\n",
      "Iteration 422, loss = 0.00368909\n",
      "Iteration 423, loss = 0.00366996\n",
      "Iteration 424, loss = 0.00365102\n",
      "Iteration 425, loss = 0.00363166\n",
      "Iteration 426, loss = 0.00361322\n",
      "Iteration 427, loss = 0.00359382\n",
      "Iteration 428, loss = 0.00357588\n",
      "Iteration 429, loss = 0.00355727\n",
      "Iteration 430, loss = 0.00353888\n",
      "Iteration 431, loss = 0.00352057\n",
      "Iteration 432, loss = 0.00350267\n",
      "Iteration 433, loss = 0.00348560\n",
      "Iteration 434, loss = 0.00346723\n",
      "Iteration 435, loss = 0.00344930\n",
      "Iteration 436, loss = 0.00343230\n",
      "Iteration 437, loss = 0.00341502\n",
      "Iteration 438, loss = 0.00339811\n",
      "Iteration 439, loss = 0.00338111\n",
      "Iteration 440, loss = 0.00336411\n",
      "Iteration 441, loss = 0.00334790\n",
      "Iteration 442, loss = 0.00333185\n",
      "Iteration 443, loss = 0.00331548\n",
      "Iteration 444, loss = 0.00329928\n",
      "Iteration 445, loss = 0.00328388\n",
      "Iteration 446, loss = 0.00326885\n",
      "Iteration 447, loss = 0.00325219\n",
      "Iteration 448, loss = 0.00323631\n",
      "Iteration 449, loss = 0.00322066\n",
      "Iteration 450, loss = 0.00320551\n",
      "Iteration 451, loss = 0.00319065\n",
      "Iteration 452, loss = 0.00317549\n",
      "Iteration 453, loss = 0.00316033\n",
      "Iteration 454, loss = 0.00314552\n",
      "Iteration 455, loss = 0.00313035\n",
      "Iteration 456, loss = 0.00311574\n",
      "Iteration 457, loss = 0.00310094\n",
      "Iteration 458, loss = 0.00308656\n",
      "Iteration 459, loss = 0.00307280\n",
      "Iteration 460, loss = 0.00305789\n",
      "Iteration 461, loss = 0.00304375\n",
      "Iteration 462, loss = 0.00302956\n",
      "Iteration 463, loss = 0.00301574\n",
      "Iteration 464, loss = 0.00300199\n",
      "Iteration 465, loss = 0.00298901\n",
      "Iteration 466, loss = 0.00297590\n",
      "Iteration 467, loss = 0.00296169\n",
      "Iteration 468, loss = 0.00294770\n",
      "Iteration 469, loss = 0.00293480\n",
      "Iteration 470, loss = 0.00292141\n",
      "Iteration 471, loss = 0.00290851\n",
      "Iteration 472, loss = 0.00289531\n",
      "Iteration 473, loss = 0.00288254\n",
      "Iteration 474, loss = 0.00286986\n",
      "Iteration 475, loss = 0.00285702\n",
      "Iteration 476, loss = 0.00284449\n",
      "Iteration 477, loss = 0.00283177\n",
      "Iteration 478, loss = 0.00281967\n",
      "Iteration 479, loss = 0.00280754\n",
      "Iteration 480, loss = 0.00279474\n",
      "Iteration 481, loss = 0.00278248\n",
      "Iteration 482, loss = 0.00277129\n",
      "Iteration 483, loss = 0.00275880\n",
      "Iteration 484, loss = 0.00274687\n",
      "Iteration 485, loss = 0.00273545\n",
      "Iteration 486, loss = 0.00272404\n",
      "Iteration 487, loss = 0.00271171\n",
      "Iteration 488, loss = 0.00270062\n",
      "Iteration 489, loss = 0.00268903\n",
      "Iteration 490, loss = 0.00267754\n",
      "Iteration 491, loss = 0.00266638\n",
      "Iteration 492, loss = 0.00265526\n",
      "Iteration 493, loss = 0.00264433\n",
      "Iteration 494, loss = 0.00263319\n",
      "Iteration 495, loss = 0.00262192\n",
      "Iteration 496, loss = 0.00261112\n",
      "Iteration 497, loss = 0.00260041\n",
      "Iteration 498, loss = 0.00258982\n",
      "Iteration 499, loss = 0.00257908\n",
      "Iteration 500, loss = 0.00256867\n",
      "Iteration 501, loss = 0.00255816\n",
      "Iteration 502, loss = 0.00254812\n",
      "Iteration 503, loss = 0.00253743\n",
      "Iteration 504, loss = 0.00252722\n",
      "Iteration 505, loss = 0.00251674\n",
      "Iteration 506, loss = 0.00250654\n",
      "Iteration 507, loss = 0.00249663\n",
      "Iteration 508, loss = 0.00248618\n",
      "Iteration 509, loss = 0.00247670\n",
      "Iteration 510, loss = 0.00246688\n",
      "Iteration 511, loss = 0.00245680\n",
      "Iteration 512, loss = 0.00244711\n",
      "Iteration 513, loss = 0.00243754\n",
      "Iteration 514, loss = 0.00242844\n",
      "Iteration 515, loss = 0.00241818\n",
      "Iteration 516, loss = 0.00240881\n",
      "Iteration 517, loss = 0.00239935\n",
      "Iteration 518, loss = 0.00238996\n",
      "Iteration 519, loss = 0.00238102\n",
      "Iteration 520, loss = 0.00237147\n",
      "Iteration 521, loss = 0.00236278\n",
      "Iteration 522, loss = 0.00235367\n",
      "Iteration 523, loss = 0.00234454\n",
      "Iteration 524, loss = 0.00233564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 525, loss = 0.00232678\n",
      "Iteration 526, loss = 0.00231804\n",
      "Iteration 527, loss = 0.00230899\n",
      "Iteration 528, loss = 0.00230037\n",
      "Iteration 529, loss = 0.00229206\n",
      "Iteration 530, loss = 0.00228330\n",
      "Iteration 531, loss = 0.00227470\n",
      "Iteration 532, loss = 0.00226652\n",
      "Iteration 533, loss = 0.00225785\n",
      "Iteration 534, loss = 0.00224969\n",
      "Iteration 535, loss = 0.00224113\n",
      "Iteration 536, loss = 0.00223319\n",
      "Iteration 537, loss = 0.00222488\n",
      "Iteration 538, loss = 0.00221657\n",
      "Iteration 539, loss = 0.00220916\n",
      "Iteration 540, loss = 0.00220071\n",
      "Iteration 541, loss = 0.00219215\n",
      "Iteration 542, loss = 0.00218440\n",
      "Iteration 543, loss = 0.00217669\n",
      "Iteration 544, loss = 0.00216879\n",
      "Iteration 545, loss = 0.00216068\n",
      "Iteration 546, loss = 0.00215337\n",
      "Iteration 547, loss = 0.00214548\n",
      "Iteration 548, loss = 0.00213729\n",
      "Iteration 549, loss = 0.00212991\n",
      "Iteration 550, loss = 0.00212220\n",
      "Iteration 551, loss = 0.00211463\n",
      "Iteration 552, loss = 0.00210711\n",
      "Iteration 553, loss = 0.00209978\n",
      "Iteration 554, loss = 0.00209228\n",
      "Iteration 555, loss = 0.00208507\n",
      "Iteration 556, loss = 0.00207753\n",
      "Iteration 557, loss = 0.00207043\n",
      "Iteration 558, loss = 0.00206345\n",
      "Iteration 559, loss = 0.00205648\n",
      "Iteration 560, loss = 0.00204894\n",
      "Iteration 561, loss = 0.00204216\n",
      "Iteration 562, loss = 0.00203516\n",
      "Iteration 563, loss = 0.00202812\n",
      "Iteration 564, loss = 0.00202117\n",
      "Iteration 565, loss = 0.00201414\n",
      "Iteration 566, loss = 0.00200717\n",
      "Iteration 567, loss = 0.00200036\n",
      "Iteration 568, loss = 0.00199355\n",
      "Iteration 569, loss = 0.00198674\n",
      "Iteration 570, loss = 0.00197988\n",
      "Iteration 571, loss = 0.00197337\n",
      "Iteration 572, loss = 0.00196641\n",
      "Iteration 573, loss = 0.00195993\n",
      "Iteration 574, loss = 0.00195341\n",
      "Iteration 575, loss = 0.00194689\n",
      "Iteration 576, loss = 0.00194034\n",
      "Iteration 577, loss = 0.00193397\n",
      "Iteration 578, loss = 0.00192759\n",
      "Iteration 579, loss = 0.00192122\n",
      "Iteration 580, loss = 0.00191460\n",
      "Iteration 581, loss = 0.00190838\n",
      "Iteration 582, loss = 0.00190249\n",
      "Iteration 583, loss = 0.00189609\n",
      "Iteration 584, loss = 0.00188986\n",
      "Iteration 585, loss = 0.00188382\n",
      "Iteration 586, loss = 0.00187757\n",
      "Iteration 587, loss = 0.00187145\n",
      "Iteration 588, loss = 0.00186533\n",
      "Iteration 589, loss = 0.00185921\n",
      "Iteration 590, loss = 0.00185347\n",
      "Iteration 591, loss = 0.00184729\n",
      "Iteration 592, loss = 0.00184137\n",
      "Iteration 593, loss = 0.00183541\n",
      "Iteration 594, loss = 0.00182976\n",
      "Iteration 595, loss = 0.00182394\n",
      "Iteration 596, loss = 0.00181796\n",
      "Iteration 597, loss = 0.00181218\n",
      "Iteration 598, loss = 0.00180646\n",
      "Iteration 599, loss = 0.00180079\n",
      "Iteration 600, loss = 0.00179505\n",
      "Iteration 601, loss = 0.00178944\n",
      "Iteration 602, loss = 0.00178383\n",
      "Iteration 603, loss = 0.00177845\n",
      "Iteration 604, loss = 0.00177291\n",
      "Iteration 605, loss = 0.00176733\n",
      "Iteration 606, loss = 0.00176192\n",
      "Iteration 607, loss = 0.00175626\n",
      "Iteration 608, loss = 0.00175068\n",
      "Iteration 609, loss = 0.00174526\n",
      "Iteration 610, loss = 0.00174011\n",
      "Iteration 611, loss = 0.00173439\n",
      "Iteration 612, loss = 0.00172919\n",
      "Iteration 613, loss = 0.00172384\n",
      "Iteration 614, loss = 0.00171848\n",
      "Iteration 615, loss = 0.00171336\n",
      "Iteration 616, loss = 0.00170811\n",
      "Iteration 617, loss = 0.00170299\n",
      "Iteration 618, loss = 0.00169756\n",
      "Iteration 619, loss = 0.00169251\n",
      "Iteration 620, loss = 0.00168733\n",
      "Iteration 621, loss = 0.00168227\n",
      "Iteration 622, loss = 0.00167719\n",
      "Iteration 623, loss = 0.00167218\n",
      "Iteration 624, loss = 0.00166736\n",
      "Iteration 625, loss = 0.00166214\n",
      "Iteration 626, loss = 0.00165714\n",
      "Iteration 627, loss = 0.00165228\n",
      "Iteration 628, loss = 0.00164732\n",
      "Iteration 629, loss = 0.00164234\n",
      "Iteration 630, loss = 0.00163763\n",
      "Iteration 631, loss = 0.00163271\n",
      "Iteration 632, loss = 0.00162786\n",
      "Iteration 633, loss = 0.00162307\n",
      "Iteration 634, loss = 0.00161832\n",
      "Iteration 635, loss = 0.00161342\n",
      "Iteration 636, loss = 0.00160871\n",
      "Iteration 637, loss = 0.00160393\n",
      "Iteration 638, loss = 0.00159948\n",
      "Iteration 639, loss = 0.00159472\n",
      "Iteration 640, loss = 0.00159015\n",
      "Iteration 641, loss = 0.00158550\n",
      "Iteration 642, loss = 0.00158120\n",
      "Iteration 643, loss = 0.00157661\n",
      "Iteration 644, loss = 0.00157183\n",
      "Iteration 645, loss = 0.00156736\n",
      "Iteration 646, loss = 0.00156285\n",
      "Iteration 647, loss = 0.00155848\n",
      "Iteration 648, loss = 0.00155398\n",
      "Iteration 649, loss = 0.00154946\n",
      "Iteration 650, loss = 0.00154505\n",
      "Iteration 651, loss = 0.00154058\n",
      "Iteration 652, loss = 0.00153620\n",
      "Iteration 653, loss = 0.00153197\n",
      "Iteration 654, loss = 0.00152762\n",
      "Iteration 655, loss = 0.00152344\n",
      "Iteration 656, loss = 0.00151905\n",
      "Iteration 657, loss = 0.00151497\n",
      "Iteration 658, loss = 0.00151066\n",
      "Iteration 659, loss = 0.00150650\n",
      "Iteration 660, loss = 0.00150238\n",
      "Iteration 661, loss = 0.00149822\n",
      "Iteration 662, loss = 0.00149390\n",
      "Iteration 663, loss = 0.00148999\n",
      "Iteration 664, loss = 0.00148571\n",
      "Iteration 665, loss = 0.00148167\n",
      "Iteration 666, loss = 0.00147764\n",
      "Iteration 667, loss = 0.00147358\n",
      "Iteration 668, loss = 0.00146958\n",
      "Iteration 669, loss = 0.00146572\n",
      "Iteration 670, loss = 0.00146152\n",
      "Iteration 671, loss = 0.00145751\n",
      "Iteration 672, loss = 0.00145361\n",
      "Iteration 673, loss = 0.00144953\n",
      "Iteration 674, loss = 0.00144582\n",
      "Iteration 675, loss = 0.00144190\n",
      "Iteration 676, loss = 0.00143802\n",
      "Iteration 677, loss = 0.00143407\n",
      "Iteration 678, loss = 0.00143029\n",
      "Iteration 679, loss = 0.00142646\n",
      "Iteration 680, loss = 0.00142276\n",
      "Iteration 681, loss = 0.00141886\n",
      "Iteration 682, loss = 0.00141508\n",
      "Iteration 683, loss = 0.00141147\n",
      "Iteration 684, loss = 0.00140756\n",
      "Iteration 685, loss = 0.00140382\n",
      "Iteration 686, loss = 0.00140026\n",
      "Iteration 687, loss = 0.00139641\n",
      "Iteration 688, loss = 0.00139274\n",
      "Iteration 689, loss = 0.00138920\n",
      "Iteration 690, loss = 0.00138558\n",
      "Iteration 691, loss = 0.00138194\n",
      "Iteration 692, loss = 0.00137828\n",
      "Iteration 693, loss = 0.00137471\n",
      "Iteration 694, loss = 0.00137128\n",
      "Iteration 695, loss = 0.00136768\n",
      "Iteration 696, loss = 0.00136427\n",
      "Iteration 697, loss = 0.00136064\n",
      "Iteration 698, loss = 0.00135717\n",
      "Iteration 699, loss = 0.00135379\n",
      "Iteration 700, loss = 0.00135015\n",
      "Iteration 701, loss = 0.00134687\n",
      "Iteration 702, loss = 0.00134343\n",
      "Iteration 703, loss = 0.00134003\n",
      "Iteration 704, loss = 0.00133652\n",
      "Iteration 705, loss = 0.00133320\n",
      "Iteration 706, loss = 0.00132979\n",
      "Iteration 707, loss = 0.00132645\n",
      "Iteration 708, loss = 0.00132320\n",
      "Iteration 709, loss = 0.00131985\n",
      "Iteration 710, loss = 0.00131647\n",
      "Iteration 711, loss = 0.00131313\n",
      "Iteration 712, loss = 0.00130995\n",
      "Iteration 713, loss = 0.00130661\n",
      "Iteration 714, loss = 0.00130336\n",
      "Iteration 715, loss = 0.00130021\n",
      "Iteration 716, loss = 0.00129690\n",
      "Iteration 717, loss = 0.00129388\n",
      "Iteration 718, loss = 0.00129046\n",
      "Iteration 719, loss = 0.00128736\n",
      "Iteration 720, loss = 0.00128411\n",
      "Iteration 721, loss = 0.00128100\n",
      "Iteration 722, loss = 0.00127780\n",
      "Iteration 723, loss = 0.00127472\n",
      "Iteration 724, loss = 0.00127162\n",
      "Iteration 725, loss = 0.00126842\n",
      "Iteration 726, loss = 0.00126539\n",
      "Iteration 727, loss = 0.00126222\n",
      "Iteration 728, loss = 0.00125927\n",
      "Iteration 729, loss = 0.00125616\n",
      "Iteration 730, loss = 0.00125306\n",
      "Iteration 731, loss = 0.00125008\n",
      "Iteration 732, loss = 0.00124708\n",
      "Iteration 733, loss = 0.00124423\n",
      "Iteration 734, loss = 0.00124110\n",
      "Iteration 735, loss = 0.00123837\n",
      "Iteration 736, loss = 0.00123536\n",
      "Iteration 737, loss = 0.00123231\n",
      "Iteration 738, loss = 0.00122944\n",
      "Iteration 739, loss = 0.00122647\n",
      "Iteration 740, loss = 0.00122358\n",
      "Iteration 741, loss = 0.00122075\n",
      "Iteration 742, loss = 0.00121794\n",
      "Iteration 743, loss = 0.00121499\n",
      "Iteration 744, loss = 0.00121229\n",
      "Iteration 745, loss = 0.00120933\n",
      "Iteration 746, loss = 0.00120629\n",
      "Iteration 747, loss = 0.00120350\n",
      "Iteration 748, loss = 0.00120070\n",
      "Iteration 749, loss = 0.00119797\n",
      "Iteration 750, loss = 0.00119514\n",
      "Iteration 751, loss = 0.00119240\n",
      "Iteration 752, loss = 0.00118965\n",
      "Iteration 753, loss = 0.00118696\n",
      "Iteration 754, loss = 0.00118420\n",
      "Iteration 755, loss = 0.00118149\n",
      "Iteration 756, loss = 0.00117869\n",
      "Iteration 757, loss = 0.00117616\n",
      "Iteration 758, loss = 0.00117337\n",
      "Iteration 759, loss = 0.00117062\n",
      "Iteration 760, loss = 0.00116801\n",
      "Iteration 761, loss = 0.00116535\n",
      "Iteration 762, loss = 0.00116266\n",
      "Iteration 763, loss = 0.00115997\n",
      "Iteration 764, loss = 0.00115739\n",
      "Iteration 765, loss = 0.00115475\n",
      "Iteration 766, loss = 0.00115218\n",
      "Iteration 767, loss = 0.00114955\n",
      "Iteration 768, loss = 0.00114697\n",
      "Iteration 769, loss = 0.00114439\n",
      "Iteration 770, loss = 0.00114187\n",
      "Iteration 771, loss = 0.00113928\n",
      "Iteration 772, loss = 0.00113686\n",
      "Iteration 773, loss = 0.00113423\n",
      "Iteration 774, loss = 0.00113183\n",
      "Iteration 775, loss = 0.00112918\n",
      "Iteration 776, loss = 0.00112673\n",
      "Iteration 777, loss = 0.00112426\n",
      "Iteration 778, loss = 0.00112179\n",
      "Iteration 779, loss = 0.00111927\n",
      "Iteration 780, loss = 0.00111683\n",
      "Iteration 781, loss = 0.00111437\n",
      "Iteration 782, loss = 0.00111194\n",
      "Iteration 783, loss = 0.00110949\n",
      "Iteration 784, loss = 0.00110701\n",
      "Iteration 785, loss = 0.00110462\n",
      "Iteration 786, loss = 0.00110223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 787, loss = 0.00109986\n",
      "Iteration 788, loss = 0.00109746\n",
      "Iteration 789, loss = 0.00109513\n",
      "Iteration 790, loss = 0.00109282\n",
      "Iteration 791, loss = 0.00109042\n",
      "Iteration 792, loss = 0.00108805\n",
      "Iteration 793, loss = 0.00108571\n",
      "Iteration 794, loss = 0.00108344\n",
      "Iteration 795, loss = 0.00108113\n",
      "Iteration 796, loss = 0.00107878\n",
      "Iteration 797, loss = 0.00107645\n",
      "Iteration 798, loss = 0.00107424\n",
      "Iteration 799, loss = 0.00107191\n",
      "Iteration 800, loss = 0.00106967\n",
      "Iteration 801, loss = 0.00106740\n",
      "Iteration 802, loss = 0.00106520\n",
      "Iteration 803, loss = 0.00106289\n",
      "Iteration 804, loss = 0.00106063\n",
      "Iteration 805, loss = 0.00105845\n",
      "Iteration 806, loss = 0.00105616\n",
      "Iteration 807, loss = 0.00105399\n",
      "Iteration 808, loss = 0.00105175\n",
      "Iteration 809, loss = 0.00104960\n",
      "Iteration 810, loss = 0.00104741\n",
      "Iteration 811, loss = 0.00104528\n",
      "Iteration 812, loss = 0.00104299\n",
      "Iteration 813, loss = 0.00104090\n",
      "Iteration 814, loss = 0.00103873\n",
      "Iteration 815, loss = 0.00103658\n",
      "Iteration 816, loss = 0.00103442\n",
      "Iteration 817, loss = 0.00103230\n",
      "Iteration 818, loss = 0.00103011\n",
      "Iteration 819, loss = 0.00102805\n",
      "Iteration 820, loss = 0.00102600\n",
      "Iteration 821, loss = 0.00102395\n",
      "Iteration 822, loss = 0.00102179\n",
      "Iteration 823, loss = 0.00101971\n",
      "Iteration 824, loss = 0.00101761\n",
      "Iteration 825, loss = 0.00101554\n",
      "Iteration 826, loss = 0.00101343\n",
      "Iteration 827, loss = 0.00101143\n",
      "Iteration 828, loss = 0.00100934\n",
      "Iteration 829, loss = 0.00100735\n",
      "Iteration 830, loss = 0.00100526\n",
      "Iteration 831, loss = 0.00100325\n",
      "Iteration 832, loss = 0.00100121\n",
      "Iteration 833, loss = 0.00099930\n",
      "Iteration 834, loss = 0.00099722\n",
      "Iteration 835, loss = 0.00099522\n",
      "Iteration 836, loss = 0.00099328\n",
      "Iteration 837, loss = 0.00099130\n",
      "Iteration 838, loss = 0.00098932\n",
      "Iteration 839, loss = 0.00098732\n",
      "Iteration 840, loss = 0.00098539\n",
      "Iteration 841, loss = 0.00098351\n",
      "Iteration 842, loss = 0.00098149\n",
      "Iteration 843, loss = 0.00097957\n",
      "Iteration 844, loss = 0.00097762\n",
      "Iteration 845, loss = 0.00097573\n",
      "Iteration 846, loss = 0.00097379\n",
      "Iteration 847, loss = 0.00097191\n",
      "Iteration 848, loss = 0.00096994\n",
      "Iteration 849, loss = 0.00096809\n",
      "Iteration 850, loss = 0.00096623\n",
      "Iteration 851, loss = 0.00096432\n",
      "Iteration 852, loss = 0.00096251\n",
      "Iteration 853, loss = 0.00096058\n",
      "Iteration 854, loss = 0.00095870\n",
      "Iteration 855, loss = 0.00095692\n",
      "Iteration 856, loss = 0.00095500\n",
      "Iteration 857, loss = 0.00095325\n",
      "Iteration 858, loss = 0.00095140\n",
      "Iteration 859, loss = 0.00094956\n",
      "Iteration 860, loss = 0.00094768\n",
      "Iteration 861, loss = 0.00094596\n",
      "Iteration 862, loss = 0.00094405\n",
      "Iteration 863, loss = 0.00094241\n",
      "Iteration 864, loss = 0.00094050\n",
      "Iteration 865, loss = 0.00093871\n",
      "Iteration 866, loss = 0.00093690\n",
      "Iteration 867, loss = 0.00093514\n",
      "Iteration 868, loss = 0.00093342\n",
      "Iteration 869, loss = 0.00093160\n",
      "Iteration 870, loss = 0.00092988\n",
      "Iteration 871, loss = 0.00092805\n",
      "Iteration 872, loss = 0.00092637\n",
      "Iteration 873, loss = 0.00092463\n",
      "Iteration 874, loss = 0.00092286\n",
      "Iteration 875, loss = 0.00092116\n",
      "Iteration 876, loss = 0.00091945\n",
      "Iteration 877, loss = 0.00091771\n",
      "Iteration 878, loss = 0.00091606\n",
      "Iteration 879, loss = 0.00091436\n",
      "Iteration 880, loss = 0.00091263\n",
      "Iteration 881, loss = 0.00091095\n",
      "Iteration 882, loss = 0.00090921\n",
      "Iteration 883, loss = 0.00090763\n",
      "Iteration 884, loss = 0.00090595\n",
      "Iteration 885, loss = 0.00090422\n",
      "Iteration 886, loss = 0.00090258\n",
      "Iteration 887, loss = 0.00090102\n",
      "Iteration 888, loss = 0.00089934\n",
      "Iteration 889, loss = 0.00089768\n",
      "Iteration 890, loss = 0.00089599\n",
      "Iteration 891, loss = 0.00089435\n",
      "Iteration 892, loss = 0.00089285\n",
      "Iteration 893, loss = 0.00089115\n",
      "Iteration 894, loss = 0.00088955\n",
      "Iteration 895, loss = 0.00088791\n",
      "Iteration 896, loss = 0.00088631\n",
      "Iteration 897, loss = 0.00088476\n",
      "Iteration 898, loss = 0.00088311\n",
      "Iteration 899, loss = 0.00088156\n",
      "Iteration 900, loss = 0.00087989\n",
      "Iteration 901, loss = 0.00087834\n",
      "Iteration 902, loss = 0.00087678\n",
      "Iteration 903, loss = 0.00087518\n",
      "Iteration 904, loss = 0.00087363\n",
      "Iteration 905, loss = 0.00087211\n",
      "Iteration 906, loss = 0.00087050\n",
      "Iteration 907, loss = 0.00086893\n",
      "Iteration 908, loss = 0.00086740\n",
      "Iteration 909, loss = 0.00086589\n",
      "Iteration 910, loss = 0.00086436\n",
      "Iteration 911, loss = 0.00086278\n",
      "Iteration 912, loss = 0.00086132\n",
      "Iteration 913, loss = 0.00085978\n",
      "Iteration 914, loss = 0.00085827\n",
      "Iteration 915, loss = 0.00085674\n",
      "Iteration 916, loss = 0.00085524\n",
      "Iteration 917, loss = 0.00085378\n",
      "Iteration 918, loss = 0.00085230\n",
      "Iteration 919, loss = 0.00085077\n",
      "Iteration 920, loss = 0.00084930\n",
      "Iteration 921, loss = 0.00084780\n",
      "Iteration 922, loss = 0.00084631\n",
      "Iteration 923, loss = 0.00084488\n",
      "Iteration 924, loss = 0.00084341\n",
      "Iteration 925, loss = 0.00084201\n",
      "Iteration 926, loss = 0.00084047\n",
      "Iteration 927, loss = 0.00083902\n",
      "Iteration 928, loss = 0.00083756\n",
      "Iteration 929, loss = 0.00083618\n",
      "Iteration 930, loss = 0.00083468\n",
      "Iteration 931, loss = 0.00083325\n",
      "Iteration 932, loss = 0.00083185\n",
      "Iteration 933, loss = 0.00083045\n",
      "Iteration 934, loss = 0.00082903\n",
      "Iteration 935, loss = 0.00082759\n",
      "Iteration 936, loss = 0.00082615\n",
      "Iteration 937, loss = 0.00082481\n",
      "Iteration 938, loss = 0.00082338\n",
      "Iteration 939, loss = 0.00082194\n",
      "Iteration 940, loss = 0.00082065\n",
      "Iteration 941, loss = 0.00081918\n",
      "Iteration 942, loss = 0.00081777\n",
      "Iteration 943, loss = 0.00081646\n",
      "Iteration 944, loss = 0.00081508\n",
      "Iteration 945, loss = 0.00081372\n",
      "Iteration 946, loss = 0.00081231\n",
      "Iteration 947, loss = 0.00081096\n",
      "Iteration 948, loss = 0.00080965\n",
      "Iteration 949, loss = 0.00080827\n",
      "Iteration 950, loss = 0.00080695\n",
      "Iteration 951, loss = 0.00080558\n",
      "Iteration 952, loss = 0.00080425\n",
      "Iteration 953, loss = 0.00080292\n",
      "Iteration 954, loss = 0.00080160\n",
      "Iteration 955, loss = 0.00080027\n",
      "Iteration 956, loss = 0.00079897\n",
      "Iteration 957, loss = 0.00079760\n",
      "Iteration 958, loss = 0.00079627\n",
      "Iteration 959, loss = 0.00079498\n",
      "Iteration 960, loss = 0.00079367\n",
      "Iteration 961, loss = 0.00079240\n",
      "Iteration 962, loss = 0.00079109\n",
      "Iteration 963, loss = 0.00078980\n",
      "Iteration 964, loss = 0.00078856\n",
      "Iteration 965, loss = 0.00078718\n",
      "Iteration 966, loss = 0.00078599\n",
      "Iteration 967, loss = 0.00078466\n",
      "Iteration 968, loss = 0.00078341\n",
      "Iteration 969, loss = 0.00078214\n",
      "Iteration 970, loss = 0.00078085\n",
      "Iteration 971, loss = 0.00077962\n",
      "Iteration 972, loss = 0.00077834\n",
      "Iteration 973, loss = 0.00077712\n",
      "Iteration 974, loss = 0.00077582\n",
      "Iteration 975, loss = 0.00077462\n",
      "Iteration 976, loss = 0.00077337\n",
      "Iteration 977, loss = 0.00077214\n",
      "Iteration 978, loss = 0.00077093\n",
      "Iteration 979, loss = 0.00076967\n",
      "Iteration 980, loss = 0.00076848\n",
      "Iteration 981, loss = 0.00076721\n",
      "Iteration 982, loss = 0.00076601\n",
      "Iteration 983, loss = 0.00076476\n",
      "Iteration 984, loss = 0.00076356\n",
      "Iteration 985, loss = 0.00076241\n",
      "Iteration 986, loss = 0.00076114\n",
      "Iteration 987, loss = 0.00075996\n",
      "Iteration 988, loss = 0.00075878\n",
      "Iteration 989, loss = 0.00075759\n",
      "Iteration 990, loss = 0.00075640\n",
      "Iteration 991, loss = 0.00075519\n",
      "Iteration 992, loss = 0.00075397\n",
      "Iteration 993, loss = 0.00075285\n",
      "Iteration 994, loss = 0.00075166\n",
      "Iteration 995, loss = 0.00075043\n",
      "Iteration 996, loss = 0.00074930\n",
      "Iteration 997, loss = 0.00074811\n",
      "Iteration 998, loss = 0.00074695\n",
      "Iteration 999, loss = 0.00074581\n",
      "Iteration 1000, loss = 0.00074465\n",
      "Attribute -> Gaussian Prime\n",
      "Accuracy -> 1.0\n",
      "Iteration 1, loss = 0.43974292\n",
      "Iteration 2, loss = 0.27153110\n",
      "Iteration 3, loss = 0.17976928\n",
      "Iteration 4, loss = 0.14258629\n",
      "Iteration 5, loss = 0.12479118\n",
      "Iteration 6, loss = 0.11642435\n",
      "Iteration 7, loss = 0.11128855\n",
      "Iteration 8, loss = 0.10767580\n",
      "Iteration 9, loss = 0.10455290\n",
      "Iteration 10, loss = 0.10186323\n",
      "Iteration 11, loss = 0.09937633\n",
      "Iteration 12, loss = 0.09709534\n",
      "Iteration 13, loss = 0.09496423\n",
      "Iteration 14, loss = 0.09284240\n",
      "Iteration 15, loss = 0.09092498\n",
      "Iteration 16, loss = 0.08894683\n",
      "Iteration 17, loss = 0.08698483\n",
      "Iteration 18, loss = 0.08528605\n",
      "Iteration 19, loss = 0.08335836\n",
      "Iteration 20, loss = 0.08158691\n",
      "Iteration 21, loss = 0.07972515\n",
      "Iteration 22, loss = 0.07810233\n",
      "Iteration 23, loss = 0.07634758\n",
      "Iteration 24, loss = 0.07478952\n",
      "Iteration 25, loss = 0.07325764\n",
      "Iteration 26, loss = 0.07194758\n",
      "Iteration 27, loss = 0.07040862\n",
      "Iteration 28, loss = 0.06892250\n",
      "Iteration 29, loss = 0.06774162\n",
      "Iteration 30, loss = 0.06648414\n",
      "Iteration 31, loss = 0.06504427\n",
      "Iteration 32, loss = 0.06392142\n",
      "Iteration 33, loss = 0.06260661\n",
      "Iteration 34, loss = 0.06150427\n",
      "Iteration 35, loss = 0.06034313\n",
      "Iteration 36, loss = 0.05937247\n",
      "Iteration 37, loss = 0.05817582\n",
      "Iteration 38, loss = 0.05717390\n",
      "Iteration 39, loss = 0.05612833\n",
      "Iteration 40, loss = 0.05525079\n",
      "Iteration 41, loss = 0.05427830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.05346324\n",
      "Iteration 43, loss = 0.05255547\n",
      "Iteration 44, loss = 0.05177871\n",
      "Iteration 45, loss = 0.05101747\n",
      "Iteration 46, loss = 0.05015876\n",
      "Iteration 47, loss = 0.04949387\n",
      "Iteration 48, loss = 0.04873126\n",
      "Iteration 49, loss = 0.04809877\n",
      "Iteration 50, loss = 0.04744610\n",
      "Iteration 51, loss = 0.04690134\n",
      "Iteration 52, loss = 0.04625175\n",
      "Iteration 53, loss = 0.04573108\n",
      "Iteration 54, loss = 0.04514909\n",
      "Iteration 55, loss = 0.04470249\n",
      "Iteration 56, loss = 0.04417198\n",
      "Iteration 57, loss = 0.04384165\n",
      "Iteration 58, loss = 0.04334180\n",
      "Iteration 59, loss = 0.04289540\n",
      "Iteration 60, loss = 0.04260168\n",
      "Iteration 61, loss = 0.04216814\n",
      "Iteration 62, loss = 0.04186211\n",
      "Iteration 63, loss = 0.04148571\n",
      "Iteration 64, loss = 0.04116885\n",
      "Iteration 65, loss = 0.04091863\n",
      "Iteration 66, loss = 0.04070837\n",
      "Iteration 67, loss = 0.04034501\n",
      "Iteration 68, loss = 0.04005812\n",
      "Iteration 69, loss = 0.03984294\n",
      "Iteration 70, loss = 0.03957131\n",
      "Iteration 71, loss = 0.03935285\n",
      "Iteration 72, loss = 0.03913255\n",
      "Iteration 73, loss = 0.03897280\n",
      "Iteration 74, loss = 0.03873572\n",
      "Iteration 75, loss = 0.03855268\n",
      "Iteration 76, loss = 0.03831708\n",
      "Iteration 77, loss = 0.03811893\n",
      "Iteration 78, loss = 0.03797921\n",
      "Iteration 79, loss = 0.03776557\n",
      "Iteration 80, loss = 0.03767486\n",
      "Iteration 81, loss = 0.03756015\n",
      "Iteration 82, loss = 0.03733904\n",
      "Iteration 83, loss = 0.03718010\n",
      "Iteration 84, loss = 0.03699467\n",
      "Iteration 85, loss = 0.03686182\n",
      "Iteration 86, loss = 0.03670635\n",
      "Iteration 87, loss = 0.03656083\n",
      "Iteration 88, loss = 0.03648329\n",
      "Iteration 89, loss = 0.03650307\n",
      "Iteration 90, loss = 0.03621778\n",
      "Iteration 91, loss = 0.03614508\n",
      "Iteration 92, loss = 0.03596849\n",
      "Iteration 93, loss = 0.03593875\n",
      "Iteration 94, loss = 0.03579164\n",
      "Iteration 95, loss = 0.03568185\n",
      "Iteration 96, loss = 0.03557531\n",
      "Iteration 97, loss = 0.03552369\n",
      "Iteration 98, loss = 0.03538673\n",
      "Iteration 99, loss = 0.03527256\n",
      "Iteration 100, loss = 0.03534862\n",
      "Iteration 101, loss = 0.03503550\n",
      "Iteration 102, loss = 0.03499789\n",
      "Iteration 103, loss = 0.03497894\n",
      "Iteration 104, loss = 0.03494834\n",
      "Iteration 105, loss = 0.03472430\n",
      "Iteration 106, loss = 0.03473393\n",
      "Iteration 107, loss = 0.03457858\n",
      "Iteration 108, loss = 0.03444127\n",
      "Iteration 109, loss = 0.03440469\n",
      "Iteration 110, loss = 0.03437922\n",
      "Iteration 111, loss = 0.03421067\n",
      "Iteration 112, loss = 0.03411281\n",
      "Iteration 113, loss = 0.03405360\n",
      "Iteration 114, loss = 0.03401848\n",
      "Iteration 115, loss = 0.03404011\n",
      "Iteration 116, loss = 0.03380971\n",
      "Iteration 117, loss = 0.03392215\n",
      "Iteration 118, loss = 0.03371725\n",
      "Iteration 119, loss = 0.03367240\n",
      "Iteration 120, loss = 0.03361717\n",
      "Iteration 121, loss = 0.03354412\n",
      "Iteration 122, loss = 0.03354153\n",
      "Iteration 123, loss = 0.03344206\n",
      "Iteration 124, loss = 0.03343601\n",
      "Iteration 125, loss = 0.03333638\n",
      "Iteration 126, loss = 0.03326249\n",
      "Iteration 127, loss = 0.03321728\n",
      "Iteration 128, loss = 0.03317938\n",
      "Iteration 129, loss = 0.03314571\n",
      "Iteration 130, loss = 0.03313638\n",
      "Iteration 131, loss = 0.03307892\n",
      "Iteration 132, loss = 0.03297842\n",
      "Iteration 133, loss = 0.03309579\n",
      "Iteration 134, loss = 0.03289996\n",
      "Iteration 135, loss = 0.03280569\n",
      "Iteration 136, loss = 0.03285774\n",
      "Iteration 137, loss = 0.03289228\n",
      "Iteration 138, loss = 0.03275409\n",
      "Iteration 139, loss = 0.03268163\n",
      "Iteration 140, loss = 0.03268128\n",
      "Iteration 141, loss = 0.03270312\n",
      "Iteration 142, loss = 0.03262029\n",
      "Iteration 143, loss = 0.03248564\n",
      "Iteration 144, loss = 0.03252638\n",
      "Iteration 145, loss = 0.03241955\n",
      "Iteration 146, loss = 0.03236385\n",
      "Iteration 147, loss = 0.03226199\n",
      "Iteration 148, loss = 0.03230940\n",
      "Iteration 149, loss = 0.03220186\n",
      "Iteration 150, loss = 0.03219216\n",
      "Iteration 151, loss = 0.03208719\n",
      "Iteration 152, loss = 0.03204814\n",
      "Iteration 153, loss = 0.03208865\n",
      "Iteration 154, loss = 0.03204717\n",
      "Iteration 155, loss = 0.03196100\n",
      "Iteration 156, loss = 0.03199581\n",
      "Iteration 157, loss = 0.03192550\n",
      "Iteration 158, loss = 0.03188032\n",
      "Iteration 159, loss = 0.03177280\n",
      "Iteration 160, loss = 0.03173001\n",
      "Iteration 161, loss = 0.03168465\n",
      "Iteration 162, loss = 0.03168736\n",
      "Iteration 163, loss = 0.03155765\n",
      "Iteration 164, loss = 0.03160538\n",
      "Iteration 165, loss = 0.03153113\n",
      "Iteration 166, loss = 0.03148989\n",
      "Iteration 167, loss = 0.03151840\n",
      "Iteration 168, loss = 0.03148599\n",
      "Iteration 169, loss = 0.03135455\n",
      "Iteration 170, loss = 0.03128084\n",
      "Iteration 171, loss = 0.03125840\n",
      "Iteration 172, loss = 0.03122758\n",
      "Iteration 173, loss = 0.03127783\n",
      "Iteration 174, loss = 0.03129952\n",
      "Iteration 175, loss = 0.03126264\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Attribute -> Highly Composite\n",
      "Accuracy -> 0.998046875\n",
      "Iteration 1, loss = 1.75850363\n",
      "Iteration 2, loss = 1.63017662\n",
      "Iteration 3, loss = 1.57442999\n",
      "Iteration 4, loss = 1.52824312\n",
      "Iteration 5, loss = 1.48595114\n",
      "Iteration 6, loss = 1.45277945\n",
      "Iteration 7, loss = 1.42213173\n",
      "Iteration 8, loss = 1.39274183\n",
      "Iteration 9, loss = 1.36585990\n",
      "Iteration 10, loss = 1.33915174\n",
      "Iteration 11, loss = 1.31322153\n",
      "Iteration 12, loss = 1.28758466\n",
      "Iteration 13, loss = 1.26183874\n",
      "Iteration 14, loss = 1.23660050\n",
      "Iteration 15, loss = 1.21149981\n",
      "Iteration 16, loss = 1.18556124\n",
      "Iteration 17, loss = 1.16077067\n",
      "Iteration 18, loss = 1.13533375\n",
      "Iteration 19, loss = 1.11123755\n",
      "Iteration 20, loss = 1.08783578\n",
      "Iteration 21, loss = 1.06561332\n",
      "Iteration 22, loss = 1.04388666\n",
      "Iteration 23, loss = 1.02293106\n",
      "Iteration 24, loss = 1.00218969\n",
      "Iteration 25, loss = 0.98267122\n",
      "Iteration 26, loss = 0.96332742\n",
      "Iteration 27, loss = 0.94484655\n",
      "Iteration 28, loss = 0.92669075\n",
      "Iteration 29, loss = 0.90956411\n",
      "Iteration 30, loss = 0.89188905\n",
      "Iteration 31, loss = 0.87496633\n",
      "Iteration 32, loss = 0.85845352\n",
      "Iteration 33, loss = 0.84227711\n",
      "Iteration 34, loss = 0.82670214\n",
      "Iteration 35, loss = 0.81154723\n",
      "Iteration 36, loss = 0.79748001\n",
      "Iteration 37, loss = 0.78267044\n",
      "Iteration 38, loss = 0.76848822\n",
      "Iteration 39, loss = 0.75447662\n",
      "Iteration 40, loss = 0.74102745\n",
      "Iteration 41, loss = 0.72733960\n",
      "Iteration 42, loss = 0.71417488\n",
      "Iteration 43, loss = 0.70175737\n",
      "Iteration 44, loss = 0.68829646\n",
      "Iteration 45, loss = 0.67594864\n",
      "Iteration 46, loss = 0.66388259\n",
      "Iteration 47, loss = 0.65143110\n",
      "Iteration 48, loss = 0.64023169\n",
      "Iteration 49, loss = 0.62844062\n",
      "Iteration 50, loss = 0.61671301\n",
      "Iteration 51, loss = 0.60563054\n",
      "Iteration 52, loss = 0.59491447\n",
      "Iteration 53, loss = 0.58356443\n",
      "Iteration 54, loss = 0.57317971\n",
      "Iteration 55, loss = 0.56264012\n",
      "Iteration 56, loss = 0.55266901\n",
      "Iteration 57, loss = 0.54249151\n",
      "Iteration 58, loss = 0.53295171\n",
      "Iteration 59, loss = 0.52345513\n",
      "Iteration 60, loss = 0.51420157\n",
      "Iteration 61, loss = 0.50479039\n",
      "Iteration 62, loss = 0.49552709\n",
      "Iteration 63, loss = 0.48681698\n",
      "Iteration 64, loss = 0.47841163\n",
      "Iteration 65, loss = 0.47027090\n",
      "Iteration 66, loss = 0.46107615\n",
      "Iteration 67, loss = 0.45327408\n",
      "Iteration 68, loss = 0.44543176\n",
      "Iteration 69, loss = 0.43801687\n",
      "Iteration 70, loss = 0.42967348\n",
      "Iteration 71, loss = 0.42283937\n",
      "Iteration 72, loss = 0.41513747\n",
      "Iteration 73, loss = 0.40842593\n",
      "Iteration 74, loss = 0.40076994\n",
      "Iteration 75, loss = 0.39390290\n",
      "Iteration 76, loss = 0.38781005\n",
      "Iteration 77, loss = 0.38072635\n",
      "Iteration 78, loss = 0.37453767\n",
      "Iteration 79, loss = 0.36814614\n",
      "Iteration 80, loss = 0.36178600\n",
      "Iteration 81, loss = 0.35587875\n",
      "Iteration 82, loss = 0.35034887\n",
      "Iteration 83, loss = 0.34509139\n",
      "Iteration 84, loss = 0.33836250\n",
      "Iteration 85, loss = 0.33344821\n",
      "Iteration 86, loss = 0.32772132\n",
      "Iteration 87, loss = 0.32240495\n",
      "Iteration 88, loss = 0.31772294\n",
      "Iteration 89, loss = 0.31301611\n",
      "Iteration 90, loss = 0.30743421\n",
      "Iteration 91, loss = 0.30241637\n",
      "Iteration 92, loss = 0.29720563\n",
      "Iteration 93, loss = 0.29309589\n",
      "Iteration 94, loss = 0.28880361\n",
      "Iteration 95, loss = 0.28378791\n",
      "Iteration 96, loss = 0.27933261\n",
      "Iteration 97, loss = 0.27520216\n",
      "Iteration 98, loss = 0.27100806\n",
      "Iteration 99, loss = 0.26668302\n",
      "Iteration 100, loss = 0.26235507\n",
      "Iteration 101, loss = 0.25941107\n",
      "Iteration 102, loss = 0.25600432\n",
      "Iteration 103, loss = 0.25058848\n",
      "Iteration 104, loss = 0.24645392\n",
      "Iteration 105, loss = 0.24267857\n",
      "Iteration 106, loss = 0.23909745\n",
      "Iteration 107, loss = 0.23516926\n",
      "Iteration 108, loss = 0.23191301\n",
      "Iteration 109, loss = 0.22857796\n",
      "Iteration 110, loss = 0.22473787\n",
      "Iteration 111, loss = 0.22148994\n",
      "Iteration 112, loss = 0.21812202\n",
      "Iteration 113, loss = 0.21523620\n",
      "Iteration 114, loss = 0.21182659\n",
      "Iteration 115, loss = 0.20925440\n",
      "Iteration 116, loss = 0.20541709\n",
      "Iteration 117, loss = 0.20221183\n",
      "Iteration 118, loss = 0.19994127\n",
      "Iteration 119, loss = 0.19624277\n",
      "Iteration 120, loss = 0.19383583\n",
      "Iteration 121, loss = 0.19101805\n",
      "Iteration 122, loss = 0.18839401\n",
      "Iteration 123, loss = 0.18489081\n",
      "Iteration 124, loss = 0.18248319\n",
      "Iteration 125, loss = 0.17941761\n",
      "Iteration 126, loss = 0.17703731\n",
      "Iteration 127, loss = 0.17432236\n",
      "Iteration 128, loss = 0.17130982\n",
      "Iteration 129, loss = 0.16843172\n",
      "Iteration 130, loss = 0.16709436\n",
      "Iteration 131, loss = 0.16364472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 132, loss = 0.16120224\n",
      "Iteration 133, loss = 0.15913873\n",
      "Iteration 134, loss = 0.15658568\n",
      "Iteration 135, loss = 0.15427807\n",
      "Iteration 136, loss = 0.15290322\n",
      "Iteration 137, loss = 0.15035664\n",
      "Iteration 138, loss = 0.14881808\n",
      "Iteration 139, loss = 0.14609241\n",
      "Iteration 140, loss = 0.14430954\n",
      "Iteration 141, loss = 0.14267250\n",
      "Iteration 142, loss = 0.14126974\n",
      "Iteration 143, loss = 0.13941060\n",
      "Iteration 144, loss = 0.13681337\n",
      "Iteration 145, loss = 0.13512975\n",
      "Iteration 146, loss = 0.13401967\n",
      "Iteration 147, loss = 0.13188459\n",
      "Iteration 148, loss = 0.12996819\n",
      "Iteration 149, loss = 0.12812309\n",
      "Iteration 150, loss = 0.12584418\n",
      "Iteration 151, loss = 0.12408046\n",
      "Iteration 152, loss = 0.12309437\n",
      "Iteration 153, loss = 0.12121277\n",
      "Iteration 154, loss = 0.11939779\n",
      "Iteration 155, loss = 0.11781482\n",
      "Iteration 156, loss = 0.11736787\n",
      "Iteration 157, loss = 0.11499813\n",
      "Iteration 158, loss = 0.11473749\n",
      "Iteration 159, loss = 0.11245962\n",
      "Iteration 160, loss = 0.11034755\n",
      "Iteration 161, loss = 0.10985358\n",
      "Iteration 162, loss = 0.10785009\n",
      "Iteration 163, loss = 0.10603544\n",
      "Iteration 164, loss = 0.10535872\n",
      "Iteration 165, loss = 0.10385927\n",
      "Iteration 166, loss = 0.10293209\n",
      "Iteration 167, loss = 0.10111727\n",
      "Iteration 168, loss = 0.09946456\n",
      "Iteration 169, loss = 0.10044438\n",
      "Iteration 170, loss = 0.09837607\n",
      "Iteration 171, loss = 0.09603410\n",
      "Iteration 172, loss = 0.09568328\n",
      "Iteration 173, loss = 0.09376796\n",
      "Iteration 174, loss = 0.09296396\n",
      "Iteration 175, loss = 0.09136824\n",
      "Iteration 176, loss = 0.09007216\n",
      "Iteration 177, loss = 0.08934175\n",
      "Iteration 178, loss = 0.08767425\n",
      "Iteration 179, loss = 0.08718109\n",
      "Iteration 180, loss = 0.08602846\n",
      "Iteration 181, loss = 0.08475172\n",
      "Iteration 182, loss = 0.08408110\n",
      "Iteration 183, loss = 0.08233721\n",
      "Iteration 184, loss = 0.08139166\n",
      "Iteration 185, loss = 0.08124280\n",
      "Iteration 186, loss = 0.08034684\n",
      "Iteration 187, loss = 0.07839831\n",
      "Iteration 188, loss = 0.07792319\n",
      "Iteration 189, loss = 0.07661102\n",
      "Iteration 190, loss = 0.07611136\n",
      "Iteration 191, loss = 0.07453764\n",
      "Iteration 192, loss = 0.07401422\n",
      "Iteration 193, loss = 0.07284890\n",
      "Iteration 194, loss = 0.07224849\n",
      "Iteration 195, loss = 0.07099275\n",
      "Iteration 196, loss = 0.07023148\n",
      "Iteration 197, loss = 0.06984850\n",
      "Iteration 198, loss = 0.06999240\n",
      "Iteration 199, loss = 0.06891333\n",
      "Iteration 200, loss = 0.06776130\n",
      "Iteration 201, loss = 0.06636070\n",
      "Iteration 202, loss = 0.06560112\n",
      "Iteration 203, loss = 0.06479809\n",
      "Iteration 204, loss = 0.06442440\n",
      "Iteration 205, loss = 0.06376131\n",
      "Iteration 206, loss = 0.06225989\n",
      "Iteration 207, loss = 0.06237681\n",
      "Iteration 208, loss = 0.06133981\n",
      "Iteration 209, loss = 0.06144179\n",
      "Iteration 210, loss = 0.06019248\n",
      "Iteration 211, loss = 0.06036636\n",
      "Iteration 212, loss = 0.05860670\n",
      "Iteration 213, loss = 0.05827209\n",
      "Iteration 214, loss = 0.05741371\n",
      "Iteration 215, loss = 0.05685841\n",
      "Iteration 216, loss = 0.05612785\n",
      "Iteration 217, loss = 0.05568497\n",
      "Iteration 218, loss = 0.05540033\n",
      "Iteration 219, loss = 0.05414313\n",
      "Iteration 220, loss = 0.05323207\n",
      "Iteration 221, loss = 0.05329742\n",
      "Iteration 222, loss = 0.05331631\n",
      "Iteration 223, loss = 0.05162054\n",
      "Iteration 224, loss = 0.05159448\n",
      "Iteration 225, loss = 0.05101146\n",
      "Iteration 226, loss = 0.05026263\n",
      "Iteration 227, loss = 0.04951799\n",
      "Iteration 228, loss = 0.04923247\n",
      "Iteration 229, loss = 0.04831606\n",
      "Iteration 230, loss = 0.04777712\n",
      "Iteration 231, loss = 0.04743561\n",
      "Iteration 232, loss = 0.04736690\n",
      "Iteration 233, loss = 0.04674891\n",
      "Iteration 234, loss = 0.04579560\n",
      "Iteration 235, loss = 0.04527626\n",
      "Iteration 236, loss = 0.04526383\n",
      "Iteration 237, loss = 0.04452158\n",
      "Iteration 238, loss = 0.04389449\n",
      "Iteration 239, loss = 0.04352318\n",
      "Iteration 240, loss = 0.04333702\n",
      "Iteration 241, loss = 0.04247588\n",
      "Iteration 242, loss = 0.04222291\n",
      "Iteration 243, loss = 0.04150437\n",
      "Iteration 244, loss = 0.04111895\n",
      "Iteration 245, loss = 0.04055814\n",
      "Iteration 246, loss = 0.04017626\n",
      "Iteration 247, loss = 0.04015307\n",
      "Iteration 248, loss = 0.03952107\n",
      "Iteration 249, loss = 0.03957618\n",
      "Iteration 250, loss = 0.03928452\n",
      "Iteration 251, loss = 0.03841697\n",
      "Iteration 252, loss = 0.03823946\n",
      "Iteration 253, loss = 0.03770567\n",
      "Iteration 254, loss = 0.03707854\n",
      "Iteration 255, loss = 0.03691174\n",
      "Iteration 256, loss = 0.03659713\n",
      "Iteration 257, loss = 0.03668897\n",
      "Iteration 258, loss = 0.03582159\n",
      "Iteration 259, loss = 0.03552956\n",
      "Iteration 260, loss = 0.03534640\n",
      "Iteration 261, loss = 0.03535883\n",
      "Iteration 262, loss = 0.03567767\n",
      "Iteration 263, loss = 0.03454697\n",
      "Iteration 264, loss = 0.03372097\n",
      "Iteration 265, loss = 0.03368546\n",
      "Iteration 266, loss = 0.03332870\n",
      "Iteration 267, loss = 0.03329818\n",
      "Iteration 268, loss = 0.03314658\n",
      "Iteration 269, loss = 0.03214118\n",
      "Iteration 270, loss = 0.03199186\n",
      "Iteration 271, loss = 0.03184338\n",
      "Iteration 272, loss = 0.03125835\n",
      "Iteration 273, loss = 0.03103253\n",
      "Iteration 274, loss = 0.03080427\n",
      "Iteration 275, loss = 0.03100134\n",
      "Iteration 276, loss = 0.02996175\n",
      "Iteration 277, loss = 0.02961577\n",
      "Iteration 278, loss = 0.02931906\n",
      "Iteration 279, loss = 0.02901653\n",
      "Iteration 280, loss = 0.02941425\n",
      "Iteration 281, loss = 0.02869770\n",
      "Iteration 282, loss = 0.02843784\n",
      "Iteration 283, loss = 0.02809476\n",
      "Iteration 284, loss = 0.02800115\n",
      "Iteration 285, loss = 0.02754216\n",
      "Iteration 286, loss = 0.02723480\n",
      "Iteration 287, loss = 0.02710428\n",
      "Iteration 288, loss = 0.02681972\n",
      "Iteration 289, loss = 0.02655965\n",
      "Iteration 290, loss = 0.02630386\n",
      "Iteration 291, loss = 0.02633862\n",
      "Iteration 292, loss = 0.02587059\n",
      "Iteration 293, loss = 0.02575401\n",
      "Iteration 294, loss = 0.02552677\n",
      "Iteration 295, loss = 0.02548666\n",
      "Iteration 296, loss = 0.02500823\n",
      "Iteration 297, loss = 0.02473278\n",
      "Iteration 298, loss = 0.02486948\n",
      "Iteration 299, loss = 0.02462260\n",
      "Iteration 300, loss = 0.02462520\n",
      "Iteration 301, loss = 0.02410688\n",
      "Iteration 302, loss = 0.02381820\n",
      "Iteration 303, loss = 0.02387924\n",
      "Iteration 304, loss = 0.02374724\n",
      "Iteration 305, loss = 0.02354872\n",
      "Iteration 306, loss = 0.02289467\n",
      "Iteration 307, loss = 0.02256428\n",
      "Iteration 308, loss = 0.02251797\n",
      "Iteration 309, loss = 0.02261437\n",
      "Iteration 310, loss = 0.02243865\n",
      "Iteration 311, loss = 0.02226458\n",
      "Iteration 312, loss = 0.02173642\n",
      "Iteration 313, loss = 0.02147451\n",
      "Iteration 314, loss = 0.02136308\n",
      "Iteration 315, loss = 0.02127441\n",
      "Iteration 316, loss = 0.02108526\n",
      "Iteration 317, loss = 0.02104867\n",
      "Iteration 318, loss = 0.02086350\n",
      "Iteration 319, loss = 0.02110334\n",
      "Iteration 320, loss = 0.02058863\n",
      "Iteration 321, loss = 0.02029441\n",
      "Iteration 322, loss = 0.02004243\n",
      "Iteration 323, loss = 0.02015890\n",
      "Iteration 324, loss = 0.02006033\n",
      "Iteration 325, loss = 0.02012326\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Attribute -> Mod 5\n",
      "Accuracy -> 1.0\n",
      "Iteration 1, loss = 1.74983975\n",
      "Iteration 2, loss = 1.68259620\n",
      "Iteration 3, loss = 1.60181135\n",
      "Iteration 4, loss = 1.52948728\n",
      "Iteration 5, loss = 1.46530389\n",
      "Iteration 6, loss = 1.40934332\n",
      "Iteration 7, loss = 1.35673594\n",
      "Iteration 8, loss = 1.30755020\n",
      "Iteration 9, loss = 1.26245681\n",
      "Iteration 10, loss = 1.21983385\n",
      "Iteration 11, loss = 1.17922596\n",
      "Iteration 12, loss = 1.14014021\n",
      "Iteration 13, loss = 1.10258197\n",
      "Iteration 14, loss = 1.06626502\n",
      "Iteration 15, loss = 1.03119094\n",
      "Iteration 16, loss = 0.99908934\n",
      "Iteration 17, loss = 0.96811111\n",
      "Iteration 18, loss = 0.93942940\n",
      "Iteration 19, loss = 0.91184802\n",
      "Iteration 20, loss = 0.88629296\n",
      "Iteration 21, loss = 0.86270790\n",
      "Iteration 22, loss = 0.84032590\n",
      "Iteration 23, loss = 0.81876647\n",
      "Iteration 24, loss = 0.79871928\n",
      "Iteration 25, loss = 0.77952432\n",
      "Iteration 26, loss = 0.76192455\n",
      "Iteration 27, loss = 0.74528617\n",
      "Iteration 28, loss = 0.72910158\n",
      "Iteration 29, loss = 0.71374178\n",
      "Iteration 30, loss = 0.69926105\n",
      "Iteration 31, loss = 0.68529533\n",
      "Iteration 32, loss = 0.67239427\n",
      "Iteration 33, loss = 0.65914681\n",
      "Iteration 34, loss = 0.64744564\n",
      "Iteration 35, loss = 0.63620306\n",
      "Iteration 36, loss = 0.62493499\n",
      "Iteration 37, loss = 0.61450778\n",
      "Iteration 38, loss = 0.60440710\n",
      "Iteration 39, loss = 0.59428531\n",
      "Iteration 40, loss = 0.58498638\n",
      "Iteration 41, loss = 0.57558227\n",
      "Iteration 42, loss = 0.56688035\n",
      "Iteration 43, loss = 0.55850443\n",
      "Iteration 44, loss = 0.55047833\n",
      "Iteration 45, loss = 0.54264272\n",
      "Iteration 46, loss = 0.53540423\n",
      "Iteration 47, loss = 0.52782591\n",
      "Iteration 48, loss = 0.52078958\n",
      "Iteration 49, loss = 0.51429627\n",
      "Iteration 50, loss = 0.50721285\n",
      "Iteration 51, loss = 0.50067594\n",
      "Iteration 52, loss = 0.49503984\n",
      "Iteration 53, loss = 0.48863513\n",
      "Iteration 54, loss = 0.48267306\n",
      "Iteration 55, loss = 0.47722922\n",
      "Iteration 56, loss = 0.47167894\n",
      "Iteration 57, loss = 0.46593245\n",
      "Iteration 58, loss = 0.46083895\n",
      "Iteration 59, loss = 0.45588900\n",
      "Iteration 60, loss = 0.45033427\n",
      "Iteration 61, loss = 0.44562956\n",
      "Iteration 62, loss = 0.44068443\n",
      "Iteration 63, loss = 0.43597244\n",
      "Iteration 64, loss = 0.43169423\n",
      "Iteration 65, loss = 0.42754222\n",
      "Iteration 66, loss = 0.42352737\n",
      "Iteration 67, loss = 0.41845161\n",
      "Iteration 68, loss = 0.41456793\n",
      "Iteration 69, loss = 0.41044986\n",
      "Iteration 70, loss = 0.40642366\n",
      "Iteration 71, loss = 0.40308382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.39981919\n",
      "Iteration 73, loss = 0.39545923\n",
      "Iteration 74, loss = 0.39153795\n",
      "Iteration 75, loss = 0.38819170\n",
      "Iteration 76, loss = 0.38491041\n",
      "Iteration 77, loss = 0.38200810\n",
      "Iteration 78, loss = 0.37733359\n",
      "Iteration 79, loss = 0.37496202\n",
      "Iteration 80, loss = 0.37170082\n",
      "Iteration 81, loss = 0.36783928\n",
      "Iteration 82, loss = 0.36565888\n",
      "Iteration 83, loss = 0.36113894\n",
      "Iteration 84, loss = 0.35818477\n",
      "Iteration 85, loss = 0.35561054\n",
      "Iteration 86, loss = 0.35262834\n",
      "Iteration 87, loss = 0.34930141\n",
      "Iteration 88, loss = 0.34674750\n",
      "Iteration 89, loss = 0.34348574\n",
      "Iteration 90, loss = 0.34092014\n",
      "Iteration 91, loss = 0.33767948\n",
      "Iteration 92, loss = 0.33549350\n",
      "Iteration 93, loss = 0.33317603\n",
      "Iteration 94, loss = 0.33031317\n",
      "Iteration 95, loss = 0.32736845\n",
      "Iteration 96, loss = 0.32480203\n",
      "Iteration 97, loss = 0.32231080\n",
      "Iteration 98, loss = 0.31934872\n",
      "Iteration 99, loss = 0.31715793\n",
      "Iteration 100, loss = 0.31428153\n",
      "Iteration 101, loss = 0.31198315\n",
      "Iteration 102, loss = 0.31041116\n",
      "Iteration 103, loss = 0.30769957\n",
      "Iteration 104, loss = 0.30528104\n",
      "Iteration 105, loss = 0.30334891\n",
      "Iteration 106, loss = 0.30039400\n",
      "Iteration 107, loss = 0.29751470\n",
      "Iteration 108, loss = 0.29541955\n",
      "Iteration 109, loss = 0.29347574\n",
      "Iteration 110, loss = 0.29107514\n",
      "Iteration 111, loss = 0.28900399\n",
      "Iteration 112, loss = 0.28676009\n",
      "Iteration 113, loss = 0.28375415\n",
      "Iteration 114, loss = 0.28220749\n",
      "Iteration 115, loss = 0.27948812\n",
      "Iteration 116, loss = 0.27820232\n",
      "Iteration 117, loss = 0.27589626\n",
      "Iteration 118, loss = 0.27320827\n",
      "Iteration 119, loss = 0.27108371\n",
      "Iteration 120, loss = 0.26922814\n",
      "Iteration 121, loss = 0.26758981\n",
      "Iteration 122, loss = 0.26536789\n",
      "Iteration 123, loss = 0.26298943\n",
      "Iteration 124, loss = 0.26136792\n",
      "Iteration 125, loss = 0.25912476\n",
      "Iteration 126, loss = 0.25801454\n",
      "Iteration 127, loss = 0.25510955\n",
      "Iteration 128, loss = 0.25362228\n",
      "Iteration 129, loss = 0.25235594\n",
      "Iteration 130, loss = 0.24906892\n",
      "Iteration 131, loss = 0.24752882\n",
      "Iteration 132, loss = 0.24602003\n",
      "Iteration 133, loss = 0.24401023\n",
      "Iteration 134, loss = 0.24281356\n",
      "Iteration 135, loss = 0.24077118\n",
      "Iteration 136, loss = 0.23829872\n",
      "Iteration 137, loss = 0.23646483\n",
      "Iteration 138, loss = 0.23442807\n",
      "Iteration 139, loss = 0.23275956\n",
      "Iteration 140, loss = 0.23119748\n",
      "Iteration 141, loss = 0.22991690\n",
      "Iteration 142, loss = 0.22806883\n",
      "Iteration 143, loss = 0.22601915\n",
      "Iteration 144, loss = 0.22437181\n",
      "Iteration 145, loss = 0.22273879\n",
      "Iteration 146, loss = 0.22088914\n",
      "Iteration 147, loss = 0.21878004\n",
      "Iteration 148, loss = 0.21746536\n",
      "Iteration 149, loss = 0.21575499\n",
      "Iteration 150, loss = 0.21388234\n",
      "Iteration 151, loss = 0.21222865\n",
      "Iteration 152, loss = 0.21038702\n",
      "Iteration 153, loss = 0.20869343\n",
      "Iteration 154, loss = 0.20783264\n",
      "Iteration 155, loss = 0.20618876\n",
      "Iteration 156, loss = 0.20464514\n",
      "Iteration 157, loss = 0.20257368\n",
      "Iteration 158, loss = 0.20056196\n",
      "Iteration 159, loss = 0.19887135\n",
      "Iteration 160, loss = 0.19741933\n",
      "Iteration 161, loss = 0.19648496\n",
      "Iteration 162, loss = 0.19474157\n",
      "Iteration 163, loss = 0.19297270\n",
      "Iteration 164, loss = 0.19166753\n",
      "Iteration 165, loss = 0.19024360\n",
      "Iteration 166, loss = 0.18841123\n",
      "Iteration 167, loss = 0.18685128\n",
      "Iteration 168, loss = 0.18598552\n",
      "Iteration 169, loss = 0.18385932\n",
      "Iteration 170, loss = 0.18271580\n",
      "Iteration 171, loss = 0.18072321\n",
      "Iteration 172, loss = 0.17929804\n",
      "Iteration 173, loss = 0.17814270\n",
      "Iteration 174, loss = 0.17635794\n",
      "Iteration 175, loss = 0.17530572\n",
      "Iteration 176, loss = 0.17394605\n",
      "Iteration 177, loss = 0.17224139\n",
      "Iteration 178, loss = 0.17062482\n",
      "Iteration 179, loss = 0.16920867\n",
      "Iteration 180, loss = 0.16861472\n",
      "Iteration 181, loss = 0.16664079\n",
      "Iteration 182, loss = 0.16525977\n",
      "Iteration 183, loss = 0.16416277\n",
      "Iteration 184, loss = 0.16233430\n",
      "Iteration 185, loss = 0.16084401\n",
      "Iteration 186, loss = 0.15945221\n",
      "Iteration 187, loss = 0.15806035\n",
      "Iteration 188, loss = 0.15664160\n",
      "Iteration 189, loss = 0.15560264\n",
      "Iteration 190, loss = 0.15413249\n",
      "Iteration 191, loss = 0.15278762\n",
      "Iteration 192, loss = 0.15117298\n",
      "Iteration 193, loss = 0.14976167\n",
      "Iteration 194, loss = 0.14873256\n",
      "Iteration 195, loss = 0.14709087\n",
      "Iteration 196, loss = 0.14626156\n",
      "Iteration 197, loss = 0.14443750\n",
      "Iteration 198, loss = 0.14356780\n",
      "Iteration 199, loss = 0.14223507\n",
      "Iteration 200, loss = 0.14138866\n",
      "Iteration 201, loss = 0.14036654\n",
      "Iteration 202, loss = 0.13906437\n",
      "Iteration 203, loss = 0.13718113\n",
      "Iteration 204, loss = 0.13605356\n",
      "Iteration 205, loss = 0.13473085\n",
      "Iteration 206, loss = 0.13411198\n",
      "Iteration 207, loss = 0.13361576\n",
      "Iteration 208, loss = 0.13128673\n",
      "Iteration 209, loss = 0.13020273\n",
      "Iteration 210, loss = 0.12979201\n",
      "Iteration 211, loss = 0.12806314\n",
      "Iteration 212, loss = 0.12771738\n",
      "Iteration 213, loss = 0.12676602\n",
      "Iteration 214, loss = 0.12523389\n",
      "Iteration 215, loss = 0.12391765\n",
      "Iteration 216, loss = 0.12261248\n",
      "Iteration 217, loss = 0.12194148\n",
      "Iteration 218, loss = 0.12066310\n",
      "Iteration 219, loss = 0.11965731\n",
      "Iteration 220, loss = 0.11851315\n",
      "Iteration 221, loss = 0.11755722\n",
      "Iteration 222, loss = 0.11667906\n",
      "Iteration 223, loss = 0.11561802\n",
      "Iteration 224, loss = 0.11503362\n",
      "Iteration 225, loss = 0.11412254\n",
      "Iteration 226, loss = 0.11364983\n",
      "Iteration 227, loss = 0.11154694\n",
      "Iteration 228, loss = 0.11094935\n",
      "Iteration 229, loss = 0.10972849\n",
      "Iteration 230, loss = 0.10962112\n",
      "Iteration 231, loss = 0.10864477\n",
      "Iteration 232, loss = 0.10751576\n",
      "Iteration 233, loss = 0.10593680\n",
      "Iteration 234, loss = 0.10529863\n",
      "Iteration 235, loss = 0.10440235\n",
      "Iteration 236, loss = 0.10353092\n",
      "Iteration 237, loss = 0.10294092\n",
      "Iteration 238, loss = 0.10159647\n",
      "Iteration 239, loss = 0.10085355\n",
      "Iteration 240, loss = 0.09996275\n",
      "Iteration 241, loss = 0.09940545\n",
      "Iteration 242, loss = 0.09847088\n",
      "Iteration 243, loss = 0.09729278\n",
      "Iteration 244, loss = 0.09669384\n",
      "Iteration 245, loss = 0.09579370\n",
      "Iteration 246, loss = 0.09510531\n",
      "Iteration 247, loss = 0.09435074\n",
      "Iteration 248, loss = 0.09360785\n",
      "Iteration 249, loss = 0.09275548\n",
      "Iteration 250, loss = 0.09202054\n",
      "Iteration 251, loss = 0.09133854\n",
      "Iteration 252, loss = 0.09058168\n",
      "Iteration 253, loss = 0.09050689\n",
      "Iteration 254, loss = 0.08897483\n",
      "Iteration 255, loss = 0.08828403\n",
      "Iteration 256, loss = 0.08745502\n",
      "Iteration 257, loss = 0.08680035\n",
      "Iteration 258, loss = 0.08601492\n",
      "Iteration 259, loss = 0.08576277\n",
      "Iteration 260, loss = 0.08522250\n",
      "Iteration 261, loss = 0.08379568\n",
      "Iteration 262, loss = 0.08344114\n",
      "Iteration 263, loss = 0.08232988\n",
      "Iteration 264, loss = 0.08195983\n",
      "Iteration 265, loss = 0.08121714\n",
      "Iteration 266, loss = 0.08079945\n",
      "Iteration 267, loss = 0.07999796\n",
      "Iteration 268, loss = 0.07943309\n",
      "Iteration 269, loss = 0.07855238\n",
      "Iteration 270, loss = 0.07815265\n",
      "Iteration 271, loss = 0.07731401\n",
      "Iteration 272, loss = 0.07675009\n",
      "Iteration 273, loss = 0.07640838\n",
      "Iteration 274, loss = 0.07613457\n",
      "Iteration 275, loss = 0.07492442\n",
      "Iteration 276, loss = 0.07435507\n",
      "Iteration 277, loss = 0.07356049\n",
      "Iteration 278, loss = 0.07310586\n",
      "Iteration 279, loss = 0.07260234\n",
      "Iteration 280, loss = 0.07214571\n",
      "Iteration 281, loss = 0.07164290\n",
      "Iteration 282, loss = 0.07058758\n",
      "Iteration 283, loss = 0.07017716\n",
      "Iteration 284, loss = 0.06992837\n",
      "Iteration 285, loss = 0.06905545\n",
      "Iteration 286, loss = 0.06844648\n",
      "Iteration 287, loss = 0.06810481\n",
      "Iteration 288, loss = 0.06727821\n",
      "Iteration 289, loss = 0.06673537\n",
      "Iteration 290, loss = 0.06692677\n",
      "Iteration 291, loss = 0.06555512\n",
      "Iteration 292, loss = 0.06559419\n",
      "Iteration 293, loss = 0.06460158\n",
      "Iteration 294, loss = 0.06417915\n",
      "Iteration 295, loss = 0.06382252\n",
      "Iteration 296, loss = 0.06344729\n",
      "Iteration 297, loss = 0.06274600\n",
      "Iteration 298, loss = 0.06259937\n",
      "Iteration 299, loss = 0.06220982\n",
      "Iteration 300, loss = 0.06139653\n",
      "Iteration 301, loss = 0.06093638\n",
      "Iteration 302, loss = 0.06043988\n",
      "Iteration 303, loss = 0.06015761\n",
      "Iteration 304, loss = 0.05983256\n",
      "Iteration 305, loss = 0.05945108\n",
      "Iteration 306, loss = 0.05863140\n",
      "Iteration 307, loss = 0.05833084\n",
      "Iteration 308, loss = 0.05781868\n",
      "Iteration 309, loss = 0.05744828\n",
      "Iteration 310, loss = 0.05712912\n",
      "Iteration 311, loss = 0.05683025\n",
      "Iteration 312, loss = 0.05617952\n",
      "Iteration 313, loss = 0.05598084\n",
      "Iteration 314, loss = 0.05542182\n",
      "Iteration 315, loss = 0.05498705\n",
      "Iteration 316, loss = 0.05480448\n",
      "Iteration 317, loss = 0.05437342\n",
      "Iteration 318, loss = 0.05374410\n",
      "Iteration 319, loss = 0.05356218\n",
      "Iteration 320, loss = 0.05306490\n",
      "Iteration 321, loss = 0.05262282\n",
      "Iteration 322, loss = 0.05223183\n",
      "Iteration 323, loss = 0.05193800\n",
      "Iteration 324, loss = 0.05154444\n",
      "Iteration 325, loss = 0.05118316\n",
      "Iteration 326, loss = 0.05078553\n",
      "Iteration 327, loss = 0.05052337\n",
      "Iteration 328, loss = 0.05004337\n",
      "Iteration 329, loss = 0.04995094\n",
      "Iteration 330, loss = 0.04936571\n",
      "Iteration 331, loss = 0.04897281\n",
      "Iteration 332, loss = 0.04868913\n",
      "Iteration 333, loss = 0.04827935\n",
      "Iteration 334, loss = 0.04802802\n",
      "Iteration 335, loss = 0.04783546\n",
      "Iteration 336, loss = 0.04738482\n",
      "Iteration 337, loss = 0.04733235\n",
      "Iteration 338, loss = 0.04739267\n",
      "Iteration 339, loss = 0.04638698\n",
      "Iteration 340, loss = 0.04617165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 341, loss = 0.04581755\n",
      "Iteration 342, loss = 0.04540354\n",
      "Iteration 343, loss = 0.04519347\n",
      "Iteration 344, loss = 0.04486940\n",
      "Iteration 345, loss = 0.04455787\n",
      "Iteration 346, loss = 0.04442960\n",
      "Iteration 347, loss = 0.04402803\n",
      "Iteration 348, loss = 0.04361330\n",
      "Iteration 349, loss = 0.04341681\n",
      "Iteration 350, loss = 0.04334528\n",
      "Iteration 351, loss = 0.04265230\n",
      "Iteration 352, loss = 0.04252529\n",
      "Iteration 353, loss = 0.04218571\n",
      "Iteration 354, loss = 0.04196136\n",
      "Iteration 355, loss = 0.04156793\n",
      "Iteration 356, loss = 0.04154617\n",
      "Iteration 357, loss = 0.04109674\n",
      "Iteration 358, loss = 0.04086690\n",
      "Iteration 359, loss = 0.04042017\n",
      "Iteration 360, loss = 0.04028716\n",
      "Iteration 361, loss = 0.03996170\n",
      "Iteration 362, loss = 0.03986046\n",
      "Iteration 363, loss = 0.03957929\n",
      "Iteration 364, loss = 0.03925990\n",
      "Iteration 365, loss = 0.03922753\n",
      "Iteration 366, loss = 0.03877000\n",
      "Iteration 367, loss = 0.03847655\n",
      "Iteration 368, loss = 0.03824293\n",
      "Iteration 369, loss = 0.03792112\n",
      "Iteration 370, loss = 0.03767447\n",
      "Iteration 371, loss = 0.03748073\n",
      "Iteration 372, loss = 0.03717638\n",
      "Iteration 373, loss = 0.03726817\n",
      "Iteration 374, loss = 0.03679602\n",
      "Iteration 375, loss = 0.03645395\n",
      "Iteration 376, loss = 0.03623529\n",
      "Iteration 377, loss = 0.03615698\n",
      "Iteration 378, loss = 0.03592564\n",
      "Iteration 379, loss = 0.03559758\n",
      "Iteration 380, loss = 0.03555010\n",
      "Iteration 381, loss = 0.03523827\n",
      "Iteration 382, loss = 0.03485835\n",
      "Iteration 383, loss = 0.03479423\n",
      "Iteration 384, loss = 0.03453585\n",
      "Iteration 385, loss = 0.03426480\n",
      "Iteration 386, loss = 0.03418619\n",
      "Iteration 387, loss = 0.03395805\n",
      "Iteration 388, loss = 0.03389281\n",
      "Iteration 389, loss = 0.03372977\n",
      "Iteration 390, loss = 0.03356812\n",
      "Iteration 391, loss = 0.03318012\n",
      "Iteration 392, loss = 0.03280615\n",
      "Iteration 393, loss = 0.03271755\n",
      "Iteration 394, loss = 0.03248621\n",
      "Iteration 395, loss = 0.03250535\n",
      "Iteration 396, loss = 0.03207519\n",
      "Iteration 397, loss = 0.03205771\n",
      "Iteration 398, loss = 0.03181136\n",
      "Iteration 399, loss = 0.03176417\n",
      "Iteration 400, loss = 0.03142183\n",
      "Iteration 401, loss = 0.03121170\n",
      "Iteration 402, loss = 0.03114054\n",
      "Iteration 403, loss = 0.03076477\n",
      "Iteration 404, loss = 0.03052071\n",
      "Iteration 405, loss = 0.03034647\n",
      "Iteration 406, loss = 0.03018678\n",
      "Iteration 407, loss = 0.03008221\n",
      "Iteration 408, loss = 0.03002847\n",
      "Iteration 409, loss = 0.02972426\n",
      "Iteration 410, loss = 0.02962430\n",
      "Iteration 411, loss = 0.02947712\n",
      "Iteration 412, loss = 0.02920804\n",
      "Iteration 413, loss = 0.02888981\n",
      "Iteration 414, loss = 0.02899446\n",
      "Iteration 415, loss = 0.02865255\n",
      "Iteration 416, loss = 0.02856452\n",
      "Iteration 417, loss = 0.02860303\n",
      "Iteration 418, loss = 0.02835716\n",
      "Iteration 419, loss = 0.02791499\n",
      "Iteration 420, loss = 0.02823796\n",
      "Iteration 421, loss = 0.02779662\n",
      "Iteration 422, loss = 0.02756558\n",
      "Iteration 423, loss = 0.02738716\n",
      "Iteration 424, loss = 0.02730855\n",
      "Iteration 425, loss = 0.02702197\n",
      "Iteration 426, loss = 0.02697898\n",
      "Iteration 427, loss = 0.02684434\n",
      "Iteration 428, loss = 0.02678559\n",
      "Iteration 429, loss = 0.02651867\n",
      "Iteration 430, loss = 0.02629174\n",
      "Iteration 431, loss = 0.02628544\n",
      "Iteration 432, loss = 0.02613294\n",
      "Iteration 433, loss = 0.02591401\n",
      "Iteration 434, loss = 0.02575005\n",
      "Iteration 435, loss = 0.02561052\n",
      "Iteration 436, loss = 0.02543544\n",
      "Iteration 437, loss = 0.02531460\n",
      "Iteration 438, loss = 0.02520981\n",
      "Iteration 439, loss = 0.02508927\n",
      "Iteration 440, loss = 0.02502414\n",
      "Iteration 441, loss = 0.02478730\n",
      "Iteration 442, loss = 0.02478268\n",
      "Iteration 443, loss = 0.02451497\n",
      "Iteration 444, loss = 0.02440795\n",
      "Iteration 445, loss = 0.02427408\n",
      "Iteration 446, loss = 0.02421728\n",
      "Iteration 447, loss = 0.02415150\n",
      "Iteration 448, loss = 0.02397061\n",
      "Iteration 449, loss = 0.02371355\n",
      "Iteration 450, loss = 0.02370798\n",
      "Iteration 451, loss = 0.02370647\n",
      "Iteration 452, loss = 0.02338280\n",
      "Iteration 453, loss = 0.02342598\n",
      "Iteration 454, loss = 0.02316452\n",
      "Iteration 455, loss = 0.02307733\n",
      "Iteration 456, loss = 0.02293230\n",
      "Iteration 457, loss = 0.02274591\n",
      "Iteration 458, loss = 0.02265928\n",
      "Iteration 459, loss = 0.02252505\n",
      "Iteration 460, loss = 0.02241180\n",
      "Iteration 461, loss = 0.02233144\n",
      "Iteration 462, loss = 0.02217990\n",
      "Iteration 463, loss = 0.02213038\n",
      "Iteration 464, loss = 0.02213159\n",
      "Iteration 465, loss = 0.02197925\n",
      "Iteration 466, loss = 0.02189606\n",
      "Iteration 467, loss = 0.02172158\n",
      "Iteration 468, loss = 0.02148009\n",
      "Iteration 469, loss = 0.02143114\n",
      "Iteration 470, loss = 0.02128957\n",
      "Iteration 471, loss = 0.02124883\n",
      "Iteration 472, loss = 0.02118803\n",
      "Iteration 473, loss = 0.02091636\n",
      "Iteration 474, loss = 0.02099027\n",
      "Iteration 475, loss = 0.02079749\n",
      "Iteration 476, loss = 0.02066986\n",
      "Iteration 477, loss = 0.02054961\n",
      "Iteration 478, loss = 0.02055315\n",
      "Iteration 479, loss = 0.02041386\n",
      "Iteration 480, loss = 0.02021369\n",
      "Iteration 481, loss = 0.02017916\n",
      "Iteration 482, loss = 0.02000769\n",
      "Iteration 483, loss = 0.01997703\n",
      "Iteration 484, loss = 0.01999038\n",
      "Iteration 485, loss = 0.01980041\n",
      "Iteration 486, loss = 0.01967848\n",
      "Iteration 487, loss = 0.01956321\n",
      "Iteration 488, loss = 0.01950487\n",
      "Iteration 489, loss = 0.01939278\n",
      "Iteration 490, loss = 0.01930568\n",
      "Iteration 491, loss = 0.01917917\n",
      "Iteration 492, loss = 0.01910128\n",
      "Iteration 493, loss = 0.01901576\n",
      "Iteration 494, loss = 0.01892890\n",
      "Iteration 495, loss = 0.01883850\n",
      "Iteration 496, loss = 0.01871494\n",
      "Iteration 497, loss = 0.01868108\n",
      "Iteration 498, loss = 0.01860626\n",
      "Iteration 499, loss = 0.01845388\n",
      "Iteration 500, loss = 0.01854944\n",
      "Iteration 501, loss = 0.01831590\n",
      "Iteration 502, loss = 0.01822086\n",
      "Iteration 503, loss = 0.01809601\n",
      "Iteration 504, loss = 0.01808530\n",
      "Iteration 505, loss = 0.01799279\n",
      "Iteration 506, loss = 0.01798112\n",
      "Iteration 507, loss = 0.01788359\n",
      "Iteration 508, loss = 0.01774368\n",
      "Iteration 509, loss = 0.01765253\n",
      "Iteration 510, loss = 0.01753874\n",
      "Iteration 511, loss = 0.01749856\n",
      "Iteration 512, loss = 0.01739295\n",
      "Iteration 513, loss = 0.01740527\n",
      "Iteration 514, loss = 0.01722365\n",
      "Iteration 515, loss = 0.01717995\n",
      "Iteration 516, loss = 0.01715808\n",
      "Iteration 517, loss = 0.01703616\n",
      "Iteration 518, loss = 0.01696646\n",
      "Iteration 519, loss = 0.01687848\n",
      "Iteration 520, loss = 0.01676430\n",
      "Iteration 521, loss = 0.01683381\n",
      "Iteration 522, loss = 0.01662598\n",
      "Iteration 523, loss = 0.01663239\n",
      "Iteration 524, loss = 0.01656903\n",
      "Iteration 525, loss = 0.01651130\n",
      "Iteration 526, loss = 0.01640535\n",
      "Iteration 527, loss = 0.01631825\n",
      "Iteration 528, loss = 0.01614937\n",
      "Iteration 529, loss = 0.01612607\n",
      "Iteration 530, loss = 0.01604960\n",
      "Iteration 531, loss = 0.01601179\n",
      "Iteration 532, loss = 0.01600199\n",
      "Iteration 533, loss = 0.01581762\n",
      "Iteration 534, loss = 0.01572002\n",
      "Iteration 535, loss = 0.01572741\n",
      "Iteration 536, loss = 0.01563706\n",
      "Iteration 537, loss = 0.01560120\n",
      "Iteration 538, loss = 0.01558042\n",
      "Iteration 539, loss = 0.01546392\n",
      "Iteration 540, loss = 0.01541886\n",
      "Iteration 541, loss = 0.01539537\n",
      "Iteration 542, loss = 0.01523834\n",
      "Iteration 543, loss = 0.01526005\n",
      "Iteration 544, loss = 0.01521020\n",
      "Iteration 545, loss = 0.01503271\n",
      "Iteration 546, loss = 0.01503128\n",
      "Iteration 547, loss = 0.01495066\n",
      "Iteration 548, loss = 0.01485591\n",
      "Iteration 549, loss = 0.01480217\n",
      "Iteration 550, loss = 0.01483763\n",
      "Iteration 551, loss = 0.01477829\n",
      "Iteration 552, loss = 0.01460844\n",
      "Iteration 553, loss = 0.01459213\n",
      "Iteration 554, loss = 0.01462334\n",
      "Iteration 555, loss = 0.01449788\n",
      "Iteration 556, loss = 0.01441214\n",
      "Iteration 557, loss = 0.01438963\n",
      "Iteration 558, loss = 0.01428099\n",
      "Iteration 559, loss = 0.01424207\n",
      "Iteration 560, loss = 0.01429590\n",
      "Iteration 561, loss = 0.01407346\n",
      "Iteration 562, loss = 0.01412363\n",
      "Iteration 563, loss = 0.01395157\n",
      "Iteration 564, loss = 0.01395480\n",
      "Iteration 565, loss = 0.01389681\n",
      "Iteration 566, loss = 0.01390117\n",
      "Iteration 567, loss = 0.01386503\n",
      "Iteration 568, loss = 0.01392610\n",
      "Iteration 569, loss = 0.01361743\n",
      "Iteration 570, loss = 0.01379433\n",
      "Iteration 571, loss = 0.01353359\n",
      "Iteration 572, loss = 0.01356705\n",
      "Iteration 573, loss = 0.01346895\n",
      "Iteration 574, loss = 0.01341449\n",
      "Iteration 575, loss = 0.01344358\n",
      "Iteration 576, loss = 0.01326875\n",
      "Iteration 577, loss = 0.01330686\n",
      "Iteration 578, loss = 0.01317537\n",
      "Iteration 579, loss = 0.01316165\n",
      "Iteration 580, loss = 0.01308459\n",
      "Iteration 581, loss = 0.01306476\n",
      "Iteration 582, loss = 0.01293187\n",
      "Iteration 583, loss = 0.01293545\n",
      "Iteration 584, loss = 0.01288620\n",
      "Iteration 585, loss = 0.01289877\n",
      "Iteration 586, loss = 0.01274476\n",
      "Iteration 587, loss = 0.01275048\n",
      "Iteration 588, loss = 0.01271722\n",
      "Iteration 589, loss = 0.01266574\n",
      "Iteration 590, loss = 0.01261077\n",
      "Iteration 591, loss = 0.01261399\n",
      "Iteration 592, loss = 0.01255062\n",
      "Iteration 593, loss = 0.01248120\n",
      "Iteration 594, loss = 0.01244928\n",
      "Iteration 595, loss = 0.01242037\n",
      "Iteration 596, loss = 0.01232694\n",
      "Iteration 597, loss = 0.01228307\n",
      "Iteration 598, loss = 0.01238285\n",
      "Iteration 599, loss = 0.01217572\n",
      "Iteration 600, loss = 0.01215202\n",
      "Iteration 601, loss = 0.01205728\n",
      "Iteration 602, loss = 0.01213064\n",
      "Iteration 603, loss = 0.01201913\n",
      "Iteration 604, loss = 0.01195649\n",
      "Iteration 605, loss = 0.01197916\n",
      "Iteration 606, loss = 0.01191270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 607, loss = 0.01188676\n",
      "Iteration 608, loss = 0.01180917\n",
      "Iteration 609, loss = 0.01174238\n",
      "Iteration 610, loss = 0.01169174\n",
      "Iteration 611, loss = 0.01164764\n",
      "Iteration 612, loss = 0.01158537\n",
      "Iteration 613, loss = 0.01159687\n",
      "Iteration 614, loss = 0.01166317\n",
      "Iteration 615, loss = 0.01152665\n",
      "Iteration 616, loss = 0.01154891\n",
      "Iteration 617, loss = 0.01145219\n",
      "Iteration 618, loss = 0.01151101\n",
      "Iteration 619, loss = 0.01137992\n",
      "Iteration 620, loss = 0.01132529\n",
      "Iteration 621, loss = 0.01132041\n",
      "Iteration 622, loss = 0.01118947\n",
      "Iteration 623, loss = 0.01123559\n",
      "Iteration 624, loss = 0.01117995\n",
      "Iteration 625, loss = 0.01114234\n",
      "Iteration 626, loss = 0.01107039\n",
      "Iteration 627, loss = 0.01101791\n",
      "Iteration 628, loss = 0.01100217\n",
      "Iteration 629, loss = 0.01096651\n",
      "Iteration 630, loss = 0.01093443\n",
      "Iteration 631, loss = 0.01086947\n",
      "Iteration 632, loss = 0.01102376\n",
      "Iteration 633, loss = 0.01095715\n",
      "Iteration 634, loss = 0.01074745\n",
      "Iteration 635, loss = 0.01079606\n",
      "Iteration 636, loss = 0.01065111\n",
      "Iteration 637, loss = 0.01067152\n",
      "Iteration 638, loss = 0.01058939\n",
      "Iteration 639, loss = 0.01058780\n",
      "Iteration 640, loss = 0.01056441\n",
      "Iteration 641, loss = 0.01052656\n",
      "Iteration 642, loss = 0.01050689\n",
      "Iteration 643, loss = 0.01045011\n",
      "Iteration 644, loss = 0.01041882\n",
      "Iteration 645, loss = 0.01042064\n",
      "Iteration 646, loss = 0.01031271\n",
      "Iteration 647, loss = 0.01032302\n",
      "Iteration 648, loss = 0.01030343\n",
      "Iteration 649, loss = 0.01022651\n",
      "Iteration 650, loss = 0.01019523\n",
      "Iteration 651, loss = 0.01013052\n",
      "Iteration 652, loss = 0.01014672\n",
      "Iteration 653, loss = 0.01010257\n",
      "Iteration 654, loss = 0.01004077\n",
      "Iteration 655, loss = 0.01001704\n",
      "Iteration 656, loss = 0.00997112\n",
      "Iteration 657, loss = 0.01000770\n",
      "Iteration 658, loss = 0.00997828\n",
      "Iteration 659, loss = 0.00995058\n",
      "Iteration 660, loss = 0.00997925\n",
      "Iteration 661, loss = 0.00983664\n",
      "Iteration 662, loss = 0.00981796\n",
      "Iteration 663, loss = 0.00974055\n",
      "Iteration 664, loss = 0.00986753\n",
      "Iteration 665, loss = 0.00988763\n",
      "Iteration 666, loss = 0.00967239\n",
      "Iteration 667, loss = 0.00971782\n",
      "Iteration 668, loss = 0.00966968\n",
      "Iteration 669, loss = 0.00960108\n",
      "Iteration 670, loss = 0.00955911\n",
      "Iteration 671, loss = 0.00952730\n",
      "Iteration 672, loss = 0.00949304\n",
      "Iteration 673, loss = 0.00942450\n",
      "Iteration 674, loss = 0.00941346\n",
      "Iteration 675, loss = 0.00936540\n",
      "Iteration 676, loss = 0.00939790\n",
      "Iteration 677, loss = 0.00939975\n",
      "Iteration 678, loss = 0.00931995\n",
      "Iteration 679, loss = 0.00937342\n",
      "Iteration 680, loss = 0.00942513\n",
      "Iteration 681, loss = 0.00928080\n",
      "Iteration 682, loss = 0.00929554\n",
      "Iteration 683, loss = 0.00914731\n",
      "Iteration 684, loss = 0.00912729\n",
      "Iteration 685, loss = 0.00910780\n",
      "Iteration 686, loss = 0.00908974\n",
      "Iteration 687, loss = 0.00912506\n",
      "Iteration 688, loss = 0.00905139\n",
      "Iteration 689, loss = 0.00898368\n",
      "Iteration 690, loss = 0.00899399\n",
      "Iteration 691, loss = 0.00893929\n",
      "Iteration 692, loss = 0.00900438\n",
      "Iteration 693, loss = 0.00890547\n",
      "Iteration 694, loss = 0.00884171\n",
      "Iteration 695, loss = 0.00885944\n",
      "Iteration 696, loss = 0.00878025\n",
      "Iteration 697, loss = 0.00879407\n",
      "Iteration 698, loss = 0.00881358\n",
      "Iteration 699, loss = 0.00874123\n",
      "Iteration 700, loss = 0.00868729\n",
      "Iteration 701, loss = 0.00867902\n",
      "Iteration 702, loss = 0.00868672\n",
      "Iteration 703, loss = 0.00862549\n",
      "Iteration 704, loss = 0.00862030\n",
      "Iteration 705, loss = 0.00856246\n",
      "Iteration 706, loss = 0.00853318\n",
      "Iteration 707, loss = 0.00856514\n",
      "Iteration 708, loss = 0.00852936\n",
      "Iteration 709, loss = 0.00846711\n",
      "Iteration 710, loss = 0.00847135\n",
      "Iteration 711, loss = 0.00842903\n",
      "Iteration 712, loss = 0.00844032\n",
      "Iteration 713, loss = 0.00847428\n",
      "Iteration 714, loss = 0.00835546\n",
      "Iteration 715, loss = 0.00839267\n",
      "Iteration 716, loss = 0.00836060\n",
      "Iteration 717, loss = 0.00826069\n",
      "Iteration 718, loss = 0.00827078\n",
      "Iteration 719, loss = 0.00824729\n",
      "Iteration 720, loss = 0.00821413\n",
      "Iteration 721, loss = 0.00827015\n",
      "Iteration 722, loss = 0.00814162\n",
      "Iteration 723, loss = 0.00815773\n",
      "Iteration 724, loss = 0.00816651\n",
      "Iteration 725, loss = 0.00810692\n",
      "Iteration 726, loss = 0.00824340\n",
      "Iteration 727, loss = 0.00803256\n",
      "Iteration 728, loss = 0.00803669\n",
      "Iteration 729, loss = 0.00800497\n",
      "Iteration 730, loss = 0.00796647\n",
      "Iteration 731, loss = 0.00793974\n",
      "Iteration 732, loss = 0.00794841\n",
      "Iteration 733, loss = 0.00788829\n",
      "Iteration 734, loss = 0.00788780\n",
      "Iteration 735, loss = 0.00788454\n",
      "Iteration 736, loss = 0.00782135\n",
      "Iteration 737, loss = 0.00783841\n",
      "Iteration 738, loss = 0.00778212\n",
      "Iteration 739, loss = 0.00777129\n",
      "Iteration 740, loss = 0.00779035\n",
      "Iteration 741, loss = 0.00776136\n",
      "Iteration 742, loss = 0.00767710\n",
      "Iteration 743, loss = 0.00770590\n",
      "Iteration 744, loss = 0.00766647\n",
      "Iteration 745, loss = 0.00764041\n",
      "Iteration 746, loss = 0.00761117\n",
      "Iteration 747, loss = 0.00760778\n",
      "Iteration 748, loss = 0.00759030\n",
      "Iteration 749, loss = 0.00755716\n",
      "Iteration 750, loss = 0.00752414\n",
      "Iteration 751, loss = 0.00758007\n",
      "Iteration 752, loss = 0.00748068\n",
      "Iteration 753, loss = 0.00750030\n",
      "Iteration 754, loss = 0.00744145\n",
      "Iteration 755, loss = 0.00747344\n",
      "Iteration 756, loss = 0.00746859\n",
      "Iteration 757, loss = 0.00739928\n",
      "Iteration 758, loss = 0.00743517\n",
      "Iteration 759, loss = 0.00735190\n",
      "Iteration 760, loss = 0.00734891\n",
      "Iteration 761, loss = 0.00730695\n",
      "Iteration 762, loss = 0.00730724\n",
      "Iteration 763, loss = 0.00737567\n",
      "Iteration 764, loss = 0.00727637\n",
      "Iteration 765, loss = 0.00727386\n",
      "Iteration 766, loss = 0.00727512\n",
      "Iteration 767, loss = 0.00720856\n",
      "Iteration 768, loss = 0.00716686\n",
      "Iteration 769, loss = 0.00716521\n",
      "Iteration 770, loss = 0.00713265\n",
      "Iteration 771, loss = 0.00723444\n",
      "Iteration 772, loss = 0.00711115\n",
      "Iteration 773, loss = 0.00709398\n",
      "Iteration 774, loss = 0.00705679\n",
      "Iteration 775, loss = 0.00705748\n",
      "Iteration 776, loss = 0.00703943\n",
      "Iteration 777, loss = 0.00705422\n",
      "Iteration 778, loss = 0.00697209\n",
      "Iteration 779, loss = 0.00699032\n",
      "Iteration 780, loss = 0.00696107\n",
      "Iteration 781, loss = 0.00696598\n",
      "Iteration 782, loss = 0.00694625\n",
      "Iteration 783, loss = 0.00693139\n",
      "Iteration 784, loss = 0.00693050\n",
      "Iteration 785, loss = 0.00697512\n",
      "Iteration 786, loss = 0.00685846\n",
      "Iteration 787, loss = 0.00685920\n",
      "Iteration 788, loss = 0.00680605\n",
      "Iteration 789, loss = 0.00680444\n",
      "Iteration 790, loss = 0.00677559\n",
      "Iteration 791, loss = 0.00679107\n",
      "Iteration 792, loss = 0.00675805\n",
      "Iteration 793, loss = 0.00674614\n",
      "Iteration 794, loss = 0.00671879\n",
      "Iteration 795, loss = 0.00673972\n",
      "Iteration 796, loss = 0.00666358\n",
      "Iteration 797, loss = 0.00668604\n",
      "Iteration 798, loss = 0.00662546\n",
      "Iteration 799, loss = 0.00661580\n",
      "Iteration 800, loss = 0.00659399\n",
      "Iteration 801, loss = 0.00660717\n",
      "Iteration 802, loss = 0.00658227\n",
      "Iteration 803, loss = 0.00657239\n",
      "Iteration 804, loss = 0.00658812\n",
      "Iteration 805, loss = 0.00651660\n",
      "Iteration 806, loss = 0.00651937\n",
      "Iteration 807, loss = 0.00649256\n",
      "Iteration 808, loss = 0.00649967\n",
      "Iteration 809, loss = 0.00647204\n",
      "Iteration 810, loss = 0.00644276\n",
      "Iteration 811, loss = 0.00644631\n",
      "Iteration 812, loss = 0.00644803\n",
      "Iteration 813, loss = 0.00643479\n",
      "Iteration 814, loss = 0.00638076\n",
      "Iteration 815, loss = 0.00636638\n",
      "Iteration 816, loss = 0.00639258\n",
      "Iteration 817, loss = 0.00631851\n",
      "Iteration 818, loss = 0.00634122\n",
      "Iteration 819, loss = 0.00632549\n",
      "Iteration 820, loss = 0.00628447\n",
      "Iteration 821, loss = 0.00626671\n",
      "Iteration 822, loss = 0.00627110\n",
      "Iteration 823, loss = 0.00623774\n",
      "Iteration 824, loss = 0.00624806\n",
      "Iteration 825, loss = 0.00620545\n",
      "Iteration 826, loss = 0.00623369\n",
      "Iteration 827, loss = 0.00619602\n",
      "Iteration 828, loss = 0.00617293\n",
      "Iteration 829, loss = 0.00616720\n",
      "Iteration 830, loss = 0.00616886\n",
      "Iteration 831, loss = 0.00610578\n",
      "Iteration 832, loss = 0.00611627\n",
      "Iteration 833, loss = 0.00608942\n",
      "Iteration 834, loss = 0.00611115\n",
      "Iteration 835, loss = 0.00610303\n",
      "Iteration 836, loss = 0.00609661\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Attribute -> Mod 6\n",
      "Accuracy -> 0.998046875\n",
      "Iteration 1, loss = 0.46709741\n",
      "Iteration 2, loss = 0.33084621\n",
      "Iteration 3, loss = 0.26056521\n",
      "Iteration 4, loss = 0.23396281\n",
      "Iteration 5, loss = 0.22379838\n",
      "Iteration 6, loss = 0.21687840\n",
      "Iteration 7, loss = 0.21147966\n",
      "Iteration 8, loss = 0.20649496\n",
      "Iteration 9, loss = 0.20153282\n",
      "Iteration 10, loss = 0.19714725\n",
      "Iteration 11, loss = 0.19325577\n",
      "Iteration 12, loss = 0.18923964\n",
      "Iteration 13, loss = 0.18566623\n",
      "Iteration 14, loss = 0.18237289\n",
      "Iteration 15, loss = 0.17895589\n",
      "Iteration 16, loss = 0.17631756\n",
      "Iteration 17, loss = 0.17320954\n",
      "Iteration 18, loss = 0.17063844\n",
      "Iteration 19, loss = 0.16803757\n",
      "Iteration 20, loss = 0.16574318\n",
      "Iteration 21, loss = 0.16347759\n",
      "Iteration 22, loss = 0.16152014\n",
      "Iteration 23, loss = 0.15957487\n",
      "Iteration 24, loss = 0.15769022\n",
      "Iteration 25, loss = 0.15608158\n",
      "Iteration 26, loss = 0.15438353\n",
      "Iteration 27, loss = 0.15282648\n",
      "Iteration 28, loss = 0.15145333\n",
      "Iteration 29, loss = 0.14995361\n",
      "Iteration 30, loss = 0.14862463\n",
      "Iteration 31, loss = 0.14745688\n",
      "Iteration 32, loss = 0.14626579\n",
      "Iteration 33, loss = 0.14513919\n",
      "Iteration 34, loss = 0.14398729\n",
      "Iteration 35, loss = 0.14297775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.14202602\n",
      "Iteration 37, loss = 0.14106565\n",
      "Iteration 38, loss = 0.14020660\n",
      "Iteration 39, loss = 0.13936068\n",
      "Iteration 40, loss = 0.13863325\n",
      "Iteration 41, loss = 0.13787236\n",
      "Iteration 42, loss = 0.13693250\n",
      "Iteration 43, loss = 0.13617416\n",
      "Iteration 44, loss = 0.13550212\n",
      "Iteration 45, loss = 0.13463171\n",
      "Iteration 46, loss = 0.13408115\n",
      "Iteration 47, loss = 0.13320634\n",
      "Iteration 48, loss = 0.13244483\n",
      "Iteration 49, loss = 0.13181459\n",
      "Iteration 50, loss = 0.13103196\n",
      "Iteration 51, loss = 0.13038265\n",
      "Iteration 52, loss = 0.12982863\n",
      "Iteration 53, loss = 0.12907060\n",
      "Iteration 54, loss = 0.12845163\n",
      "Iteration 55, loss = 0.12780732\n",
      "Iteration 56, loss = 0.12717948\n",
      "Iteration 57, loss = 0.12659987\n",
      "Iteration 58, loss = 0.12601880\n",
      "Iteration 59, loss = 0.12541856\n",
      "Iteration 60, loss = 0.12473450\n",
      "Iteration 61, loss = 0.12414587\n",
      "Iteration 62, loss = 0.12361078\n",
      "Iteration 63, loss = 0.12307543\n",
      "Iteration 64, loss = 0.12251548\n",
      "Iteration 65, loss = 0.12185837\n",
      "Iteration 66, loss = 0.12125153\n",
      "Iteration 67, loss = 0.12067877\n",
      "Iteration 68, loss = 0.12017236\n",
      "Iteration 69, loss = 0.11955971\n",
      "Iteration 70, loss = 0.11895461\n",
      "Iteration 71, loss = 0.11836684\n",
      "Iteration 72, loss = 0.11784985\n",
      "Iteration 73, loss = 0.11723789\n",
      "Iteration 74, loss = 0.11687922\n",
      "Iteration 75, loss = 0.11610084\n",
      "Iteration 76, loss = 0.11553469\n",
      "Iteration 77, loss = 0.11503690\n",
      "Iteration 78, loss = 0.11442222\n",
      "Iteration 79, loss = 0.11384803\n",
      "Iteration 80, loss = 0.11326773\n",
      "Iteration 81, loss = 0.11272216\n",
      "Iteration 82, loss = 0.11216917\n",
      "Iteration 83, loss = 0.11157371\n",
      "Iteration 84, loss = 0.11099973\n",
      "Iteration 85, loss = 0.11055713\n",
      "Iteration 86, loss = 0.10986634\n",
      "Iteration 87, loss = 0.10926154\n",
      "Iteration 88, loss = 0.10866496\n",
      "Iteration 89, loss = 0.10814828\n",
      "Iteration 90, loss = 0.10754972\n",
      "Iteration 91, loss = 0.10711506\n",
      "Iteration 92, loss = 0.10662996\n",
      "Iteration 93, loss = 0.10589316\n",
      "Iteration 94, loss = 0.10540517\n",
      "Iteration 95, loss = 0.10479880\n",
      "Iteration 96, loss = 0.10416360\n",
      "Iteration 97, loss = 0.10357964\n",
      "Iteration 98, loss = 0.10321957\n",
      "Iteration 99, loss = 0.10248987\n",
      "Iteration 100, loss = 0.10173985\n",
      "Iteration 101, loss = 0.10116330\n",
      "Iteration 102, loss = 0.10063775\n",
      "Iteration 103, loss = 0.09995490\n",
      "Iteration 104, loss = 0.09941927\n",
      "Iteration 105, loss = 0.09877335\n",
      "Iteration 106, loss = 0.09822367\n",
      "Iteration 107, loss = 0.09757843\n",
      "Iteration 108, loss = 0.09704201\n",
      "Iteration 109, loss = 0.09651415\n",
      "Iteration 110, loss = 0.09572758\n",
      "Iteration 111, loss = 0.09520008\n",
      "Iteration 112, loss = 0.09470760\n",
      "Iteration 113, loss = 0.09421485\n",
      "Iteration 114, loss = 0.09352853\n",
      "Iteration 115, loss = 0.09295281\n",
      "Iteration 116, loss = 0.09221753\n",
      "Iteration 117, loss = 0.09172725\n",
      "Iteration 118, loss = 0.09103227\n",
      "Iteration 119, loss = 0.09066070\n",
      "Iteration 120, loss = 0.08999003\n",
      "Iteration 121, loss = 0.08933415\n",
      "Iteration 122, loss = 0.08878293\n",
      "Iteration 123, loss = 0.08810975\n",
      "Iteration 124, loss = 0.08753341\n",
      "Iteration 125, loss = 0.08689465\n",
      "Iteration 126, loss = 0.08643217\n",
      "Iteration 127, loss = 0.08566090\n",
      "Iteration 128, loss = 0.08516199\n",
      "Iteration 129, loss = 0.08448643\n",
      "Iteration 130, loss = 0.08408564\n",
      "Iteration 131, loss = 0.08325825\n",
      "Iteration 132, loss = 0.08269262\n",
      "Iteration 133, loss = 0.08208056\n",
      "Iteration 134, loss = 0.08150173\n",
      "Iteration 135, loss = 0.08101215\n",
      "Iteration 136, loss = 0.08046494\n",
      "Iteration 137, loss = 0.07983671\n",
      "Iteration 138, loss = 0.07948854\n",
      "Iteration 139, loss = 0.07857718\n",
      "Iteration 140, loss = 0.07810973\n",
      "Iteration 141, loss = 0.07741855\n",
      "Iteration 142, loss = 0.07679848\n",
      "Iteration 143, loss = 0.07620212\n",
      "Iteration 144, loss = 0.07595175\n",
      "Iteration 145, loss = 0.07516720\n",
      "Iteration 146, loss = 0.07451911\n",
      "Iteration 147, loss = 0.07394649\n",
      "Iteration 148, loss = 0.07330985\n",
      "Iteration 149, loss = 0.07272719\n",
      "Iteration 150, loss = 0.07217259\n",
      "Iteration 151, loss = 0.07162161\n",
      "Iteration 152, loss = 0.07120187\n",
      "Iteration 153, loss = 0.07043651\n",
      "Iteration 154, loss = 0.06987876\n",
      "Iteration 155, loss = 0.06929050\n",
      "Iteration 156, loss = 0.06882190\n",
      "Iteration 157, loss = 0.06811425\n",
      "Iteration 158, loss = 0.06764333\n",
      "Iteration 159, loss = 0.06707345\n",
      "Iteration 160, loss = 0.06636913\n",
      "Iteration 161, loss = 0.06608277\n",
      "Iteration 162, loss = 0.06534456\n",
      "Iteration 163, loss = 0.06473269\n",
      "Iteration 164, loss = 0.06410980\n",
      "Iteration 165, loss = 0.06359512\n",
      "Iteration 166, loss = 0.06300413\n",
      "Iteration 167, loss = 0.06243346\n",
      "Iteration 168, loss = 0.06191322\n",
      "Iteration 169, loss = 0.06142821\n",
      "Iteration 170, loss = 0.06079768\n",
      "Iteration 171, loss = 0.06028208\n",
      "Iteration 172, loss = 0.05974376\n",
      "Iteration 173, loss = 0.05918487\n",
      "Iteration 174, loss = 0.05860689\n",
      "Iteration 175, loss = 0.05817693\n",
      "Iteration 176, loss = 0.05755465\n",
      "Iteration 177, loss = 0.05704668\n",
      "Iteration 178, loss = 0.05648180\n",
      "Iteration 179, loss = 0.05599945\n",
      "Iteration 180, loss = 0.05553842\n",
      "Iteration 181, loss = 0.05503501\n",
      "Iteration 182, loss = 0.05446046\n",
      "Iteration 183, loss = 0.05403111\n",
      "Iteration 184, loss = 0.05344079\n",
      "Iteration 185, loss = 0.05281361\n",
      "Iteration 186, loss = 0.05240112\n",
      "Iteration 187, loss = 0.05190001\n",
      "Iteration 188, loss = 0.05133632\n",
      "Iteration 189, loss = 0.05091098\n",
      "Iteration 190, loss = 0.05040864\n",
      "Iteration 191, loss = 0.04988096\n",
      "Iteration 192, loss = 0.04941794\n",
      "Iteration 193, loss = 0.04889116\n",
      "Iteration 194, loss = 0.04850059\n",
      "Iteration 195, loss = 0.04794260\n",
      "Iteration 196, loss = 0.04745000\n",
      "Iteration 197, loss = 0.04698812\n",
      "Iteration 198, loss = 0.04651317\n",
      "Iteration 199, loss = 0.04608352\n",
      "Iteration 200, loss = 0.04577370\n",
      "Iteration 201, loss = 0.04509614\n",
      "Iteration 202, loss = 0.04461271\n",
      "Iteration 203, loss = 0.04431273\n",
      "Iteration 204, loss = 0.04378495\n",
      "Iteration 205, loss = 0.04327860\n",
      "Iteration 206, loss = 0.04284350\n",
      "Iteration 207, loss = 0.04253552\n",
      "Iteration 208, loss = 0.04194258\n",
      "Iteration 209, loss = 0.04157868\n",
      "Iteration 210, loss = 0.04107350\n",
      "Iteration 211, loss = 0.04063870\n",
      "Iteration 212, loss = 0.04022339\n",
      "Iteration 213, loss = 0.03981832\n",
      "Iteration 214, loss = 0.03939514\n",
      "Iteration 215, loss = 0.03896640\n",
      "Iteration 216, loss = 0.03855486\n",
      "Iteration 217, loss = 0.03814319\n",
      "Iteration 218, loss = 0.03780177\n",
      "Iteration 219, loss = 0.03731593\n",
      "Iteration 220, loss = 0.03704363\n",
      "Iteration 221, loss = 0.03647791\n",
      "Iteration 222, loss = 0.03618076\n",
      "Iteration 223, loss = 0.03571689\n",
      "Iteration 224, loss = 0.03544139\n",
      "Iteration 225, loss = 0.03492293\n",
      "Iteration 226, loss = 0.03452527\n",
      "Iteration 227, loss = 0.03441244\n",
      "Iteration 228, loss = 0.03379378\n",
      "Iteration 229, loss = 0.03348168\n",
      "Iteration 230, loss = 0.03308570\n",
      "Iteration 231, loss = 0.03269126\n",
      "Iteration 232, loss = 0.03233528\n",
      "Iteration 233, loss = 0.03198716\n",
      "Iteration 234, loss = 0.03154055\n",
      "Iteration 235, loss = 0.03118498\n",
      "Iteration 236, loss = 0.03078779\n",
      "Iteration 237, loss = 0.03048152\n",
      "Iteration 238, loss = 0.03021172\n",
      "Iteration 239, loss = 0.02980865\n",
      "Iteration 240, loss = 0.02945072\n",
      "Iteration 241, loss = 0.02922882\n",
      "Iteration 242, loss = 0.02871130\n",
      "Iteration 243, loss = 0.02843379\n",
      "Iteration 244, loss = 0.02815584\n",
      "Iteration 245, loss = 0.02779903\n",
      "Iteration 246, loss = 0.02746666\n",
      "Iteration 247, loss = 0.02714791\n",
      "Iteration 248, loss = 0.02684173\n",
      "Iteration 249, loss = 0.02660950\n",
      "Iteration 250, loss = 0.02623347\n",
      "Iteration 251, loss = 0.02595642\n",
      "Iteration 252, loss = 0.02557518\n",
      "Iteration 253, loss = 0.02531309\n",
      "Iteration 254, loss = 0.02504468\n",
      "Iteration 255, loss = 0.02479478\n",
      "Iteration 256, loss = 0.02442534\n",
      "Iteration 257, loss = 0.02414802\n",
      "Iteration 258, loss = 0.02384159\n",
      "Iteration 259, loss = 0.02357091\n",
      "Iteration 260, loss = 0.02333286\n",
      "Iteration 261, loss = 0.02305226\n",
      "Iteration 262, loss = 0.02293883\n",
      "Iteration 263, loss = 0.02250882\n",
      "Iteration 264, loss = 0.02225119\n",
      "Iteration 265, loss = 0.02196823\n",
      "Iteration 266, loss = 0.02174666\n",
      "Iteration 267, loss = 0.02151241\n",
      "Iteration 268, loss = 0.02130004\n",
      "Iteration 269, loss = 0.02103965\n",
      "Iteration 270, loss = 0.02078308\n",
      "Iteration 271, loss = 0.02050814\n",
      "Iteration 272, loss = 0.02028146\n",
      "Iteration 273, loss = 0.02012073\n",
      "Iteration 274, loss = 0.01981660\n",
      "Iteration 275, loss = 0.01961606\n",
      "Iteration 276, loss = 0.01938920\n",
      "Iteration 277, loss = 0.01916299\n",
      "Iteration 278, loss = 0.01890307\n",
      "Iteration 279, loss = 0.01875844\n",
      "Iteration 280, loss = 0.01850740\n",
      "Iteration 281, loss = 0.01839421\n",
      "Iteration 282, loss = 0.01805076\n",
      "Iteration 283, loss = 0.01786498\n",
      "Iteration 284, loss = 0.01767641\n",
      "Iteration 285, loss = 0.01745836\n",
      "Iteration 286, loss = 0.01725924\n",
      "Iteration 287, loss = 0.01705420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 288, loss = 0.01690443\n",
      "Iteration 289, loss = 0.01669527\n",
      "Iteration 290, loss = 0.01651123\n",
      "Iteration 291, loss = 0.01628101\n",
      "Iteration 292, loss = 0.01611312\n",
      "Iteration 293, loss = 0.01599471\n",
      "Iteration 294, loss = 0.01575161\n",
      "Iteration 295, loss = 0.01560615\n",
      "Iteration 296, loss = 0.01542044\n",
      "Iteration 297, loss = 0.01528775\n",
      "Iteration 298, loss = 0.01508532\n",
      "Iteration 299, loss = 0.01495306\n",
      "Iteration 300, loss = 0.01476408\n",
      "Iteration 301, loss = 0.01462930\n",
      "Iteration 302, loss = 0.01445758\n",
      "Iteration 303, loss = 0.01430484\n",
      "Iteration 304, loss = 0.01420075\n",
      "Iteration 305, loss = 0.01397662\n",
      "Iteration 306, loss = 0.01389125\n",
      "Iteration 307, loss = 0.01374122\n",
      "Iteration 308, loss = 0.01357159\n",
      "Iteration 309, loss = 0.01345559\n",
      "Iteration 310, loss = 0.01331985\n",
      "Iteration 311, loss = 0.01323320\n",
      "Iteration 312, loss = 0.01301982\n",
      "Iteration 313, loss = 0.01287908\n",
      "Iteration 314, loss = 0.01279551\n",
      "Iteration 315, loss = 0.01265146\n",
      "Iteration 316, loss = 0.01252228\n",
      "Iteration 317, loss = 0.01237765\n",
      "Iteration 318, loss = 0.01224742\n",
      "Iteration 319, loss = 0.01222463\n",
      "Iteration 320, loss = 0.01202279\n",
      "Iteration 321, loss = 0.01189405\n",
      "Iteration 322, loss = 0.01178178\n",
      "Iteration 323, loss = 0.01164394\n",
      "Iteration 324, loss = 0.01155295\n",
      "Iteration 325, loss = 0.01142732\n",
      "Iteration 326, loss = 0.01132125\n",
      "Iteration 327, loss = 0.01120536\n",
      "Iteration 328, loss = 0.01109975\n",
      "Iteration 329, loss = 0.01100303\n",
      "Iteration 330, loss = 0.01092196\n",
      "Iteration 331, loss = 0.01078479\n",
      "Iteration 332, loss = 0.01068308\n",
      "Iteration 333, loss = 0.01058088\n",
      "Iteration 334, loss = 0.01050335\n",
      "Iteration 335, loss = 0.01039456\n",
      "Iteration 336, loss = 0.01030693\n",
      "Iteration 337, loss = 0.01019191\n",
      "Iteration 338, loss = 0.01012553\n",
      "Iteration 339, loss = 0.01002038\n",
      "Iteration 340, loss = 0.00994148\n",
      "Iteration 341, loss = 0.00982578\n",
      "Iteration 342, loss = 0.00977114\n",
      "Iteration 343, loss = 0.00965514\n",
      "Iteration 344, loss = 0.00958155\n",
      "Iteration 345, loss = 0.00948115\n",
      "Iteration 346, loss = 0.00939989\n",
      "Iteration 347, loss = 0.00931597\n",
      "Iteration 348, loss = 0.00924549\n",
      "Iteration 349, loss = 0.00915290\n",
      "Iteration 350, loss = 0.00907292\n",
      "Iteration 351, loss = 0.00901621\n",
      "Iteration 352, loss = 0.00892524\n",
      "Iteration 353, loss = 0.00885979\n",
      "Iteration 354, loss = 0.00878615\n",
      "Iteration 355, loss = 0.00869797\n",
      "Iteration 356, loss = 0.00863443\n",
      "Iteration 357, loss = 0.00854678\n",
      "Iteration 358, loss = 0.00847724\n",
      "Iteration 359, loss = 0.00841958\n",
      "Iteration 360, loss = 0.00832593\n",
      "Iteration 361, loss = 0.00827752\n",
      "Iteration 362, loss = 0.00819282\n",
      "Iteration 363, loss = 0.00814853\n",
      "Iteration 364, loss = 0.00806101\n",
      "Iteration 365, loss = 0.00798997\n",
      "Iteration 366, loss = 0.00792609\n",
      "Iteration 367, loss = 0.00786554\n",
      "Iteration 368, loss = 0.00778993\n",
      "Iteration 369, loss = 0.00774635\n",
      "Iteration 370, loss = 0.00766731\n",
      "Iteration 371, loss = 0.00760990\n",
      "Iteration 372, loss = 0.00754520\n",
      "Iteration 373, loss = 0.00748743\n",
      "Iteration 374, loss = 0.00745960\n",
      "Iteration 375, loss = 0.00737416\n",
      "Iteration 376, loss = 0.00731261\n",
      "Iteration 377, loss = 0.00727639\n",
      "Iteration 378, loss = 0.00719080\n",
      "Iteration 379, loss = 0.00714742\n",
      "Iteration 380, loss = 0.00711024\n",
      "Iteration 381, loss = 0.00702935\n",
      "Iteration 382, loss = 0.00700369\n",
      "Iteration 383, loss = 0.00692069\n",
      "Iteration 384, loss = 0.00687958\n",
      "Iteration 385, loss = 0.00683198\n",
      "Iteration 386, loss = 0.00676387\n",
      "Iteration 387, loss = 0.00671190\n",
      "Iteration 388, loss = 0.00668212\n",
      "Iteration 389, loss = 0.00661699\n",
      "Iteration 390, loss = 0.00659444\n",
      "Iteration 391, loss = 0.00657029\n",
      "Iteration 392, loss = 0.00647309\n",
      "Iteration 393, loss = 0.00644495\n",
      "Iteration 394, loss = 0.00637355\n",
      "Iteration 395, loss = 0.00632379\n",
      "Iteration 396, loss = 0.00629013\n",
      "Iteration 397, loss = 0.00624782\n",
      "Iteration 398, loss = 0.00619345\n",
      "Iteration 399, loss = 0.00614945\n",
      "Iteration 400, loss = 0.00610368\n",
      "Iteration 401, loss = 0.00609516\n",
      "Iteration 402, loss = 0.00601231\n",
      "Iteration 403, loss = 0.00597951\n",
      "Iteration 404, loss = 0.00594559\n",
      "Iteration 405, loss = 0.00590803\n",
      "Iteration 406, loss = 0.00584979\n",
      "Iteration 407, loss = 0.00581155\n",
      "Iteration 408, loss = 0.00577817\n",
      "Iteration 409, loss = 0.00572748\n",
      "Iteration 410, loss = 0.00571188\n",
      "Iteration 411, loss = 0.00566168\n",
      "Iteration 412, loss = 0.00562174\n",
      "Iteration 413, loss = 0.00559721\n",
      "Iteration 414, loss = 0.00554249\n",
      "Iteration 415, loss = 0.00550523\n",
      "Iteration 416, loss = 0.00546717\n",
      "Iteration 417, loss = 0.00542722\n",
      "Iteration 418, loss = 0.00539525\n",
      "Iteration 419, loss = 0.00535526\n",
      "Iteration 420, loss = 0.00531841\n",
      "Iteration 421, loss = 0.00529762\n",
      "Iteration 422, loss = 0.00524963\n",
      "Iteration 423, loss = 0.00522130\n",
      "Iteration 424, loss = 0.00519456\n",
      "Iteration 425, loss = 0.00515035\n",
      "Iteration 426, loss = 0.00512906\n",
      "Iteration 427, loss = 0.00509208\n",
      "Iteration 428, loss = 0.00506598\n",
      "Iteration 429, loss = 0.00503433\n",
      "Iteration 430, loss = 0.00498340\n",
      "Iteration 431, loss = 0.00497146\n",
      "Iteration 432, loss = 0.00492804\n",
      "Iteration 433, loss = 0.00489491\n",
      "Iteration 434, loss = 0.00487085\n",
      "Iteration 435, loss = 0.00482204\n",
      "Iteration 436, loss = 0.00479726\n",
      "Iteration 437, loss = 0.00476643\n",
      "Iteration 438, loss = 0.00474171\n",
      "Iteration 439, loss = 0.00470549\n",
      "Iteration 440, loss = 0.00467554\n",
      "Iteration 441, loss = 0.00465725\n",
      "Iteration 442, loss = 0.00465354\n",
      "Iteration 443, loss = 0.00458992\n",
      "Iteration 444, loss = 0.00456917\n",
      "Iteration 445, loss = 0.00455401\n",
      "Iteration 446, loss = 0.00450215\n",
      "Iteration 447, loss = 0.00448934\n",
      "Iteration 448, loss = 0.00446025\n",
      "Iteration 449, loss = 0.00443291\n",
      "Iteration 450, loss = 0.00440214\n",
      "Iteration 451, loss = 0.00437726\n",
      "Iteration 452, loss = 0.00435177\n",
      "Iteration 453, loss = 0.00432412\n",
      "Iteration 454, loss = 0.00429477\n",
      "Iteration 455, loss = 0.00428166\n",
      "Iteration 456, loss = 0.00425662\n",
      "Iteration 457, loss = 0.00422613\n",
      "Iteration 458, loss = 0.00420381\n",
      "Iteration 459, loss = 0.00418066\n",
      "Iteration 460, loss = 0.00415150\n",
      "Iteration 461, loss = 0.00412909\n",
      "Iteration 462, loss = 0.00410627\n",
      "Iteration 463, loss = 0.00407691\n",
      "Iteration 464, loss = 0.00405215\n",
      "Iteration 465, loss = 0.00403491\n",
      "Iteration 466, loss = 0.00401532\n",
      "Iteration 467, loss = 0.00398374\n",
      "Iteration 468, loss = 0.00396562\n",
      "Iteration 469, loss = 0.00394312\n",
      "Iteration 470, loss = 0.00392256\n",
      "Iteration 471, loss = 0.00390108\n",
      "Iteration 472, loss = 0.00387819\n",
      "Iteration 473, loss = 0.00386147\n",
      "Iteration 474, loss = 0.00384868\n",
      "Iteration 475, loss = 0.00384799\n",
      "Iteration 476, loss = 0.00379463\n",
      "Iteration 477, loss = 0.00377028\n",
      "Iteration 478, loss = 0.00375092\n",
      "Iteration 479, loss = 0.00373332\n",
      "Iteration 480, loss = 0.00371632\n",
      "Iteration 481, loss = 0.00370786\n",
      "Iteration 482, loss = 0.00367481\n",
      "Iteration 483, loss = 0.00365572\n",
      "Iteration 484, loss = 0.00363213\n",
      "Iteration 485, loss = 0.00362258\n",
      "Iteration 486, loss = 0.00359720\n",
      "Iteration 487, loss = 0.00357836\n",
      "Iteration 488, loss = 0.00357798\n",
      "Iteration 489, loss = 0.00353822\n",
      "Iteration 490, loss = 0.00352072\n",
      "Iteration 491, loss = 0.00350615\n",
      "Iteration 492, loss = 0.00348467\n",
      "Iteration 493, loss = 0.00347451\n",
      "Iteration 494, loss = 0.00344856\n",
      "Iteration 495, loss = 0.00343255\n",
      "Iteration 496, loss = 0.00341943\n",
      "Iteration 497, loss = 0.00339816\n",
      "Iteration 498, loss = 0.00338369\n",
      "Iteration 499, loss = 0.00336208\n",
      "Iteration 500, loss = 0.00334955\n",
      "Iteration 501, loss = 0.00333815\n",
      "Iteration 502, loss = 0.00331304\n",
      "Iteration 503, loss = 0.00331277\n",
      "Iteration 504, loss = 0.00327858\n",
      "Iteration 505, loss = 0.00327299\n",
      "Iteration 506, loss = 0.00324924\n",
      "Iteration 507, loss = 0.00323835\n",
      "Iteration 508, loss = 0.00321792\n",
      "Iteration 509, loss = 0.00320100\n",
      "Iteration 510, loss = 0.00319079\n",
      "Iteration 511, loss = 0.00317243\n",
      "Iteration 512, loss = 0.00315990\n",
      "Iteration 513, loss = 0.00313656\n",
      "Iteration 514, loss = 0.00312259\n",
      "Iteration 515, loss = 0.00310982\n",
      "Iteration 516, loss = 0.00309775\n",
      "Iteration 517, loss = 0.00308425\n",
      "Iteration 518, loss = 0.00307282\n",
      "Iteration 519, loss = 0.00304942\n",
      "Iteration 520, loss = 0.00303684\n",
      "Iteration 521, loss = 0.00302215\n",
      "Iteration 522, loss = 0.00301430\n",
      "Iteration 523, loss = 0.00300036\n",
      "Iteration 524, loss = 0.00298151\n",
      "Iteration 525, loss = 0.00297015\n",
      "Iteration 526, loss = 0.00295576\n",
      "Iteration 527, loss = 0.00294426\n",
      "Iteration 528, loss = 0.00293180\n",
      "Iteration 529, loss = 0.00291361\n",
      "Iteration 530, loss = 0.00289920\n",
      "Iteration 531, loss = 0.00288518\n",
      "Iteration 532, loss = 0.00287496\n",
      "Iteration 533, loss = 0.00285977\n",
      "Iteration 534, loss = 0.00284740\n",
      "Iteration 535, loss = 0.00283306\n",
      "Iteration 536, loss = 0.00282311\n",
      "Iteration 537, loss = 0.00280863\n",
      "Iteration 538, loss = 0.00279599\n",
      "Iteration 539, loss = 0.00278628\n",
      "Iteration 540, loss = 0.00277392\n",
      "Iteration 541, loss = 0.00275892\n",
      "Iteration 542, loss = 0.00274667\n",
      "Iteration 543, loss = 0.00273539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 544, loss = 0.00272517\n",
      "Iteration 545, loss = 0.00271811\n",
      "Iteration 546, loss = 0.00270352\n",
      "Iteration 547, loss = 0.00269121\n",
      "Iteration 548, loss = 0.00267605\n",
      "Iteration 549, loss = 0.00266676\n",
      "Iteration 550, loss = 0.00265423\n",
      "Iteration 551, loss = 0.00264653\n",
      "Iteration 552, loss = 0.00263625\n",
      "Iteration 553, loss = 0.00261979\n",
      "Iteration 554, loss = 0.00261257\n",
      "Iteration 555, loss = 0.00259876\n",
      "Iteration 556, loss = 0.00258681\n",
      "Iteration 557, loss = 0.00257738\n",
      "Iteration 558, loss = 0.00256859\n",
      "Iteration 559, loss = 0.00255519\n",
      "Iteration 560, loss = 0.00254295\n",
      "Iteration 561, loss = 0.00253274\n",
      "Iteration 562, loss = 0.00252212\n",
      "Iteration 563, loss = 0.00251484\n",
      "Iteration 564, loss = 0.00250324\n",
      "Iteration 565, loss = 0.00249314\n",
      "Iteration 566, loss = 0.00248276\n",
      "Iteration 567, loss = 0.00247131\n",
      "Iteration 568, loss = 0.00246160\n",
      "Iteration 569, loss = 0.00245003\n",
      "Iteration 570, loss = 0.00244604\n",
      "Iteration 571, loss = 0.00243209\n",
      "Iteration 572, loss = 0.00242166\n",
      "Iteration 573, loss = 0.00242089\n",
      "Iteration 574, loss = 0.00240232\n",
      "Iteration 575, loss = 0.00239940\n",
      "Iteration 576, loss = 0.00238530\n",
      "Iteration 577, loss = 0.00237494\n",
      "Iteration 578, loss = 0.00236718\n",
      "Iteration 579, loss = 0.00235966\n",
      "Iteration 580, loss = 0.00234794\n",
      "Iteration 581, loss = 0.00234414\n",
      "Iteration 582, loss = 0.00232756\n",
      "Iteration 583, loss = 0.00232374\n",
      "Iteration 584, loss = 0.00231641\n",
      "Iteration 585, loss = 0.00230245\n",
      "Iteration 586, loss = 0.00229409\n",
      "Iteration 587, loss = 0.00228352\n",
      "Iteration 588, loss = 0.00227725\n",
      "Iteration 589, loss = 0.00226875\n",
      "Iteration 590, loss = 0.00225918\n",
      "Iteration 591, loss = 0.00224974\n",
      "Iteration 592, loss = 0.00224230\n",
      "Iteration 593, loss = 0.00223873\n",
      "Iteration 594, loss = 0.00222398\n",
      "Iteration 595, loss = 0.00222963\n",
      "Iteration 596, loss = 0.00222225\n",
      "Iteration 597, loss = 0.00220202\n",
      "Iteration 598, loss = 0.00219242\n",
      "Iteration 599, loss = 0.00218514\n",
      "Iteration 600, loss = 0.00217658\n",
      "Iteration 601, loss = 0.00216837\n",
      "Iteration 602, loss = 0.00215926\n",
      "Iteration 603, loss = 0.00215408\n",
      "Iteration 604, loss = 0.00214602\n",
      "Iteration 605, loss = 0.00213649\n",
      "Iteration 606, loss = 0.00212784\n",
      "Iteration 607, loss = 0.00212097\n",
      "Iteration 608, loss = 0.00211798\n",
      "Iteration 609, loss = 0.00210707\n",
      "Iteration 610, loss = 0.00209838\n",
      "Iteration 611, loss = 0.00209639\n",
      "Iteration 612, loss = 0.00208239\n",
      "Iteration 613, loss = 0.00207750\n",
      "Iteration 614, loss = 0.00206984\n",
      "Iteration 615, loss = 0.00206290\n",
      "Iteration 616, loss = 0.00205690\n",
      "Iteration 617, loss = 0.00204785\n",
      "Iteration 618, loss = 0.00203876\n",
      "Iteration 619, loss = 0.00203166\n",
      "Iteration 620, loss = 0.00202704\n",
      "Iteration 621, loss = 0.00202156\n",
      "Iteration 622, loss = 0.00201192\n",
      "Iteration 623, loss = 0.00200622\n",
      "Iteration 624, loss = 0.00199940\n",
      "Iteration 625, loss = 0.00199177\n",
      "Iteration 626, loss = 0.00198789\n",
      "Iteration 627, loss = 0.00197792\n",
      "Iteration 628, loss = 0.00197255\n",
      "Iteration 629, loss = 0.00196366\n",
      "Iteration 630, loss = 0.00196095\n",
      "Iteration 631, loss = 0.00195040\n",
      "Iteration 632, loss = 0.00194382\n",
      "Iteration 633, loss = 0.00194051\n",
      "Iteration 634, loss = 0.00193232\n",
      "Iteration 635, loss = 0.00192711\n",
      "Iteration 636, loss = 0.00191704\n",
      "Iteration 637, loss = 0.00191319\n",
      "Iteration 638, loss = 0.00190933\n",
      "Iteration 639, loss = 0.00190002\n",
      "Iteration 640, loss = 0.00189274\n",
      "Iteration 641, loss = 0.00189056\n",
      "Iteration 642, loss = 0.00188269\n",
      "Iteration 643, loss = 0.00187344\n",
      "Iteration 644, loss = 0.00186979\n",
      "Iteration 645, loss = 0.00186451\n",
      "Iteration 646, loss = 0.00185805\n",
      "Iteration 647, loss = 0.00184958\n",
      "Iteration 648, loss = 0.00184442\n",
      "Iteration 649, loss = 0.00183793\n",
      "Iteration 650, loss = 0.00183286\n",
      "Iteration 651, loss = 0.00182688\n",
      "Iteration 652, loss = 0.00182217\n",
      "Iteration 653, loss = 0.00181691\n",
      "Iteration 654, loss = 0.00180854\n",
      "Iteration 655, loss = 0.00180323\n",
      "Iteration 656, loss = 0.00179798\n",
      "Iteration 657, loss = 0.00179145\n",
      "Iteration 658, loss = 0.00179214\n",
      "Iteration 659, loss = 0.00178508\n",
      "Iteration 660, loss = 0.00177619\n",
      "Iteration 661, loss = 0.00177145\n",
      "Iteration 662, loss = 0.00176422\n",
      "Iteration 663, loss = 0.00175823\n",
      "Iteration 664, loss = 0.00175338\n",
      "Iteration 665, loss = 0.00174951\n",
      "Iteration 666, loss = 0.00174223\n",
      "Iteration 667, loss = 0.00173717\n",
      "Iteration 668, loss = 0.00173168\n",
      "Iteration 669, loss = 0.00172844\n",
      "Iteration 670, loss = 0.00172053\n",
      "Iteration 671, loss = 0.00171546\n",
      "Iteration 672, loss = 0.00171135\n",
      "Iteration 673, loss = 0.00170645\n",
      "Iteration 674, loss = 0.00169984\n",
      "Iteration 675, loss = 0.00169481\n",
      "Iteration 676, loss = 0.00169180\n",
      "Iteration 677, loss = 0.00168461\n",
      "Iteration 678, loss = 0.00168342\n",
      "Iteration 679, loss = 0.00167620\n",
      "Iteration 680, loss = 0.00167127\n",
      "Iteration 681, loss = 0.00166453\n",
      "Iteration 682, loss = 0.00166041\n",
      "Iteration 683, loss = 0.00165767\n",
      "Iteration 684, loss = 0.00164840\n",
      "Iteration 685, loss = 0.00164383\n",
      "Iteration 686, loss = 0.00164087\n",
      "Iteration 687, loss = 0.00164045\n",
      "Iteration 688, loss = 0.00163195\n",
      "Iteration 689, loss = 0.00162582\n",
      "Iteration 690, loss = 0.00162217\n",
      "Iteration 691, loss = 0.00161569\n",
      "Iteration 692, loss = 0.00161294\n",
      "Iteration 693, loss = 0.00160699\n",
      "Iteration 694, loss = 0.00160251\n",
      "Iteration 695, loss = 0.00159757\n",
      "Iteration 696, loss = 0.00159505\n",
      "Iteration 697, loss = 0.00158835\n",
      "Iteration 698, loss = 0.00158755\n",
      "Iteration 699, loss = 0.00158305\n",
      "Iteration 700, loss = 0.00157596\n",
      "Iteration 701, loss = 0.00157109\n",
      "Iteration 702, loss = 0.00156638\n",
      "Iteration 703, loss = 0.00156247\n",
      "Iteration 704, loss = 0.00155947\n",
      "Iteration 705, loss = 0.00155308\n",
      "Iteration 706, loss = 0.00154898\n",
      "Iteration 707, loss = 0.00154504\n",
      "Iteration 708, loss = 0.00154035\n",
      "Iteration 709, loss = 0.00153625\n",
      "Iteration 710, loss = 0.00153213\n",
      "Iteration 711, loss = 0.00152643\n",
      "Iteration 712, loss = 0.00152394\n",
      "Iteration 713, loss = 0.00151976\n",
      "Iteration 714, loss = 0.00151486\n",
      "Iteration 715, loss = 0.00151105\n",
      "Iteration 716, loss = 0.00150763\n",
      "Iteration 717, loss = 0.00150317\n",
      "Iteration 718, loss = 0.00149775\n",
      "Iteration 719, loss = 0.00149505\n",
      "Iteration 720, loss = 0.00149067\n",
      "Iteration 721, loss = 0.00148524\n",
      "Iteration 722, loss = 0.00148274\n",
      "Iteration 723, loss = 0.00147851\n",
      "Iteration 724, loss = 0.00147389\n",
      "Iteration 725, loss = 0.00147211\n",
      "Iteration 726, loss = 0.00146543\n",
      "Iteration 727, loss = 0.00146241\n",
      "Iteration 728, loss = 0.00145850\n",
      "Iteration 729, loss = 0.00145551\n",
      "Iteration 730, loss = 0.00145088\n",
      "Iteration 731, loss = 0.00144657\n",
      "Iteration 732, loss = 0.00144184\n",
      "Iteration 733, loss = 0.00143916\n",
      "Iteration 734, loss = 0.00143551\n",
      "Iteration 735, loss = 0.00143030\n",
      "Iteration 736, loss = 0.00143161\n",
      "Iteration 737, loss = 0.00142488\n",
      "Iteration 738, loss = 0.00142241\n",
      "Iteration 739, loss = 0.00141676\n",
      "Iteration 740, loss = 0.00141400\n",
      "Iteration 741, loss = 0.00140833\n",
      "Iteration 742, loss = 0.00140525\n",
      "Iteration 743, loss = 0.00140198\n",
      "Iteration 744, loss = 0.00139742\n",
      "Iteration 745, loss = 0.00139557\n",
      "Iteration 746, loss = 0.00139183\n",
      "Iteration 747, loss = 0.00138952\n",
      "Iteration 748, loss = 0.00138415\n",
      "Iteration 749, loss = 0.00137978\n",
      "Iteration 750, loss = 0.00137628\n",
      "Iteration 751, loss = 0.00137388\n",
      "Iteration 752, loss = 0.00136971\n",
      "Iteration 753, loss = 0.00136789\n",
      "Iteration 754, loss = 0.00136271\n",
      "Iteration 755, loss = 0.00135910\n",
      "Iteration 756, loss = 0.00135804\n",
      "Iteration 757, loss = 0.00135409\n",
      "Iteration 758, loss = 0.00135065\n",
      "Iteration 759, loss = 0.00134574\n",
      "Iteration 760, loss = 0.00134242\n",
      "Iteration 761, loss = 0.00134044\n",
      "Iteration 762, loss = 0.00133586\n",
      "Iteration 763, loss = 0.00133252\n",
      "Iteration 764, loss = 0.00133121\n",
      "Iteration 765, loss = 0.00132649\n",
      "Iteration 766, loss = 0.00132289\n",
      "Iteration 767, loss = 0.00131875\n",
      "Iteration 768, loss = 0.00131641\n",
      "Iteration 769, loss = 0.00131293\n",
      "Iteration 770, loss = 0.00130912\n",
      "Iteration 771, loss = 0.00130583\n",
      "Iteration 772, loss = 0.00130373\n",
      "Iteration 773, loss = 0.00130098\n",
      "Iteration 774, loss = 0.00129685\n",
      "Iteration 775, loss = 0.00129434\n",
      "Iteration 776, loss = 0.00129077\n",
      "Iteration 777, loss = 0.00128740\n",
      "Iteration 778, loss = 0.00128572\n",
      "Iteration 779, loss = 0.00128191\n",
      "Iteration 780, loss = 0.00127801\n",
      "Iteration 781, loss = 0.00127734\n",
      "Iteration 782, loss = 0.00127143\n",
      "Iteration 783, loss = 0.00126944\n",
      "Iteration 784, loss = 0.00126712\n",
      "Iteration 785, loss = 0.00126513\n",
      "Iteration 786, loss = 0.00126051\n",
      "Iteration 787, loss = 0.00125739\n",
      "Iteration 788, loss = 0.00125520\n",
      "Iteration 789, loss = 0.00125204\n",
      "Iteration 790, loss = 0.00124826\n",
      "Iteration 791, loss = 0.00124556\n",
      "Iteration 792, loss = 0.00124268\n",
      "Iteration 793, loss = 0.00123950\n",
      "Iteration 794, loss = 0.00123689\n",
      "Iteration 795, loss = 0.00123468\n",
      "Iteration 796, loss = 0.00123107\n",
      "Iteration 797, loss = 0.00122936\n",
      "Iteration 798, loss = 0.00122541\n",
      "Iteration 799, loss = 0.00122431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800, loss = 0.00121920\n",
      "Iteration 801, loss = 0.00121783\n",
      "Iteration 802, loss = 0.00121363\n",
      "Iteration 803, loss = 0.00121119\n",
      "Iteration 804, loss = 0.00120813\n",
      "Iteration 805, loss = 0.00120752\n",
      "Iteration 806, loss = 0.00120366\n",
      "Iteration 807, loss = 0.00120114\n",
      "Iteration 808, loss = 0.00119666\n",
      "Iteration 809, loss = 0.00119503\n",
      "Iteration 810, loss = 0.00119189\n",
      "Iteration 811, loss = 0.00118903\n",
      "Iteration 812, loss = 0.00118681\n",
      "Iteration 813, loss = 0.00118454\n",
      "Iteration 814, loss = 0.00118249\n",
      "Iteration 815, loss = 0.00117763\n",
      "Iteration 816, loss = 0.00117630\n",
      "Iteration 817, loss = 0.00117322\n",
      "Iteration 818, loss = 0.00117274\n",
      "Iteration 819, loss = 0.00116685\n",
      "Iteration 820, loss = 0.00116499\n",
      "Iteration 821, loss = 0.00116230\n",
      "Iteration 822, loss = 0.00116012\n",
      "Iteration 823, loss = 0.00115735\n",
      "Iteration 824, loss = 0.00115559\n",
      "Iteration 825, loss = 0.00115269\n",
      "Iteration 826, loss = 0.00115024\n",
      "Iteration 827, loss = 0.00114876\n",
      "Iteration 828, loss = 0.00114581\n",
      "Iteration 829, loss = 0.00114242\n",
      "Iteration 830, loss = 0.00113926\n",
      "Iteration 831, loss = 0.00113803\n",
      "Iteration 832, loss = 0.00113429\n",
      "Iteration 833, loss = 0.00113265\n",
      "Iteration 834, loss = 0.00113007\n",
      "Iteration 835, loss = 0.00112679\n",
      "Iteration 836, loss = 0.00112476\n",
      "Iteration 837, loss = 0.00112230\n",
      "Iteration 838, loss = 0.00111937\n",
      "Iteration 839, loss = 0.00111707\n",
      "Iteration 840, loss = 0.00111554\n",
      "Iteration 841, loss = 0.00111298\n",
      "Iteration 842, loss = 0.00111128\n",
      "Iteration 843, loss = 0.00110904\n",
      "Iteration 844, loss = 0.00110545\n",
      "Iteration 845, loss = 0.00110350\n",
      "Iteration 846, loss = 0.00110243\n",
      "Iteration 847, loss = 0.00109879\n",
      "Iteration 848, loss = 0.00109602\n",
      "Iteration 849, loss = 0.00109422\n",
      "Iteration 850, loss = 0.00109112\n",
      "Iteration 851, loss = 0.00108866\n",
      "Iteration 852, loss = 0.00108673\n",
      "Iteration 853, loss = 0.00108502\n",
      "Iteration 854, loss = 0.00108270\n",
      "Iteration 855, loss = 0.00108042\n",
      "Iteration 856, loss = 0.00107891\n",
      "Iteration 857, loss = 0.00107514\n",
      "Iteration 858, loss = 0.00107305\n",
      "Iteration 859, loss = 0.00107140\n",
      "Iteration 860, loss = 0.00106929\n",
      "Iteration 861, loss = 0.00106663\n",
      "Iteration 862, loss = 0.00106518\n",
      "Iteration 863, loss = 0.00106187\n",
      "Iteration 864, loss = 0.00105934\n",
      "Iteration 865, loss = 0.00105754\n",
      "Iteration 866, loss = 0.00105539\n",
      "Iteration 867, loss = 0.00105296\n",
      "Iteration 868, loss = 0.00105311\n",
      "Iteration 869, loss = 0.00104882\n",
      "Iteration 870, loss = 0.00104645\n",
      "Iteration 871, loss = 0.00104494\n",
      "Iteration 872, loss = 0.00104279\n",
      "Iteration 873, loss = 0.00104083\n",
      "Iteration 874, loss = 0.00103859\n",
      "Iteration 875, loss = 0.00103730\n",
      "Iteration 876, loss = 0.00103413\n",
      "Iteration 877, loss = 0.00103191\n",
      "Iteration 878, loss = 0.00103073\n",
      "Iteration 879, loss = 0.00102783\n",
      "Iteration 880, loss = 0.00102673\n",
      "Iteration 881, loss = 0.00102438\n",
      "Iteration 882, loss = 0.00102160\n",
      "Iteration 883, loss = 0.00102006\n",
      "Iteration 884, loss = 0.00101736\n",
      "Iteration 885, loss = 0.00101658\n",
      "Iteration 886, loss = 0.00101328\n",
      "Iteration 887, loss = 0.00101147\n",
      "Iteration 888, loss = 0.00100923\n",
      "Iteration 889, loss = 0.00100832\n",
      "Iteration 890, loss = 0.00100649\n",
      "Iteration 891, loss = 0.00100358\n",
      "Iteration 892, loss = 0.00100131\n",
      "Iteration 893, loss = 0.00100130\n",
      "Iteration 894, loss = 0.00099740\n",
      "Iteration 895, loss = 0.00099583\n",
      "Iteration 896, loss = 0.00099369\n",
      "Iteration 897, loss = 0.00099195\n",
      "Iteration 898, loss = 0.00099094\n",
      "Iteration 899, loss = 0.00098773\n",
      "Iteration 900, loss = 0.00098835\n",
      "Iteration 901, loss = 0.00098429\n",
      "Iteration 902, loss = 0.00098257\n",
      "Iteration 903, loss = 0.00098050\n",
      "Iteration 904, loss = 0.00097826\n",
      "Iteration 905, loss = 0.00097670\n",
      "Iteration 906, loss = 0.00097460\n",
      "Iteration 907, loss = 0.00097311\n",
      "Iteration 908, loss = 0.00097081\n",
      "Iteration 909, loss = 0.00097139\n",
      "Iteration 910, loss = 0.00096731\n",
      "Iteration 911, loss = 0.00096576\n",
      "Iteration 912, loss = 0.00096411\n",
      "Iteration 913, loss = 0.00096135\n",
      "Iteration 914, loss = 0.00095966\n",
      "Iteration 915, loss = 0.00095838\n",
      "Iteration 916, loss = 0.00095622\n",
      "Iteration 917, loss = 0.00095479\n",
      "Iteration 918, loss = 0.00095289\n",
      "Iteration 919, loss = 0.00095104\n",
      "Iteration 920, loss = 0.00094901\n",
      "Iteration 921, loss = 0.00094736\n",
      "Iteration 922, loss = 0.00094595\n",
      "Iteration 923, loss = 0.00094362\n",
      "Iteration 924, loss = 0.00094276\n",
      "Iteration 925, loss = 0.00094007\n",
      "Iteration 926, loss = 0.00093880\n",
      "Iteration 927, loss = 0.00093665\n",
      "Iteration 928, loss = 0.00093488\n",
      "Iteration 929, loss = 0.00093407\n",
      "Iteration 930, loss = 0.00093205\n",
      "Iteration 931, loss = 0.00093006\n",
      "Iteration 932, loss = 0.00092778\n",
      "Iteration 933, loss = 0.00092758\n",
      "Iteration 934, loss = 0.00092457\n",
      "Iteration 935, loss = 0.00092276\n",
      "Iteration 936, loss = 0.00092165\n",
      "Iteration 937, loss = 0.00092005\n",
      "Iteration 938, loss = 0.00091823\n",
      "Iteration 939, loss = 0.00091831\n",
      "Iteration 940, loss = 0.00091513\n",
      "Iteration 941, loss = 0.00091320\n",
      "Iteration 942, loss = 0.00091267\n",
      "Iteration 943, loss = 0.00090959\n",
      "Iteration 944, loss = 0.00090802\n",
      "Iteration 945, loss = 0.00090737\n",
      "Iteration 946, loss = 0.00090503\n",
      "Iteration 947, loss = 0.00090303\n",
      "Iteration 948, loss = 0.00090152\n",
      "Iteration 949, loss = 0.00090037\n",
      "Iteration 950, loss = 0.00089829\n",
      "Iteration 951, loss = 0.00089692\n",
      "Iteration 952, loss = 0.00089518\n",
      "Iteration 953, loss = 0.00089386\n",
      "Iteration 954, loss = 0.00089271\n",
      "Iteration 955, loss = 0.00089032\n",
      "Iteration 956, loss = 0.00088951\n",
      "Iteration 957, loss = 0.00088729\n",
      "Iteration 958, loss = 0.00088646\n",
      "Iteration 959, loss = 0.00088415\n",
      "Iteration 960, loss = 0.00088335\n",
      "Iteration 961, loss = 0.00088168\n",
      "Iteration 962, loss = 0.00087929\n",
      "Iteration 963, loss = 0.00087841\n",
      "Iteration 964, loss = 0.00087793\n",
      "Iteration 965, loss = 0.00087548\n",
      "Iteration 966, loss = 0.00087378\n",
      "Iteration 967, loss = 0.00087229\n",
      "Iteration 968, loss = 0.00087115\n",
      "Iteration 969, loss = 0.00086928\n",
      "Iteration 970, loss = 0.00086732\n",
      "Iteration 971, loss = 0.00086585\n",
      "Iteration 972, loss = 0.00086433\n",
      "Iteration 973, loss = 0.00086499\n",
      "Iteration 974, loss = 0.00086191\n",
      "Iteration 975, loss = 0.00085976\n",
      "Iteration 976, loss = 0.00085908\n",
      "Iteration 977, loss = 0.00085739\n",
      "Iteration 978, loss = 0.00085633\n",
      "Iteration 979, loss = 0.00085437\n",
      "Iteration 980, loss = 0.00085275\n",
      "Iteration 981, loss = 0.00085141\n",
      "Iteration 982, loss = 0.00085021\n",
      "Iteration 983, loss = 0.00084912\n",
      "Iteration 984, loss = 0.00084736\n",
      "Iteration 985, loss = 0.00084578\n",
      "Iteration 986, loss = 0.00084385\n",
      "Iteration 987, loss = 0.00084357\n",
      "Iteration 988, loss = 0.00084166\n",
      "Iteration 989, loss = 0.00083978\n",
      "Iteration 990, loss = 0.00083826\n",
      "Iteration 991, loss = 0.00083729\n",
      "Iteration 992, loss = 0.00083602\n",
      "Iteration 993, loss = 0.00083422\n",
      "Iteration 994, loss = 0.00083323\n",
      "Iteration 995, loss = 0.00083147\n",
      "Iteration 996, loss = 0.00083037\n",
      "Iteration 997, loss = 0.00082903\n",
      "Iteration 998, loss = 0.00082716\n",
      "Iteration 999, loss = 0.00082574\n",
      "Iteration 1000, loss = 0.00082444\n",
      "Attribute -> Powerful\n",
      "Accuracy -> 1.0\n",
      "Iteration 1, loss = 0.49458324\n",
      "Iteration 2, loss = 0.38519457\n",
      "Iteration 3, loss = 0.32895320\n",
      "Iteration 4, loss = 0.30442072\n",
      "Iteration 5, loss = 0.28760076\n",
      "Iteration 6, loss = 0.27299436\n",
      "Iteration 7, loss = 0.25951272\n",
      "Iteration 8, loss = 0.24724743\n",
      "Iteration 9, loss = 0.23555994\n",
      "Iteration 10, loss = 0.22429468\n",
      "Iteration 11, loss = 0.21358665\n",
      "Iteration 12, loss = 0.20324002\n",
      "Iteration 13, loss = 0.19335000\n",
      "Iteration 14, loss = 0.18362102\n",
      "Iteration 15, loss = 0.17433341\n",
      "Iteration 16, loss = 0.16547840\n",
      "Iteration 17, loss = 0.15690496\n",
      "Iteration 18, loss = 0.14867140\n",
      "Iteration 19, loss = 0.14067516\n",
      "Iteration 20, loss = 0.13305506\n",
      "Iteration 21, loss = 0.12582097\n",
      "Iteration 22, loss = 0.11878341\n",
      "Iteration 23, loss = 0.11214570\n",
      "Iteration 24, loss = 0.10584415\n",
      "Iteration 25, loss = 0.09994707\n",
      "Iteration 26, loss = 0.09424257\n",
      "Iteration 27, loss = 0.08898080\n",
      "Iteration 28, loss = 0.08407185\n",
      "Iteration 29, loss = 0.07955695\n",
      "Iteration 30, loss = 0.07515428\n",
      "Iteration 31, loss = 0.07119325\n",
      "Iteration 32, loss = 0.06739340\n",
      "Iteration 33, loss = 0.06380807\n",
      "Iteration 34, loss = 0.06051883\n",
      "Iteration 35, loss = 0.05739601\n",
      "Iteration 36, loss = 0.05457465\n",
      "Iteration 37, loss = 0.05177772\n",
      "Iteration 38, loss = 0.04929644\n",
      "Iteration 39, loss = 0.04693437\n",
      "Iteration 40, loss = 0.04477383\n",
      "Iteration 41, loss = 0.04269947\n",
      "Iteration 42, loss = 0.04076158\n",
      "Iteration 43, loss = 0.03896196\n",
      "Iteration 44, loss = 0.03726587\n",
      "Iteration 45, loss = 0.03567544\n",
      "Iteration 46, loss = 0.03419327\n",
      "Iteration 47, loss = 0.03280245\n",
      "Iteration 48, loss = 0.03148317\n",
      "Iteration 49, loss = 0.03024446\n",
      "Iteration 50, loss = 0.02908783\n",
      "Iteration 51, loss = 0.02798436\n",
      "Iteration 52, loss = 0.02694339\n",
      "Iteration 53, loss = 0.02595378\n",
      "Iteration 54, loss = 0.02503824\n",
      "Iteration 55, loss = 0.02416559\n",
      "Iteration 56, loss = 0.02333975\n",
      "Iteration 57, loss = 0.02255277\n",
      "Iteration 58, loss = 0.02180704\n",
      "Iteration 59, loss = 0.02109176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60, loss = 0.02042786\n",
      "Iteration 61, loss = 0.01978757\n",
      "Iteration 62, loss = 0.01918396\n",
      "Iteration 63, loss = 0.01861456\n",
      "Iteration 64, loss = 0.01806258\n",
      "Iteration 65, loss = 0.01754101\n",
      "Iteration 66, loss = 0.01704791\n",
      "Iteration 67, loss = 0.01656499\n",
      "Iteration 68, loss = 0.01611802\n",
      "Iteration 69, loss = 0.01568159\n",
      "Iteration 70, loss = 0.01526572\n",
      "Iteration 71, loss = 0.01486917\n",
      "Iteration 72, loss = 0.01449636\n",
      "Iteration 73, loss = 0.01412413\n",
      "Iteration 74, loss = 0.01377847\n",
      "Iteration 75, loss = 0.01344289\n",
      "Iteration 76, loss = 0.01312765\n",
      "Iteration 77, loss = 0.01281483\n",
      "Iteration 78, loss = 0.01251890\n",
      "Iteration 79, loss = 0.01223389\n",
      "Iteration 80, loss = 0.01196229\n",
      "Iteration 81, loss = 0.01169627\n",
      "Iteration 82, loss = 0.01144591\n",
      "Iteration 83, loss = 0.01119984\n",
      "Iteration 84, loss = 0.01096401\n",
      "Iteration 85, loss = 0.01073563\n",
      "Iteration 86, loss = 0.01051811\n",
      "Iteration 87, loss = 0.01030613\n",
      "Iteration 88, loss = 0.01010245\n",
      "Iteration 89, loss = 0.00990671\n",
      "Iteration 90, loss = 0.00971569\n",
      "Iteration 91, loss = 0.00953081\n",
      "Iteration 92, loss = 0.00935451\n",
      "Iteration 93, loss = 0.00918099\n",
      "Iteration 94, loss = 0.00901548\n",
      "Iteration 95, loss = 0.00885558\n",
      "Iteration 96, loss = 0.00869924\n",
      "Iteration 97, loss = 0.00854796\n",
      "Iteration 98, loss = 0.00840179\n",
      "Iteration 99, loss = 0.00825990\n",
      "Iteration 100, loss = 0.00812021\n",
      "Iteration 101, loss = 0.00798657\n",
      "Iteration 102, loss = 0.00785797\n",
      "Iteration 103, loss = 0.00772969\n",
      "Iteration 104, loss = 0.00760691\n",
      "Iteration 105, loss = 0.00748679\n",
      "Iteration 106, loss = 0.00737297\n",
      "Iteration 107, loss = 0.00725771\n",
      "Iteration 108, loss = 0.00714928\n",
      "Iteration 109, loss = 0.00704169\n",
      "Iteration 110, loss = 0.00693633\n",
      "Iteration 111, loss = 0.00683556\n",
      "Iteration 112, loss = 0.00673652\n",
      "Iteration 113, loss = 0.00664013\n",
      "Iteration 114, loss = 0.00654778\n",
      "Iteration 115, loss = 0.00645478\n",
      "Iteration 116, loss = 0.00636506\n",
      "Iteration 117, loss = 0.00627706\n",
      "Iteration 118, loss = 0.00619371\n",
      "Iteration 119, loss = 0.00610903\n",
      "Iteration 120, loss = 0.00602602\n",
      "Iteration 121, loss = 0.00594814\n",
      "Iteration 122, loss = 0.00587023\n",
      "Iteration 123, loss = 0.00579486\n",
      "Iteration 124, loss = 0.00572102\n",
      "Iteration 125, loss = 0.00564762\n",
      "Iteration 126, loss = 0.00557770\n",
      "Iteration 127, loss = 0.00550904\n",
      "Iteration 128, loss = 0.00544200\n",
      "Iteration 129, loss = 0.00537502\n",
      "Iteration 130, loss = 0.00531038\n",
      "Iteration 131, loss = 0.00524598\n",
      "Iteration 132, loss = 0.00518326\n",
      "Iteration 133, loss = 0.00512394\n",
      "Iteration 134, loss = 0.00506454\n",
      "Iteration 135, loss = 0.00500450\n",
      "Iteration 136, loss = 0.00494835\n",
      "Iteration 137, loss = 0.00489255\n",
      "Iteration 138, loss = 0.00483728\n",
      "Iteration 139, loss = 0.00478381\n",
      "Iteration 140, loss = 0.00473062\n",
      "Iteration 141, loss = 0.00467934\n",
      "Iteration 142, loss = 0.00462946\n",
      "Iteration 143, loss = 0.00457854\n",
      "Iteration 144, loss = 0.00453097\n",
      "Iteration 145, loss = 0.00448185\n",
      "Iteration 146, loss = 0.00443481\n",
      "Iteration 147, loss = 0.00438866\n",
      "Iteration 148, loss = 0.00434433\n",
      "Iteration 149, loss = 0.00429920\n",
      "Iteration 150, loss = 0.00425602\n",
      "Iteration 151, loss = 0.00421404\n",
      "Iteration 152, loss = 0.00417104\n",
      "Iteration 153, loss = 0.00413039\n",
      "Iteration 154, loss = 0.00408981\n",
      "Iteration 155, loss = 0.00404959\n",
      "Iteration 156, loss = 0.00401082\n",
      "Iteration 157, loss = 0.00397247\n",
      "Iteration 158, loss = 0.00393443\n",
      "Iteration 159, loss = 0.00389714\n",
      "Iteration 160, loss = 0.00386073\n",
      "Iteration 161, loss = 0.00382505\n",
      "Iteration 162, loss = 0.00378952\n",
      "Iteration 163, loss = 0.00375366\n",
      "Iteration 164, loss = 0.00372018\n",
      "Iteration 165, loss = 0.00368623\n",
      "Iteration 166, loss = 0.00365389\n",
      "Iteration 167, loss = 0.00362075\n",
      "Iteration 168, loss = 0.00358867\n",
      "Iteration 169, loss = 0.00355711\n",
      "Iteration 170, loss = 0.00352640\n",
      "Iteration 171, loss = 0.00349598\n",
      "Iteration 172, loss = 0.00346512\n",
      "Iteration 173, loss = 0.00343579\n",
      "Iteration 174, loss = 0.00340664\n",
      "Iteration 175, loss = 0.00337763\n",
      "Iteration 176, loss = 0.00334968\n",
      "Iteration 177, loss = 0.00332209\n",
      "Iteration 178, loss = 0.00329497\n",
      "Iteration 179, loss = 0.00326778\n",
      "Iteration 180, loss = 0.00324065\n",
      "Iteration 181, loss = 0.00321460\n",
      "Iteration 182, loss = 0.00318902\n",
      "Iteration 183, loss = 0.00316403\n",
      "Iteration 184, loss = 0.00313836\n",
      "Iteration 185, loss = 0.00311352\n",
      "Iteration 186, loss = 0.00308957\n",
      "Iteration 187, loss = 0.00306536\n",
      "Iteration 188, loss = 0.00304218\n",
      "Iteration 189, loss = 0.00301847\n",
      "Iteration 190, loss = 0.00299578\n",
      "Iteration 191, loss = 0.00297315\n",
      "Iteration 192, loss = 0.00295062\n",
      "Iteration 193, loss = 0.00292783\n",
      "Iteration 194, loss = 0.00290638\n",
      "Iteration 195, loss = 0.00288508\n",
      "Iteration 196, loss = 0.00286372\n",
      "Iteration 197, loss = 0.00284242\n",
      "Iteration 198, loss = 0.00282190\n",
      "Iteration 199, loss = 0.00280131\n",
      "Iteration 200, loss = 0.00278111\n",
      "Iteration 201, loss = 0.00276085\n",
      "Iteration 202, loss = 0.00274117\n",
      "Iteration 203, loss = 0.00272216\n",
      "Iteration 204, loss = 0.00270266\n",
      "Iteration 205, loss = 0.00268343\n",
      "Iteration 206, loss = 0.00266502\n",
      "Iteration 207, loss = 0.00264667\n",
      "Iteration 208, loss = 0.00262818\n",
      "Iteration 209, loss = 0.00261051\n",
      "Iteration 210, loss = 0.00259235\n",
      "Iteration 211, loss = 0.00257468\n",
      "Iteration 212, loss = 0.00255743\n",
      "Iteration 213, loss = 0.00254034\n",
      "Iteration 214, loss = 0.00252319\n",
      "Iteration 215, loss = 0.00250657\n",
      "Iteration 216, loss = 0.00248991\n",
      "Iteration 217, loss = 0.00247355\n",
      "Iteration 218, loss = 0.00245710\n",
      "Iteration 219, loss = 0.00244168\n",
      "Iteration 220, loss = 0.00242545\n",
      "Iteration 221, loss = 0.00241003\n",
      "Iteration 222, loss = 0.00239452\n",
      "Iteration 223, loss = 0.00237893\n",
      "Iteration 224, loss = 0.00236395\n",
      "Iteration 225, loss = 0.00234885\n",
      "Iteration 226, loss = 0.00233401\n",
      "Iteration 227, loss = 0.00231961\n",
      "Iteration 228, loss = 0.00230484\n",
      "Iteration 229, loss = 0.00229047\n",
      "Iteration 230, loss = 0.00227671\n",
      "Iteration 231, loss = 0.00226279\n",
      "Iteration 232, loss = 0.00224907\n",
      "Iteration 233, loss = 0.00223497\n",
      "Iteration 234, loss = 0.00222165\n",
      "Iteration 235, loss = 0.00220842\n",
      "Iteration 236, loss = 0.00219535\n",
      "Iteration 237, loss = 0.00218216\n",
      "Iteration 238, loss = 0.00216953\n",
      "Iteration 239, loss = 0.00215719\n",
      "Iteration 240, loss = 0.00214431\n",
      "Iteration 241, loss = 0.00213201\n",
      "Iteration 242, loss = 0.00211939\n",
      "Iteration 243, loss = 0.00210734\n",
      "Iteration 244, loss = 0.00209531\n",
      "Iteration 245, loss = 0.00208360\n",
      "Iteration 246, loss = 0.00207140\n",
      "Iteration 247, loss = 0.00205965\n",
      "Iteration 248, loss = 0.00204843\n",
      "Iteration 249, loss = 0.00203691\n",
      "Iteration 250, loss = 0.00202549\n",
      "Iteration 251, loss = 0.00201436\n",
      "Iteration 252, loss = 0.00200319\n",
      "Iteration 253, loss = 0.00199237\n",
      "Iteration 254, loss = 0.00198124\n",
      "Iteration 255, loss = 0.00197058\n",
      "Iteration 256, loss = 0.00195990\n",
      "Iteration 257, loss = 0.00194937\n",
      "Iteration 258, loss = 0.00193888\n",
      "Iteration 259, loss = 0.00192880\n",
      "Iteration 260, loss = 0.00191846\n",
      "Iteration 261, loss = 0.00190840\n",
      "Iteration 262, loss = 0.00189833\n",
      "Iteration 263, loss = 0.00188827\n",
      "Iteration 264, loss = 0.00187857\n",
      "Iteration 265, loss = 0.00186889\n",
      "Iteration 266, loss = 0.00185911\n",
      "Iteration 267, loss = 0.00184954\n",
      "Iteration 268, loss = 0.00184004\n",
      "Iteration 269, loss = 0.00183062\n",
      "Iteration 270, loss = 0.00182153\n",
      "Iteration 271, loss = 0.00181244\n",
      "Iteration 272, loss = 0.00180319\n",
      "Iteration 273, loss = 0.00179398\n",
      "Iteration 274, loss = 0.00178524\n",
      "Iteration 275, loss = 0.00177621\n",
      "Iteration 276, loss = 0.00176762\n",
      "Iteration 277, loss = 0.00175897\n",
      "Iteration 278, loss = 0.00175012\n",
      "Iteration 279, loss = 0.00174181\n",
      "Iteration 280, loss = 0.00173324\n",
      "Iteration 281, loss = 0.00172485\n",
      "Iteration 282, loss = 0.00171651\n",
      "Iteration 283, loss = 0.00170824\n",
      "Iteration 284, loss = 0.00170005\n",
      "Iteration 285, loss = 0.00169187\n",
      "Iteration 286, loss = 0.00168361\n",
      "Iteration 287, loss = 0.00167573\n",
      "Iteration 288, loss = 0.00166787\n",
      "Iteration 289, loss = 0.00165987\n",
      "Iteration 290, loss = 0.00165202\n",
      "Iteration 291, loss = 0.00164442\n",
      "Iteration 292, loss = 0.00163658\n",
      "Iteration 293, loss = 0.00162913\n",
      "Iteration 294, loss = 0.00162125\n",
      "Iteration 295, loss = 0.00161405\n",
      "Iteration 296, loss = 0.00160682\n",
      "Iteration 297, loss = 0.00159938\n",
      "Iteration 298, loss = 0.00159219\n",
      "Iteration 299, loss = 0.00158491\n",
      "Iteration 300, loss = 0.00157777\n",
      "Iteration 301, loss = 0.00157073\n",
      "Iteration 302, loss = 0.00156392\n",
      "Iteration 303, loss = 0.00155688\n",
      "Iteration 304, loss = 0.00154992\n",
      "Iteration 305, loss = 0.00154318\n",
      "Iteration 306, loss = 0.00153635\n",
      "Iteration 307, loss = 0.00152946\n",
      "Iteration 308, loss = 0.00152305\n",
      "Iteration 309, loss = 0.00151629\n",
      "Iteration 310, loss = 0.00150985\n",
      "Iteration 311, loss = 0.00150343\n",
      "Iteration 312, loss = 0.00149693\n",
      "Iteration 313, loss = 0.00149042\n",
      "Iteration 314, loss = 0.00148423\n",
      "Iteration 315, loss = 0.00147777\n",
      "Iteration 316, loss = 0.00147164\n",
      "Iteration 317, loss = 0.00146529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 318, loss = 0.00145910\n",
      "Iteration 319, loss = 0.00145303\n",
      "Iteration 320, loss = 0.00144705\n",
      "Iteration 321, loss = 0.00144082\n",
      "Iteration 322, loss = 0.00143481\n",
      "Iteration 323, loss = 0.00142893\n",
      "Iteration 324, loss = 0.00142310\n",
      "Iteration 325, loss = 0.00141719\n",
      "Iteration 326, loss = 0.00141120\n",
      "Iteration 327, loss = 0.00140560\n",
      "Iteration 328, loss = 0.00139990\n",
      "Iteration 329, loss = 0.00139435\n",
      "Iteration 330, loss = 0.00138887\n",
      "Iteration 331, loss = 0.00138311\n",
      "Iteration 332, loss = 0.00137776\n",
      "Iteration 333, loss = 0.00137221\n",
      "Iteration 334, loss = 0.00136674\n",
      "Iteration 335, loss = 0.00136147\n",
      "Iteration 336, loss = 0.00135606\n",
      "Iteration 337, loss = 0.00135062\n",
      "Iteration 338, loss = 0.00134551\n",
      "Iteration 339, loss = 0.00134025\n",
      "Iteration 340, loss = 0.00133500\n",
      "Iteration 341, loss = 0.00132989\n",
      "Iteration 342, loss = 0.00132485\n",
      "Iteration 343, loss = 0.00131961\n",
      "Iteration 344, loss = 0.00131458\n",
      "Iteration 345, loss = 0.00130967\n",
      "Iteration 346, loss = 0.00130440\n",
      "Iteration 347, loss = 0.00129945\n",
      "Iteration 348, loss = 0.00129451\n",
      "Iteration 349, loss = 0.00128958\n",
      "Iteration 350, loss = 0.00128473\n",
      "Iteration 351, loss = 0.00127987\n",
      "Iteration 352, loss = 0.00127523\n",
      "Iteration 353, loss = 0.00127036\n",
      "Iteration 354, loss = 0.00126557\n",
      "Iteration 355, loss = 0.00126093\n",
      "Iteration 356, loss = 0.00125635\n",
      "Iteration 357, loss = 0.00125188\n",
      "Iteration 358, loss = 0.00124699\n",
      "Iteration 359, loss = 0.00124269\n",
      "Iteration 360, loss = 0.00123810\n",
      "Iteration 361, loss = 0.00123366\n",
      "Iteration 362, loss = 0.00122905\n",
      "Iteration 363, loss = 0.00122475\n",
      "Iteration 364, loss = 0.00122027\n",
      "Iteration 365, loss = 0.00121596\n",
      "Iteration 366, loss = 0.00121148\n",
      "Iteration 367, loss = 0.00120721\n",
      "Iteration 368, loss = 0.00120309\n",
      "Iteration 369, loss = 0.00119879\n",
      "Iteration 370, loss = 0.00119445\n",
      "Iteration 371, loss = 0.00119032\n",
      "Iteration 372, loss = 0.00118629\n",
      "Iteration 373, loss = 0.00118210\n",
      "Iteration 374, loss = 0.00117783\n",
      "Iteration 375, loss = 0.00117380\n",
      "Iteration 376, loss = 0.00116974\n",
      "Iteration 377, loss = 0.00116568\n",
      "Iteration 378, loss = 0.00116173\n",
      "Iteration 379, loss = 0.00115776\n",
      "Iteration 380, loss = 0.00115375\n",
      "Iteration 381, loss = 0.00114978\n",
      "Iteration 382, loss = 0.00114595\n",
      "Iteration 383, loss = 0.00114219\n",
      "Iteration 384, loss = 0.00113816\n",
      "Iteration 385, loss = 0.00113449\n",
      "Iteration 386, loss = 0.00113058\n",
      "Iteration 387, loss = 0.00112685\n",
      "Iteration 388, loss = 0.00112316\n",
      "Iteration 389, loss = 0.00111950\n",
      "Iteration 390, loss = 0.00111575\n",
      "Iteration 391, loss = 0.00111188\n",
      "Iteration 392, loss = 0.00110824\n",
      "Iteration 393, loss = 0.00110472\n",
      "Iteration 394, loss = 0.00110090\n",
      "Iteration 395, loss = 0.00109742\n",
      "Iteration 396, loss = 0.00109385\n",
      "Iteration 397, loss = 0.00109024\n",
      "Iteration 398, loss = 0.00108668\n",
      "Iteration 399, loss = 0.00108328\n",
      "Iteration 400, loss = 0.00107970\n",
      "Iteration 401, loss = 0.00107629\n",
      "Iteration 402, loss = 0.00107289\n",
      "Iteration 403, loss = 0.00106949\n",
      "Iteration 404, loss = 0.00106595\n",
      "Iteration 405, loss = 0.00106269\n",
      "Iteration 406, loss = 0.00105925\n",
      "Iteration 407, loss = 0.00105589\n",
      "Iteration 408, loss = 0.00105263\n",
      "Iteration 409, loss = 0.00104932\n",
      "Iteration 410, loss = 0.00104606\n",
      "Iteration 411, loss = 0.00104273\n",
      "Iteration 412, loss = 0.00103964\n",
      "Iteration 413, loss = 0.00103627\n",
      "Iteration 414, loss = 0.00103312\n",
      "Iteration 415, loss = 0.00102992\n",
      "Iteration 416, loss = 0.00102682\n",
      "Iteration 417, loss = 0.00102353\n",
      "Iteration 418, loss = 0.00102056\n",
      "Iteration 419, loss = 0.00101737\n",
      "Iteration 420, loss = 0.00101443\n",
      "Iteration 421, loss = 0.00101119\n",
      "Iteration 422, loss = 0.00100821\n",
      "Iteration 423, loss = 0.00100518\n",
      "Iteration 424, loss = 0.00100212\n",
      "Iteration 425, loss = 0.00099910\n",
      "Iteration 426, loss = 0.00099623\n",
      "Iteration 427, loss = 0.00099313\n",
      "Iteration 428, loss = 0.00099028\n",
      "Iteration 429, loss = 0.00098725\n",
      "Iteration 430, loss = 0.00098428\n",
      "Iteration 431, loss = 0.00098138\n",
      "Iteration 432, loss = 0.00097850\n",
      "Iteration 433, loss = 0.00097562\n",
      "Iteration 434, loss = 0.00097268\n",
      "Iteration 435, loss = 0.00096987\n",
      "Iteration 436, loss = 0.00096697\n",
      "Iteration 437, loss = 0.00096416\n",
      "Iteration 438, loss = 0.00096140\n",
      "Iteration 439, loss = 0.00095863\n",
      "Iteration 440, loss = 0.00095582\n",
      "Iteration 441, loss = 0.00095315\n",
      "Iteration 442, loss = 0.00095051\n",
      "Iteration 443, loss = 0.00094779\n",
      "Iteration 444, loss = 0.00094506\n",
      "Iteration 445, loss = 0.00094244\n",
      "Iteration 446, loss = 0.00093976\n",
      "Iteration 447, loss = 0.00093711\n",
      "Iteration 448, loss = 0.00093445\n",
      "Iteration 449, loss = 0.00093181\n",
      "Iteration 450, loss = 0.00092929\n",
      "Iteration 451, loss = 0.00092658\n",
      "Iteration 452, loss = 0.00092407\n",
      "Iteration 453, loss = 0.00092151\n",
      "Iteration 454, loss = 0.00091889\n",
      "Iteration 455, loss = 0.00091637\n",
      "Iteration 456, loss = 0.00091381\n",
      "Iteration 457, loss = 0.00091129\n",
      "Iteration 458, loss = 0.00090876\n",
      "Iteration 459, loss = 0.00090633\n",
      "Iteration 460, loss = 0.00090385\n",
      "Iteration 461, loss = 0.00090135\n",
      "Iteration 462, loss = 0.00089884\n",
      "Iteration 463, loss = 0.00089651\n",
      "Iteration 464, loss = 0.00089394\n",
      "Iteration 465, loss = 0.00089154\n",
      "Iteration 466, loss = 0.00088919\n",
      "Iteration 467, loss = 0.00088682\n",
      "Iteration 468, loss = 0.00088439\n",
      "Iteration 469, loss = 0.00088207\n",
      "Iteration 470, loss = 0.00087971\n",
      "Iteration 471, loss = 0.00087733\n",
      "Iteration 472, loss = 0.00087504\n",
      "Iteration 473, loss = 0.00087272\n",
      "Iteration 474, loss = 0.00087042\n",
      "Iteration 475, loss = 0.00086812\n",
      "Iteration 476, loss = 0.00086586\n",
      "Iteration 477, loss = 0.00086351\n",
      "Iteration 478, loss = 0.00086134\n",
      "Iteration 479, loss = 0.00085906\n",
      "Iteration 480, loss = 0.00085681\n",
      "Iteration 481, loss = 0.00085455\n",
      "Iteration 482, loss = 0.00085246\n",
      "Iteration 483, loss = 0.00085024\n",
      "Iteration 484, loss = 0.00084804\n",
      "Iteration 485, loss = 0.00084590\n",
      "Iteration 486, loss = 0.00084366\n",
      "Iteration 487, loss = 0.00084153\n",
      "Iteration 488, loss = 0.00083940\n",
      "Iteration 489, loss = 0.00083727\n",
      "Iteration 490, loss = 0.00083510\n",
      "Iteration 491, loss = 0.00083304\n",
      "Iteration 492, loss = 0.00083092\n",
      "Iteration 493, loss = 0.00082888\n",
      "Iteration 494, loss = 0.00082677\n",
      "Iteration 495, loss = 0.00082474\n",
      "Iteration 496, loss = 0.00082262\n",
      "Iteration 497, loss = 0.00082061\n",
      "Iteration 498, loss = 0.00081862\n",
      "Iteration 499, loss = 0.00081653\n",
      "Iteration 500, loss = 0.00081450\n",
      "Iteration 501, loss = 0.00081258\n",
      "Iteration 502, loss = 0.00081049\n",
      "Iteration 503, loss = 0.00080851\n",
      "Iteration 504, loss = 0.00080656\n",
      "Iteration 505, loss = 0.00080457\n",
      "Iteration 506, loss = 0.00080260\n",
      "Iteration 507, loss = 0.00080065\n",
      "Iteration 508, loss = 0.00079872\n",
      "Iteration 509, loss = 0.00079675\n",
      "Iteration 510, loss = 0.00079479\n",
      "Iteration 511, loss = 0.00079289\n",
      "Iteration 512, loss = 0.00079098\n",
      "Iteration 513, loss = 0.00078910\n",
      "Iteration 514, loss = 0.00078719\n",
      "Iteration 515, loss = 0.00078529\n",
      "Iteration 516, loss = 0.00078346\n",
      "Iteration 517, loss = 0.00078160\n",
      "Iteration 518, loss = 0.00077973\n",
      "Iteration 519, loss = 0.00077791\n",
      "Iteration 520, loss = 0.00077601\n",
      "Iteration 521, loss = 0.00077419\n",
      "Iteration 522, loss = 0.00077240\n",
      "Iteration 523, loss = 0.00077062\n",
      "Iteration 524, loss = 0.00076877\n",
      "Iteration 525, loss = 0.00076696\n",
      "Iteration 526, loss = 0.00076518\n",
      "Iteration 527, loss = 0.00076345\n",
      "Iteration 528, loss = 0.00076158\n",
      "Iteration 529, loss = 0.00075984\n",
      "Iteration 530, loss = 0.00075808\n",
      "Iteration 531, loss = 0.00075636\n",
      "Iteration 532, loss = 0.00075461\n",
      "Iteration 533, loss = 0.00075289\n",
      "Iteration 534, loss = 0.00075120\n",
      "Iteration 535, loss = 0.00074941\n",
      "Iteration 536, loss = 0.00074773\n",
      "Iteration 537, loss = 0.00074608\n",
      "Iteration 538, loss = 0.00074433\n",
      "Iteration 539, loss = 0.00074265\n",
      "Iteration 540, loss = 0.00074100\n",
      "Iteration 541, loss = 0.00073922\n",
      "Iteration 542, loss = 0.00073760\n",
      "Iteration 543, loss = 0.00073598\n",
      "Iteration 544, loss = 0.00073427\n",
      "Iteration 545, loss = 0.00073267\n",
      "Iteration 546, loss = 0.00073099\n",
      "Iteration 547, loss = 0.00072943\n",
      "Iteration 548, loss = 0.00072774\n",
      "Iteration 549, loss = 0.00072610\n",
      "Iteration 550, loss = 0.00072456\n",
      "Iteration 551, loss = 0.00072285\n",
      "Iteration 552, loss = 0.00072133\n",
      "Iteration 553, loss = 0.00071974\n",
      "Iteration 554, loss = 0.00071815\n",
      "Iteration 555, loss = 0.00071655\n",
      "Iteration 556, loss = 0.00071499\n",
      "Iteration 557, loss = 0.00071345\n",
      "Iteration 558, loss = 0.00071192\n",
      "Iteration 559, loss = 0.00071036\n",
      "Iteration 560, loss = 0.00070883\n",
      "Iteration 561, loss = 0.00070730\n",
      "Iteration 562, loss = 0.00070579\n",
      "Iteration 563, loss = 0.00070425\n",
      "Iteration 564, loss = 0.00070279\n",
      "Iteration 565, loss = 0.00070125\n",
      "Iteration 566, loss = 0.00069972\n",
      "Iteration 567, loss = 0.00069824\n",
      "Iteration 568, loss = 0.00069675\n",
      "Iteration 569, loss = 0.00069526\n",
      "Iteration 570, loss = 0.00069376\n",
      "Iteration 571, loss = 0.00069233\n",
      "Iteration 572, loss = 0.00069078\n",
      "Iteration 573, loss = 0.00068936\n",
      "Iteration 574, loss = 0.00068787\n",
      "Iteration 575, loss = 0.00068640\n",
      "Iteration 576, loss = 0.00068498\n",
      "Iteration 577, loss = 0.00068355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 578, loss = 0.00068208\n",
      "Iteration 579, loss = 0.00068068\n",
      "Iteration 580, loss = 0.00067928\n",
      "Iteration 581, loss = 0.00067785\n",
      "Iteration 582, loss = 0.00067646\n",
      "Iteration 583, loss = 0.00067508\n",
      "Iteration 584, loss = 0.00067365\n",
      "Iteration 585, loss = 0.00067224\n",
      "Iteration 586, loss = 0.00067091\n",
      "Iteration 587, loss = 0.00066948\n",
      "Iteration 588, loss = 0.00066811\n",
      "Iteration 589, loss = 0.00066684\n",
      "Iteration 590, loss = 0.00066538\n",
      "Iteration 591, loss = 0.00066402\n",
      "Iteration 592, loss = 0.00066267\n",
      "Iteration 593, loss = 0.00066132\n",
      "Iteration 594, loss = 0.00066000\n",
      "Iteration 595, loss = 0.00065865\n",
      "Iteration 596, loss = 0.00065732\n",
      "Iteration 597, loss = 0.00065598\n",
      "Iteration 598, loss = 0.00065472\n",
      "Iteration 599, loss = 0.00065336\n",
      "Iteration 600, loss = 0.00065207\n",
      "Iteration 601, loss = 0.00065075\n",
      "Iteration 602, loss = 0.00064943\n",
      "Iteration 603, loss = 0.00064820\n",
      "Iteration 604, loss = 0.00064686\n",
      "Iteration 605, loss = 0.00064564\n",
      "Iteration 606, loss = 0.00064431\n",
      "Iteration 607, loss = 0.00064308\n",
      "Iteration 608, loss = 0.00064180\n",
      "Iteration 609, loss = 0.00064055\n",
      "Iteration 610, loss = 0.00063928\n",
      "Iteration 611, loss = 0.00063810\n",
      "Iteration 612, loss = 0.00063682\n",
      "Iteration 613, loss = 0.00063555\n",
      "Iteration 614, loss = 0.00063429\n",
      "Iteration 615, loss = 0.00063309\n",
      "Iteration 616, loss = 0.00063185\n",
      "Iteration 617, loss = 0.00063068\n",
      "Iteration 618, loss = 0.00062941\n",
      "Iteration 619, loss = 0.00062823\n",
      "Iteration 620, loss = 0.00062698\n",
      "Iteration 621, loss = 0.00062580\n",
      "Iteration 622, loss = 0.00062463\n",
      "Iteration 623, loss = 0.00062341\n",
      "Iteration 624, loss = 0.00062224\n",
      "Iteration 625, loss = 0.00062108\n",
      "Iteration 626, loss = 0.00061988\n",
      "Iteration 627, loss = 0.00061872\n",
      "Iteration 628, loss = 0.00061755\n",
      "Iteration 629, loss = 0.00061639\n",
      "Iteration 630, loss = 0.00061522\n",
      "Iteration 631, loss = 0.00061407\n",
      "Iteration 632, loss = 0.00061293\n",
      "Iteration 633, loss = 0.00061179\n",
      "Iteration 634, loss = 0.00061066\n",
      "Iteration 635, loss = 0.00060953\n",
      "Iteration 636, loss = 0.00060837\n",
      "Iteration 637, loss = 0.00060722\n",
      "Iteration 638, loss = 0.00060615\n",
      "Iteration 639, loss = 0.00060500\n",
      "Iteration 640, loss = 0.00060390\n",
      "Iteration 641, loss = 0.00060275\n",
      "Iteration 642, loss = 0.00060165\n",
      "Iteration 643, loss = 0.00060057\n",
      "Iteration 644, loss = 0.00059946\n",
      "Iteration 645, loss = 0.00059833\n",
      "Iteration 646, loss = 0.00059730\n",
      "Iteration 647, loss = 0.00059614\n",
      "Iteration 648, loss = 0.00059506\n",
      "Iteration 649, loss = 0.00059398\n",
      "Iteration 650, loss = 0.00059290\n",
      "Iteration 651, loss = 0.00059179\n",
      "Iteration 652, loss = 0.00059073\n",
      "Iteration 653, loss = 0.00058968\n",
      "Iteration 654, loss = 0.00058861\n",
      "Iteration 655, loss = 0.00058758\n",
      "Iteration 656, loss = 0.00058656\n",
      "Iteration 657, loss = 0.00058549\n",
      "Iteration 658, loss = 0.00058444\n",
      "Iteration 659, loss = 0.00058343\n",
      "Iteration 660, loss = 0.00058238\n",
      "Iteration 661, loss = 0.00058131\n",
      "Iteration 662, loss = 0.00058030\n",
      "Iteration 663, loss = 0.00057928\n",
      "Iteration 664, loss = 0.00057827\n",
      "Iteration 665, loss = 0.00057722\n",
      "Iteration 666, loss = 0.00057621\n",
      "Iteration 667, loss = 0.00057521\n",
      "Iteration 668, loss = 0.00057416\n",
      "Iteration 669, loss = 0.00057317\n",
      "Iteration 670, loss = 0.00057213\n",
      "Iteration 671, loss = 0.00057113\n",
      "Iteration 672, loss = 0.00057015\n",
      "Iteration 673, loss = 0.00056916\n",
      "Iteration 674, loss = 0.00056817\n",
      "Iteration 675, loss = 0.00056716\n",
      "Iteration 676, loss = 0.00056619\n",
      "Iteration 677, loss = 0.00056520\n",
      "Iteration 678, loss = 0.00056426\n",
      "Iteration 679, loss = 0.00056325\n",
      "Iteration 680, loss = 0.00056232\n",
      "Iteration 681, loss = 0.00056131\n",
      "Iteration 682, loss = 0.00056035\n",
      "Iteration 683, loss = 0.00055943\n",
      "Iteration 684, loss = 0.00055844\n",
      "Iteration 685, loss = 0.00055750\n",
      "Iteration 686, loss = 0.00055657\n",
      "Iteration 687, loss = 0.00055560\n",
      "Iteration 688, loss = 0.00055464\n",
      "Iteration 689, loss = 0.00055372\n",
      "Iteration 690, loss = 0.00055278\n",
      "Iteration 691, loss = 0.00055183\n",
      "Iteration 692, loss = 0.00055088\n",
      "Iteration 693, loss = 0.00054997\n",
      "Iteration 694, loss = 0.00054907\n",
      "Iteration 695, loss = 0.00054812\n",
      "Iteration 696, loss = 0.00054722\n",
      "Iteration 697, loss = 0.00054629\n",
      "Iteration 698, loss = 0.00054541\n",
      "Iteration 699, loss = 0.00054450\n",
      "Iteration 700, loss = 0.00054356\n",
      "Iteration 701, loss = 0.00054267\n",
      "Iteration 702, loss = 0.00054180\n",
      "Iteration 703, loss = 0.00054090\n",
      "Iteration 704, loss = 0.00053999\n",
      "Iteration 705, loss = 0.00053912\n",
      "Iteration 706, loss = 0.00053823\n",
      "Iteration 707, loss = 0.00053733\n",
      "Iteration 708, loss = 0.00053649\n",
      "Iteration 709, loss = 0.00053562\n",
      "Iteration 710, loss = 0.00053473\n",
      "Iteration 711, loss = 0.00053384\n",
      "Iteration 712, loss = 0.00053297\n",
      "Iteration 713, loss = 0.00053211\n",
      "Iteration 714, loss = 0.00053126\n",
      "Iteration 715, loss = 0.00053039\n",
      "Iteration 716, loss = 0.00052954\n",
      "Iteration 717, loss = 0.00052867\n",
      "Iteration 718, loss = 0.00052785\n",
      "Iteration 719, loss = 0.00052696\n",
      "Iteration 720, loss = 0.00052612\n",
      "Iteration 721, loss = 0.00052527\n",
      "Iteration 722, loss = 0.00052444\n",
      "Iteration 723, loss = 0.00052360\n",
      "Iteration 724, loss = 0.00052274\n",
      "Iteration 725, loss = 0.00052193\n",
      "Iteration 726, loss = 0.00052107\n",
      "Iteration 727, loss = 0.00052024\n",
      "Iteration 728, loss = 0.00051941\n",
      "Iteration 729, loss = 0.00051859\n",
      "Iteration 730, loss = 0.00051783\n",
      "Iteration 731, loss = 0.00051695\n",
      "Iteration 732, loss = 0.00051614\n",
      "Iteration 733, loss = 0.00051536\n",
      "Iteration 734, loss = 0.00051452\n",
      "Iteration 735, loss = 0.00051375\n",
      "Iteration 736, loss = 0.00051294\n",
      "Iteration 737, loss = 0.00051214\n",
      "Iteration 738, loss = 0.00051136\n",
      "Iteration 739, loss = 0.00051054\n",
      "Iteration 740, loss = 0.00050976\n",
      "Iteration 741, loss = 0.00050894\n",
      "Iteration 742, loss = 0.00050815\n",
      "Iteration 743, loss = 0.00050739\n",
      "Iteration 744, loss = 0.00050661\n",
      "Iteration 745, loss = 0.00050580\n",
      "Iteration 746, loss = 0.00050503\n",
      "Iteration 747, loss = 0.00050423\n",
      "Iteration 748, loss = 0.00050345\n",
      "Iteration 749, loss = 0.00050270\n",
      "Iteration 750, loss = 0.00050193\n",
      "Iteration 751, loss = 0.00050118\n",
      "Iteration 752, loss = 0.00050041\n",
      "Iteration 753, loss = 0.00049964\n",
      "Iteration 754, loss = 0.00049890\n",
      "Iteration 755, loss = 0.00049818\n",
      "Iteration 756, loss = 0.00049738\n",
      "Iteration 757, loss = 0.00049665\n",
      "Iteration 758, loss = 0.00049588\n",
      "Iteration 759, loss = 0.00049513\n",
      "Iteration 760, loss = 0.00049440\n",
      "Iteration 761, loss = 0.00049366\n",
      "Iteration 762, loss = 0.00049289\n",
      "Iteration 763, loss = 0.00049215\n",
      "Iteration 764, loss = 0.00049143\n",
      "Iteration 765, loss = 0.00049068\n",
      "Iteration 766, loss = 0.00048996\n",
      "Iteration 767, loss = 0.00048921\n",
      "Iteration 768, loss = 0.00048851\n",
      "Iteration 769, loss = 0.00048777\n",
      "Iteration 770, loss = 0.00048703\n",
      "Iteration 771, loss = 0.00048629\n",
      "Iteration 772, loss = 0.00048563\n",
      "Iteration 773, loss = 0.00048487\n",
      "Iteration 774, loss = 0.00048419\n",
      "Iteration 775, loss = 0.00048342\n",
      "Iteration 776, loss = 0.00048275\n",
      "Iteration 777, loss = 0.00048202\n",
      "Iteration 778, loss = 0.00048133\n",
      "Iteration 779, loss = 0.00048062\n",
      "Iteration 780, loss = 0.00047992\n",
      "Iteration 781, loss = 0.00047922\n",
      "Iteration 782, loss = 0.00047853\n",
      "Iteration 783, loss = 0.00047782\n",
      "Iteration 784, loss = 0.00047712\n",
      "Iteration 785, loss = 0.00047641\n",
      "Iteration 786, loss = 0.00047573\n",
      "Iteration 787, loss = 0.00047506\n",
      "Iteration 788, loss = 0.00047438\n",
      "Iteration 789, loss = 0.00047366\n",
      "Iteration 790, loss = 0.00047298\n",
      "Iteration 791, loss = 0.00047230\n",
      "Iteration 792, loss = 0.00047162\n",
      "Iteration 793, loss = 0.00047095\n",
      "Iteration 794, loss = 0.00047031\n",
      "Iteration 795, loss = 0.00046962\n",
      "Iteration 796, loss = 0.00046897\n",
      "Iteration 797, loss = 0.00046828\n",
      "Iteration 798, loss = 0.00046763\n",
      "Iteration 799, loss = 0.00046694\n",
      "Iteration 800, loss = 0.00046629\n",
      "Iteration 801, loss = 0.00046561\n",
      "Iteration 802, loss = 0.00046497\n",
      "Iteration 803, loss = 0.00046431\n",
      "Iteration 804, loss = 0.00046365\n",
      "Iteration 805, loss = 0.00046299\n",
      "Iteration 806, loss = 0.00046235\n",
      "Iteration 807, loss = 0.00046170\n",
      "Iteration 808, loss = 0.00046106\n",
      "Iteration 809, loss = 0.00046040\n",
      "Iteration 810, loss = 0.00045977\n",
      "Iteration 811, loss = 0.00045915\n",
      "Iteration 812, loss = 0.00045849\n",
      "Iteration 813, loss = 0.00045786\n",
      "Iteration 814, loss = 0.00045723\n",
      "Iteration 815, loss = 0.00045661\n",
      "Iteration 816, loss = 0.00045598\n",
      "Iteration 817, loss = 0.00045535\n",
      "Iteration 818, loss = 0.00045472\n",
      "Iteration 819, loss = 0.00045410\n",
      "Iteration 820, loss = 0.00045349\n",
      "Iteration 821, loss = 0.00045284\n",
      "Iteration 822, loss = 0.00045226\n",
      "Iteration 823, loss = 0.00045162\n",
      "Iteration 824, loss = 0.00045102\n",
      "Iteration 825, loss = 0.00045041\n",
      "Iteration 826, loss = 0.00044976\n",
      "Iteration 827, loss = 0.00044919\n",
      "Iteration 828, loss = 0.00044858\n",
      "Iteration 829, loss = 0.00044796\n",
      "Iteration 830, loss = 0.00044735\n",
      "Iteration 831, loss = 0.00044675\n",
      "Iteration 832, loss = 0.00044614\n",
      "Iteration 833, loss = 0.00044557\n",
      "Iteration 834, loss = 0.00044492\n",
      "Iteration 835, loss = 0.00044434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 836, loss = 0.00044375\n",
      "Iteration 837, loss = 0.00044315\n",
      "Iteration 838, loss = 0.00044256\n",
      "Iteration 839, loss = 0.00044196\n",
      "Iteration 840, loss = 0.00044137\n",
      "Iteration 841, loss = 0.00044078\n",
      "Iteration 842, loss = 0.00044019\n",
      "Iteration 843, loss = 0.00043962\n",
      "Iteration 844, loss = 0.00043903\n",
      "Iteration 845, loss = 0.00043845\n",
      "Iteration 846, loss = 0.00043788\n",
      "Iteration 847, loss = 0.00043730\n",
      "Iteration 848, loss = 0.00043673\n",
      "Iteration 849, loss = 0.00043615\n",
      "Iteration 850, loss = 0.00043559\n",
      "Iteration 851, loss = 0.00043501\n",
      "Iteration 852, loss = 0.00043445\n",
      "Iteration 853, loss = 0.00043388\n",
      "Iteration 854, loss = 0.00043331\n",
      "Iteration 855, loss = 0.00043276\n",
      "Iteration 856, loss = 0.00043217\n",
      "Iteration 857, loss = 0.00043163\n",
      "Iteration 858, loss = 0.00043107\n",
      "Iteration 859, loss = 0.00043052\n",
      "Iteration 860, loss = 0.00042994\n",
      "Iteration 861, loss = 0.00042939\n",
      "Iteration 862, loss = 0.00042882\n",
      "Iteration 863, loss = 0.00042830\n",
      "Iteration 864, loss = 0.00042771\n",
      "Iteration 865, loss = 0.00042718\n",
      "Iteration 866, loss = 0.00042662\n",
      "Iteration 867, loss = 0.00042609\n",
      "Iteration 868, loss = 0.00042555\n",
      "Iteration 869, loss = 0.00042499\n",
      "Iteration 870, loss = 0.00042447\n",
      "Iteration 871, loss = 0.00042388\n",
      "Iteration 872, loss = 0.00042338\n",
      "Iteration 873, loss = 0.00042282\n",
      "Iteration 874, loss = 0.00042230\n",
      "Iteration 875, loss = 0.00042176\n",
      "Iteration 876, loss = 0.00042123\n",
      "Iteration 877, loss = 0.00042071\n",
      "Iteration 878, loss = 0.00042017\n",
      "Iteration 879, loss = 0.00041963\n",
      "Iteration 880, loss = 0.00041912\n",
      "Iteration 881, loss = 0.00041859\n",
      "Iteration 882, loss = 0.00041808\n",
      "Iteration 883, loss = 0.00041755\n",
      "Iteration 884, loss = 0.00041704\n",
      "Iteration 885, loss = 0.00041650\n",
      "Iteration 886, loss = 0.00041597\n",
      "Iteration 887, loss = 0.00041547\n",
      "Iteration 888, loss = 0.00041496\n",
      "Iteration 889, loss = 0.00041445\n",
      "Iteration 890, loss = 0.00041392\n",
      "Iteration 891, loss = 0.00041343\n",
      "Iteration 892, loss = 0.00041292\n",
      "Iteration 893, loss = 0.00041242\n",
      "Iteration 894, loss = 0.00041191\n",
      "Iteration 895, loss = 0.00041139\n",
      "Iteration 896, loss = 0.00041088\n",
      "Iteration 897, loss = 0.00041040\n",
      "Iteration 898, loss = 0.00040989\n",
      "Iteration 899, loss = 0.00040940\n",
      "Iteration 900, loss = 0.00040887\n",
      "Iteration 901, loss = 0.00040839\n",
      "Iteration 902, loss = 0.00040788\n",
      "Iteration 903, loss = 0.00040738\n",
      "Iteration 904, loss = 0.00040688\n",
      "Iteration 905, loss = 0.00040639\n",
      "Iteration 906, loss = 0.00040589\n",
      "Iteration 907, loss = 0.00040541\n",
      "Iteration 908, loss = 0.00040490\n",
      "Iteration 909, loss = 0.00040441\n",
      "Iteration 910, loss = 0.00040394\n",
      "Iteration 911, loss = 0.00040344\n",
      "Iteration 912, loss = 0.00040296\n",
      "Iteration 913, loss = 0.00040246\n",
      "Iteration 914, loss = 0.00040199\n",
      "Iteration 915, loss = 0.00040150\n",
      "Iteration 916, loss = 0.00040104\n",
      "Iteration 917, loss = 0.00040056\n",
      "Iteration 918, loss = 0.00040008\n",
      "Iteration 919, loss = 0.00039961\n",
      "Iteration 920, loss = 0.00039912\n",
      "Iteration 921, loss = 0.00039865\n",
      "Iteration 922, loss = 0.00039818\n",
      "Iteration 923, loss = 0.00039770\n",
      "Iteration 924, loss = 0.00039725\n",
      "Iteration 925, loss = 0.00039678\n",
      "Iteration 926, loss = 0.00039629\n",
      "Iteration 927, loss = 0.00039583\n",
      "Iteration 928, loss = 0.00039537\n",
      "Iteration 929, loss = 0.00039490\n",
      "Iteration 930, loss = 0.00039443\n",
      "Iteration 931, loss = 0.00039397\n",
      "Iteration 932, loss = 0.00039352\n",
      "Iteration 933, loss = 0.00039307\n",
      "Iteration 934, loss = 0.00039262\n",
      "Iteration 935, loss = 0.00039213\n",
      "Iteration 936, loss = 0.00039172\n",
      "Iteration 937, loss = 0.00039124\n",
      "Iteration 938, loss = 0.00039079\n",
      "Iteration 939, loss = 0.00039032\n",
      "Iteration 940, loss = 0.00038989\n",
      "Iteration 941, loss = 0.00038942\n",
      "Iteration 942, loss = 0.00038898\n",
      "Iteration 943, loss = 0.00038854\n",
      "Iteration 944, loss = 0.00038808\n",
      "Iteration 945, loss = 0.00038764\n",
      "Iteration 946, loss = 0.00038719\n",
      "Iteration 947, loss = 0.00038673\n",
      "Iteration 948, loss = 0.00038630\n",
      "Iteration 949, loss = 0.00038586\n",
      "Iteration 950, loss = 0.00038542\n",
      "Iteration 951, loss = 0.00038498\n",
      "Iteration 952, loss = 0.00038451\n",
      "Iteration 953, loss = 0.00038408\n",
      "Iteration 954, loss = 0.00038365\n",
      "Iteration 955, loss = 0.00038322\n",
      "Iteration 956, loss = 0.00038280\n",
      "Iteration 957, loss = 0.00038234\n",
      "Iteration 958, loss = 0.00038192\n",
      "Iteration 959, loss = 0.00038148\n",
      "Iteration 960, loss = 0.00038105\n",
      "Iteration 961, loss = 0.00038063\n",
      "Iteration 962, loss = 0.00038020\n",
      "Iteration 963, loss = 0.00037977\n",
      "Iteration 964, loss = 0.00037935\n",
      "Iteration 965, loss = 0.00037892\n",
      "Iteration 966, loss = 0.00037848\n",
      "Iteration 967, loss = 0.00037807\n",
      "Iteration 968, loss = 0.00037765\n",
      "Iteration 969, loss = 0.00037722\n",
      "Iteration 970, loss = 0.00037679\n",
      "Iteration 971, loss = 0.00037638\n",
      "Iteration 972, loss = 0.00037596\n",
      "Iteration 973, loss = 0.00037556\n",
      "Iteration 974, loss = 0.00037513\n",
      "Iteration 975, loss = 0.00037471\n",
      "Iteration 976, loss = 0.00037429\n",
      "Iteration 977, loss = 0.00037388\n",
      "Iteration 978, loss = 0.00037348\n",
      "Iteration 979, loss = 0.00037307\n",
      "Iteration 980, loss = 0.00037266\n",
      "Iteration 981, loss = 0.00037224\n",
      "Iteration 982, loss = 0.00037184\n",
      "Iteration 983, loss = 0.00037143\n",
      "Iteration 984, loss = 0.00037104\n",
      "Iteration 985, loss = 0.00037063\n",
      "Iteration 986, loss = 0.00037021\n",
      "Iteration 987, loss = 0.00036983\n",
      "Iteration 988, loss = 0.00036941\n",
      "Iteration 989, loss = 0.00036902\n",
      "Iteration 990, loss = 0.00036861\n",
      "Iteration 991, loss = 0.00036820\n",
      "Iteration 992, loss = 0.00036781\n",
      "Iteration 993, loss = 0.00036740\n",
      "Iteration 994, loss = 0.00036701\n",
      "Iteration 995, loss = 0.00036661\n",
      "Iteration 996, loss = 0.00036621\n",
      "Iteration 997, loss = 0.00036582\n",
      "Iteration 998, loss = 0.00036542\n",
      "Iteration 999, loss = 0.00036503\n",
      "Iteration 1000, loss = 0.00036465\n",
      "Attribute -> Prime\n",
      "Accuracy -> 1.0\n",
      "Iteration 1, loss = 0.44880730\n",
      "Iteration 2, loss = 0.29031350\n",
      "Iteration 3, loss = 0.20885441\n",
      "Iteration 4, loss = 0.17493430\n",
      "Iteration 5, loss = 0.16069584\n",
      "Iteration 6, loss = 0.15238085\n",
      "Iteration 7, loss = 0.14704444\n",
      "Iteration 8, loss = 0.14246982\n",
      "Iteration 9, loss = 0.13832684\n",
      "Iteration 10, loss = 0.13443960\n",
      "Iteration 11, loss = 0.13059901\n",
      "Iteration 12, loss = 0.12684096\n",
      "Iteration 13, loss = 0.12352778\n",
      "Iteration 14, loss = 0.12055904\n",
      "Iteration 15, loss = 0.11711993\n",
      "Iteration 16, loss = 0.11401233\n",
      "Iteration 17, loss = 0.11133557\n",
      "Iteration 18, loss = 0.10849521\n",
      "Iteration 19, loss = 0.10606988\n",
      "Iteration 20, loss = 0.10337959\n",
      "Iteration 21, loss = 0.10106698\n",
      "Iteration 22, loss = 0.09878089\n",
      "Iteration 23, loss = 0.09658596\n",
      "Iteration 24, loss = 0.09449310\n",
      "Iteration 25, loss = 0.09266864\n",
      "Iteration 26, loss = 0.09067958\n",
      "Iteration 27, loss = 0.08905655\n",
      "Iteration 28, loss = 0.08734344\n",
      "Iteration 29, loss = 0.08598396\n",
      "Iteration 30, loss = 0.08438078\n",
      "Iteration 31, loss = 0.08303435\n",
      "Iteration 32, loss = 0.08164971\n",
      "Iteration 33, loss = 0.08039962\n",
      "Iteration 34, loss = 0.07923808\n",
      "Iteration 35, loss = 0.07809925\n",
      "Iteration 36, loss = 0.07709006\n",
      "Iteration 37, loss = 0.07606769\n",
      "Iteration 38, loss = 0.07522839\n",
      "Iteration 39, loss = 0.07437988\n",
      "Iteration 40, loss = 0.07356959\n",
      "Iteration 41, loss = 0.07285454\n",
      "Iteration 42, loss = 0.07220072\n",
      "Iteration 43, loss = 0.07148836\n",
      "Iteration 44, loss = 0.07082087\n",
      "Iteration 45, loss = 0.07039037\n",
      "Iteration 46, loss = 0.06974068\n",
      "Iteration 47, loss = 0.06915156\n",
      "Iteration 48, loss = 0.06873628\n",
      "Iteration 49, loss = 0.06820696\n",
      "Iteration 50, loss = 0.06783161\n",
      "Iteration 51, loss = 0.06751464\n",
      "Iteration 52, loss = 0.06698258\n",
      "Iteration 53, loss = 0.06661126\n",
      "Iteration 54, loss = 0.06629878\n",
      "Iteration 55, loss = 0.06605442\n",
      "Iteration 56, loss = 0.06560428\n",
      "Iteration 57, loss = 0.06536759\n",
      "Iteration 58, loss = 0.06508024\n",
      "Iteration 59, loss = 0.06469218\n",
      "Iteration 60, loss = 0.06448297\n",
      "Iteration 61, loss = 0.06419952\n",
      "Iteration 62, loss = 0.06394982\n",
      "Iteration 63, loss = 0.06383600\n",
      "Iteration 64, loss = 0.06350766\n",
      "Iteration 65, loss = 0.06323271\n",
      "Iteration 66, loss = 0.06303939\n",
      "Iteration 67, loss = 0.06289567\n",
      "Iteration 68, loss = 0.06263967\n",
      "Iteration 69, loss = 0.06251704\n",
      "Iteration 70, loss = 0.06228541\n",
      "Iteration 71, loss = 0.06204804\n",
      "Iteration 72, loss = 0.06188362\n",
      "Iteration 73, loss = 0.06175787\n",
      "Iteration 74, loss = 0.06154025\n",
      "Iteration 75, loss = 0.06139568\n",
      "Iteration 76, loss = 0.06126649\n",
      "Iteration 77, loss = 0.06137558\n",
      "Iteration 78, loss = 0.06102105\n",
      "Iteration 79, loss = 0.06079812\n",
      "Iteration 80, loss = 0.06071230\n",
      "Iteration 81, loss = 0.06050610\n",
      "Iteration 82, loss = 0.06043242\n",
      "Iteration 83, loss = 0.06036354\n",
      "Iteration 84, loss = 0.06026429\n",
      "Iteration 85, loss = 0.06020580\n",
      "Iteration 86, loss = 0.06009735\n",
      "Iteration 87, loss = 0.06008047\n",
      "Iteration 88, loss = 0.05975555\n",
      "Iteration 89, loss = 0.05959608\n",
      "Iteration 90, loss = 0.05949607\n",
      "Iteration 91, loss = 0.05944952\n",
      "Iteration 92, loss = 0.05930407\n",
      "Iteration 93, loss = 0.05916775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 94, loss = 0.05906621\n",
      "Iteration 95, loss = 0.05897418\n",
      "Iteration 96, loss = 0.05887889\n",
      "Iteration 97, loss = 0.05881615\n",
      "Iteration 98, loss = 0.05869403\n",
      "Iteration 99, loss = 0.05854493\n",
      "Iteration 100, loss = 0.05850358\n",
      "Iteration 101, loss = 0.05845570\n",
      "Iteration 102, loss = 0.05836583\n",
      "Iteration 103, loss = 0.05820120\n",
      "Iteration 104, loss = 0.05815858\n",
      "Iteration 105, loss = 0.05799920\n",
      "Iteration 106, loss = 0.05787330\n",
      "Iteration 107, loss = 0.05785140\n",
      "Iteration 108, loss = 0.05776345\n",
      "Iteration 109, loss = 0.05769027\n",
      "Iteration 110, loss = 0.05754875\n",
      "Iteration 111, loss = 0.05743886\n",
      "Iteration 112, loss = 0.05737230\n",
      "Iteration 113, loss = 0.05750674\n",
      "Iteration 114, loss = 0.05719084\n",
      "Iteration 115, loss = 0.05714631\n",
      "Iteration 116, loss = 0.05705556\n",
      "Iteration 117, loss = 0.05699705\n",
      "Iteration 118, loss = 0.05679846\n",
      "Iteration 119, loss = 0.05680230\n",
      "Iteration 120, loss = 0.05667129\n",
      "Iteration 121, loss = 0.05671861\n",
      "Iteration 122, loss = 0.05651835\n",
      "Iteration 123, loss = 0.05667997\n",
      "Iteration 124, loss = 0.05632361\n",
      "Iteration 125, loss = 0.05634558\n",
      "Iteration 126, loss = 0.05615765\n",
      "Iteration 127, loss = 0.05604815\n",
      "Iteration 128, loss = 0.05602476\n",
      "Iteration 129, loss = 0.05598119\n",
      "Iteration 130, loss = 0.05591355\n",
      "Iteration 131, loss = 0.05573326\n",
      "Iteration 132, loss = 0.05577585\n",
      "Iteration 133, loss = 0.05557465\n",
      "Iteration 134, loss = 0.05555940\n",
      "Iteration 135, loss = 0.05565590\n",
      "Iteration 136, loss = 0.05532529\n",
      "Iteration 137, loss = 0.05522036\n",
      "Iteration 138, loss = 0.05517466\n",
      "Iteration 139, loss = 0.05507394\n",
      "Iteration 140, loss = 0.05500595\n",
      "Iteration 141, loss = 0.05490739\n",
      "Iteration 142, loss = 0.05478938\n",
      "Iteration 143, loss = 0.05473304\n",
      "Iteration 144, loss = 0.05472620\n",
      "Iteration 145, loss = 0.05455446\n",
      "Iteration 146, loss = 0.05452716\n",
      "Iteration 147, loss = 0.05448172\n",
      "Iteration 148, loss = 0.05432561\n",
      "Iteration 149, loss = 0.05429069\n",
      "Iteration 150, loss = 0.05404573\n",
      "Iteration 151, loss = 0.05401258\n",
      "Iteration 152, loss = 0.05389342\n",
      "Iteration 153, loss = 0.05385549\n",
      "Iteration 154, loss = 0.05370382\n",
      "Iteration 155, loss = 0.05359918\n",
      "Iteration 156, loss = 0.05359566\n",
      "Iteration 157, loss = 0.05340186\n",
      "Iteration 158, loss = 0.05332235\n",
      "Iteration 159, loss = 0.05324434\n",
      "Iteration 160, loss = 0.05311398\n",
      "Iteration 161, loss = 0.05305458\n",
      "Iteration 162, loss = 0.05303341\n",
      "Iteration 163, loss = 0.05287696\n",
      "Iteration 164, loss = 0.05276680\n",
      "Iteration 165, loss = 0.05267060\n",
      "Iteration 166, loss = 0.05260779\n",
      "Iteration 167, loss = 0.05266475\n",
      "Iteration 168, loss = 0.05240122\n",
      "Iteration 169, loss = 0.05236256\n",
      "Iteration 170, loss = 0.05224498\n",
      "Iteration 171, loss = 0.05217951\n",
      "Iteration 172, loss = 0.05214591\n",
      "Iteration 173, loss = 0.05191015\n",
      "Iteration 174, loss = 0.05190323\n",
      "Iteration 175, loss = 0.05183581\n",
      "Iteration 176, loss = 0.05159752\n",
      "Iteration 177, loss = 0.05155671\n",
      "Iteration 178, loss = 0.05145892\n",
      "Iteration 179, loss = 0.05156109\n",
      "Iteration 180, loss = 0.05126592\n",
      "Iteration 181, loss = 0.05121900\n",
      "Iteration 182, loss = 0.05111958\n",
      "Iteration 183, loss = 0.05105404\n",
      "Iteration 184, loss = 0.05095806\n",
      "Iteration 185, loss = 0.05078928\n",
      "Iteration 186, loss = 0.05082689\n",
      "Iteration 187, loss = 0.05065438\n",
      "Iteration 188, loss = 0.05052370\n",
      "Iteration 189, loss = 0.05046130\n",
      "Iteration 190, loss = 0.05042822\n",
      "Iteration 191, loss = 0.05038163\n",
      "Iteration 192, loss = 0.05017292\n",
      "Iteration 193, loss = 0.05020792\n",
      "Iteration 194, loss = 0.04997814\n",
      "Iteration 195, loss = 0.04987268\n",
      "Iteration 196, loss = 0.04981720\n",
      "Iteration 197, loss = 0.04979312\n",
      "Iteration 198, loss = 0.04958860\n",
      "Iteration 199, loss = 0.04957079\n",
      "Iteration 200, loss = 0.04962600\n",
      "Iteration 201, loss = 0.04931073\n",
      "Iteration 202, loss = 0.04933069\n",
      "Iteration 203, loss = 0.04926256\n",
      "Iteration 204, loss = 0.04907615\n",
      "Iteration 205, loss = 0.04901231\n",
      "Iteration 206, loss = 0.04899029\n",
      "Iteration 207, loss = 0.04880119\n",
      "Iteration 208, loss = 0.04872400\n",
      "Iteration 209, loss = 0.04867894\n",
      "Iteration 210, loss = 0.04861281\n",
      "Iteration 211, loss = 0.04851236\n",
      "Iteration 212, loss = 0.04855900\n",
      "Iteration 213, loss = 0.04820895\n",
      "Iteration 214, loss = 0.04812936\n",
      "Iteration 215, loss = 0.04810891\n",
      "Iteration 216, loss = 0.04790822\n",
      "Iteration 217, loss = 0.04794199\n",
      "Iteration 218, loss = 0.04784620\n",
      "Iteration 219, loss = 0.04767995\n",
      "Iteration 220, loss = 0.04768255\n",
      "Iteration 221, loss = 0.04770010\n",
      "Iteration 222, loss = 0.04734236\n",
      "Iteration 223, loss = 0.04746444\n",
      "Iteration 224, loss = 0.04741939\n",
      "Iteration 225, loss = 0.04718699\n",
      "Iteration 226, loss = 0.04704337\n",
      "Iteration 227, loss = 0.04688643\n",
      "Iteration 228, loss = 0.04684378\n",
      "Iteration 229, loss = 0.04699539\n",
      "Iteration 230, loss = 0.04664804\n",
      "Iteration 231, loss = 0.04655845\n",
      "Iteration 232, loss = 0.04639608\n",
      "Iteration 233, loss = 0.04643554\n",
      "Iteration 234, loss = 0.04653803\n",
      "Iteration 235, loss = 0.04607938\n",
      "Iteration 236, loss = 0.04597881\n",
      "Iteration 237, loss = 0.04592252\n",
      "Iteration 238, loss = 0.04587883\n",
      "Iteration 239, loss = 0.04577236\n",
      "Iteration 240, loss = 0.04557957\n",
      "Iteration 241, loss = 0.04550681\n",
      "Iteration 242, loss = 0.04546701\n",
      "Iteration 243, loss = 0.04529173\n",
      "Iteration 244, loss = 0.04535751\n",
      "Iteration 245, loss = 0.04511191\n",
      "Iteration 246, loss = 0.04517007\n",
      "Iteration 247, loss = 0.04514327\n",
      "Iteration 248, loss = 0.04485941\n",
      "Iteration 249, loss = 0.04468783\n",
      "Iteration 250, loss = 0.04488091\n",
      "Iteration 251, loss = 0.04460753\n",
      "Iteration 252, loss = 0.04451014\n",
      "Iteration 253, loss = 0.04442246\n",
      "Iteration 254, loss = 0.04427865\n",
      "Iteration 255, loss = 0.04420170\n",
      "Iteration 256, loss = 0.04398263\n",
      "Iteration 257, loss = 0.04399682\n",
      "Iteration 258, loss = 0.04374677\n",
      "Iteration 259, loss = 0.04368736\n",
      "Iteration 260, loss = 0.04355931\n",
      "Iteration 261, loss = 0.04347121\n",
      "Iteration 262, loss = 0.04332429\n",
      "Iteration 263, loss = 0.04319967\n",
      "Iteration 264, loss = 0.04316977\n",
      "Iteration 265, loss = 0.04312901\n",
      "Iteration 266, loss = 0.04290160\n",
      "Iteration 267, loss = 0.04284271\n",
      "Iteration 268, loss = 0.04293012\n",
      "Iteration 269, loss = 0.04267346\n",
      "Iteration 270, loss = 0.04251893\n",
      "Iteration 271, loss = 0.04241158\n",
      "Iteration 272, loss = 0.04223917\n",
      "Iteration 273, loss = 0.04206776\n",
      "Iteration 274, loss = 0.04206785\n",
      "Iteration 275, loss = 0.04184143\n",
      "Iteration 276, loss = 0.04176224\n",
      "Iteration 277, loss = 0.04170411\n",
      "Iteration 278, loss = 0.04165163\n",
      "Iteration 279, loss = 0.04160272\n",
      "Iteration 280, loss = 0.04128733\n",
      "Iteration 281, loss = 0.04122412\n",
      "Iteration 282, loss = 0.04104688\n",
      "Iteration 283, loss = 0.04090989\n",
      "Iteration 284, loss = 0.04079915\n",
      "Iteration 285, loss = 0.04069522\n",
      "Iteration 286, loss = 0.04065119\n",
      "Iteration 287, loss = 0.04046922\n",
      "Iteration 288, loss = 0.04053585\n",
      "Iteration 289, loss = 0.04054401\n",
      "Iteration 290, loss = 0.04012363\n",
      "Iteration 291, loss = 0.04004643\n",
      "Iteration 292, loss = 0.03994621\n",
      "Iteration 293, loss = 0.04002777\n",
      "Iteration 294, loss = 0.03971048\n",
      "Iteration 295, loss = 0.03968967\n",
      "Iteration 296, loss = 0.03949554\n",
      "Iteration 297, loss = 0.03930608\n",
      "Iteration 298, loss = 0.03914332\n",
      "Iteration 299, loss = 0.03906470\n",
      "Iteration 300, loss = 0.03896854\n",
      "Iteration 301, loss = 0.03907530\n",
      "Iteration 302, loss = 0.03885048\n",
      "Iteration 303, loss = 0.03859377\n",
      "Iteration 304, loss = 0.03845722\n",
      "Iteration 305, loss = 0.03830273\n",
      "Iteration 306, loss = 0.03818635\n",
      "Iteration 307, loss = 0.03811184\n",
      "Iteration 308, loss = 0.03810392\n",
      "Iteration 309, loss = 0.03786190\n",
      "Iteration 310, loss = 0.03814352\n",
      "Iteration 311, loss = 0.03757952\n",
      "Iteration 312, loss = 0.03749558\n",
      "Iteration 313, loss = 0.03739883\n",
      "Iteration 314, loss = 0.03722742\n",
      "Iteration 315, loss = 0.03720221\n",
      "Iteration 316, loss = 0.03702399\n",
      "Iteration 317, loss = 0.03718133\n",
      "Iteration 318, loss = 0.03689731\n",
      "Iteration 319, loss = 0.03673409\n",
      "Iteration 320, loss = 0.03682422\n",
      "Iteration 321, loss = 0.03627732\n",
      "Iteration 322, loss = 0.03631612\n",
      "Iteration 323, loss = 0.03604548\n",
      "Iteration 324, loss = 0.03605439\n",
      "Iteration 325, loss = 0.03597992\n",
      "Iteration 326, loss = 0.03577120\n",
      "Iteration 327, loss = 0.03574964\n",
      "Iteration 328, loss = 0.03547192\n",
      "Iteration 329, loss = 0.03539192\n",
      "Iteration 330, loss = 0.03512252\n",
      "Iteration 331, loss = 0.03511573\n",
      "Iteration 332, loss = 0.03502805\n",
      "Iteration 333, loss = 0.03478994\n",
      "Iteration 334, loss = 0.03470724\n",
      "Iteration 335, loss = 0.03450687\n",
      "Iteration 336, loss = 0.03441442\n",
      "Iteration 337, loss = 0.03431815\n",
      "Iteration 338, loss = 0.03413573\n",
      "Iteration 339, loss = 0.03401704\n",
      "Iteration 340, loss = 0.03397283\n",
      "Iteration 341, loss = 0.03387821\n",
      "Iteration 342, loss = 0.03356687\n",
      "Iteration 343, loss = 0.03346131\n",
      "Iteration 344, loss = 0.03335606\n",
      "Iteration 345, loss = 0.03322425\n",
      "Iteration 346, loss = 0.03308990\n",
      "Iteration 347, loss = 0.03309089\n",
      "Iteration 348, loss = 0.03285397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 349, loss = 0.03278246\n",
      "Iteration 350, loss = 0.03264305\n",
      "Iteration 351, loss = 0.03245685\n",
      "Iteration 352, loss = 0.03229451\n",
      "Iteration 353, loss = 0.03230750\n",
      "Iteration 354, loss = 0.03227482\n",
      "Iteration 355, loss = 0.03193129\n",
      "Iteration 356, loss = 0.03195375\n",
      "Iteration 357, loss = 0.03169324\n",
      "Iteration 358, loss = 0.03159629\n",
      "Iteration 359, loss = 0.03144274\n",
      "Iteration 360, loss = 0.03127698\n",
      "Iteration 361, loss = 0.03130272\n",
      "Iteration 362, loss = 0.03100131\n",
      "Iteration 363, loss = 0.03095980\n",
      "Iteration 364, loss = 0.03102439\n",
      "Iteration 365, loss = 0.03068589\n",
      "Iteration 366, loss = 0.03046941\n",
      "Iteration 367, loss = 0.03043169\n",
      "Iteration 368, loss = 0.03037127\n",
      "Iteration 369, loss = 0.03014491\n",
      "Iteration 370, loss = 0.02990556\n",
      "Iteration 371, loss = 0.02984073\n",
      "Iteration 372, loss = 0.02973042\n",
      "Iteration 373, loss = 0.02956689\n",
      "Iteration 374, loss = 0.02965860\n",
      "Iteration 375, loss = 0.02930922\n",
      "Iteration 376, loss = 0.02921857\n",
      "Iteration 377, loss = 0.02909992\n",
      "Iteration 378, loss = 0.02889329\n",
      "Iteration 379, loss = 0.02877614\n",
      "Iteration 380, loss = 0.02869836\n",
      "Iteration 381, loss = 0.02849852\n",
      "Iteration 382, loss = 0.02840147\n",
      "Iteration 383, loss = 0.02838976\n",
      "Iteration 384, loss = 0.02815901\n",
      "Iteration 385, loss = 0.02804985\n",
      "Iteration 386, loss = 0.02800344\n",
      "Iteration 387, loss = 0.02772733\n",
      "Iteration 388, loss = 0.02764520\n",
      "Iteration 389, loss = 0.02748633\n",
      "Iteration 390, loss = 0.02746127\n",
      "Iteration 391, loss = 0.02718632\n",
      "Iteration 392, loss = 0.02717369\n",
      "Iteration 393, loss = 0.02706864\n",
      "Iteration 394, loss = 0.02691295\n",
      "Iteration 395, loss = 0.02668287\n",
      "Iteration 396, loss = 0.02652683\n",
      "Iteration 397, loss = 0.02649273\n",
      "Iteration 398, loss = 0.02634180\n",
      "Iteration 399, loss = 0.02619207\n",
      "Iteration 400, loss = 0.02608172\n",
      "Iteration 401, loss = 0.02604622\n",
      "Iteration 402, loss = 0.02583392\n",
      "Iteration 403, loss = 0.02567516\n",
      "Iteration 404, loss = 0.02560868\n",
      "Iteration 405, loss = 0.02543408\n",
      "Iteration 406, loss = 0.02532719\n",
      "Iteration 407, loss = 0.02538609\n",
      "Iteration 408, loss = 0.02508388\n",
      "Iteration 409, loss = 0.02490061\n",
      "Iteration 410, loss = 0.02476271\n",
      "Iteration 411, loss = 0.02468787\n",
      "Iteration 412, loss = 0.02463146\n",
      "Iteration 413, loss = 0.02457454\n",
      "Iteration 414, loss = 0.02429824\n",
      "Iteration 415, loss = 0.02420273\n",
      "Iteration 416, loss = 0.02397339\n",
      "Iteration 417, loss = 0.02391581\n",
      "Iteration 418, loss = 0.02379948\n",
      "Iteration 419, loss = 0.02373067\n",
      "Iteration 420, loss = 0.02355480\n",
      "Iteration 421, loss = 0.02339918\n",
      "Iteration 422, loss = 0.02324120\n",
      "Iteration 423, loss = 0.02310843\n",
      "Iteration 424, loss = 0.02296954\n",
      "Iteration 425, loss = 0.02286839\n",
      "Iteration 426, loss = 0.02286596\n",
      "Iteration 427, loss = 0.02266272\n",
      "Iteration 428, loss = 0.02249059\n",
      "Iteration 429, loss = 0.02248638\n",
      "Iteration 430, loss = 0.02227104\n",
      "Iteration 431, loss = 0.02218077\n",
      "Iteration 432, loss = 0.02209200\n",
      "Iteration 433, loss = 0.02188665\n",
      "Iteration 434, loss = 0.02174325\n",
      "Iteration 435, loss = 0.02164621\n",
      "Iteration 436, loss = 0.02168750\n",
      "Iteration 437, loss = 0.02141698\n",
      "Iteration 438, loss = 0.02133556\n",
      "Iteration 439, loss = 0.02119075\n",
      "Iteration 440, loss = 0.02106976\n",
      "Iteration 441, loss = 0.02094749\n",
      "Iteration 442, loss = 0.02086717\n",
      "Iteration 443, loss = 0.02066847\n",
      "Iteration 444, loss = 0.02057527\n",
      "Iteration 445, loss = 0.02044580\n",
      "Iteration 446, loss = 0.02036064\n",
      "Iteration 447, loss = 0.02022890\n",
      "Iteration 448, loss = 0.02017741\n",
      "Iteration 449, loss = 0.01999989\n",
      "Iteration 450, loss = 0.02002640\n",
      "Iteration 451, loss = 0.01968731\n",
      "Iteration 452, loss = 0.01973999\n",
      "Iteration 453, loss = 0.01975227\n",
      "Iteration 454, loss = 0.01955332\n",
      "Iteration 455, loss = 0.01927893\n",
      "Iteration 456, loss = 0.01918035\n",
      "Iteration 457, loss = 0.01901050\n",
      "Iteration 458, loss = 0.01901982\n",
      "Iteration 459, loss = 0.01880298\n",
      "Iteration 460, loss = 0.01874734\n",
      "Iteration 461, loss = 0.01861545\n",
      "Iteration 462, loss = 0.01856005\n",
      "Iteration 463, loss = 0.01840206\n",
      "Iteration 464, loss = 0.01832618\n",
      "Iteration 465, loss = 0.01814653\n",
      "Iteration 466, loss = 0.01809213\n",
      "Iteration 467, loss = 0.01786471\n",
      "Iteration 468, loss = 0.01778963\n",
      "Iteration 469, loss = 0.01773937\n",
      "Iteration 470, loss = 0.01761985\n",
      "Iteration 471, loss = 0.01753536\n",
      "Iteration 472, loss = 0.01739385\n",
      "Iteration 473, loss = 0.01727091\n",
      "Iteration 474, loss = 0.01730574\n",
      "Iteration 475, loss = 0.01697293\n",
      "Iteration 476, loss = 0.01702298\n",
      "Iteration 477, loss = 0.01676735\n",
      "Iteration 478, loss = 0.01670480\n",
      "Iteration 479, loss = 0.01658439\n",
      "Iteration 480, loss = 0.01659085\n",
      "Iteration 481, loss = 0.01639585\n",
      "Iteration 482, loss = 0.01638870\n",
      "Iteration 483, loss = 0.01616375\n",
      "Iteration 484, loss = 0.01616610\n",
      "Iteration 485, loss = 0.01591025\n",
      "Iteration 486, loss = 0.01601311\n",
      "Iteration 487, loss = 0.01572609\n",
      "Iteration 488, loss = 0.01578499\n",
      "Iteration 489, loss = 0.01546965\n",
      "Iteration 490, loss = 0.01557917\n",
      "Iteration 491, loss = 0.01537106\n",
      "Iteration 492, loss = 0.01538419\n",
      "Iteration 493, loss = 0.01524097\n",
      "Iteration 494, loss = 0.01500914\n",
      "Iteration 495, loss = 0.01514977\n",
      "Iteration 496, loss = 0.01491548\n",
      "Iteration 497, loss = 0.01478885\n",
      "Iteration 498, loss = 0.01467848\n",
      "Iteration 499, loss = 0.01455677\n",
      "Iteration 500, loss = 0.01462569\n",
      "Iteration 501, loss = 0.01436603\n",
      "Iteration 502, loss = 0.01429546\n",
      "Iteration 503, loss = 0.01418403\n",
      "Iteration 504, loss = 0.01406002\n",
      "Iteration 505, loss = 0.01403388\n",
      "Iteration 506, loss = 0.01389733\n",
      "Iteration 507, loss = 0.01384856\n",
      "Iteration 508, loss = 0.01376307\n",
      "Iteration 509, loss = 0.01358820\n",
      "Iteration 510, loss = 0.01352420\n",
      "Iteration 511, loss = 0.01345240\n",
      "Iteration 512, loss = 0.01338441\n",
      "Iteration 513, loss = 0.01329616\n",
      "Iteration 514, loss = 0.01316992\n",
      "Iteration 515, loss = 0.01309534\n",
      "Iteration 516, loss = 0.01301964\n",
      "Iteration 517, loss = 0.01288371\n",
      "Iteration 518, loss = 0.01281420\n",
      "Iteration 519, loss = 0.01274904\n",
      "Iteration 520, loss = 0.01268336\n",
      "Iteration 521, loss = 0.01257285\n",
      "Iteration 522, loss = 0.01252948\n",
      "Iteration 523, loss = 0.01243152\n",
      "Iteration 524, loss = 0.01233874\n",
      "Iteration 525, loss = 0.01222506\n",
      "Iteration 526, loss = 0.01216426\n",
      "Iteration 527, loss = 0.01207845\n",
      "Iteration 528, loss = 0.01205868\n",
      "Iteration 529, loss = 0.01199379\n",
      "Iteration 530, loss = 0.01189570\n",
      "Iteration 531, loss = 0.01184491\n",
      "Iteration 532, loss = 0.01175346\n",
      "Iteration 533, loss = 0.01165071\n",
      "Iteration 534, loss = 0.01159598\n",
      "Iteration 535, loss = 0.01147871\n",
      "Iteration 536, loss = 0.01140583\n",
      "Iteration 537, loss = 0.01136298\n",
      "Iteration 538, loss = 0.01126210\n",
      "Iteration 539, loss = 0.01115322\n",
      "Iteration 540, loss = 0.01116052\n",
      "Iteration 541, loss = 0.01102336\n",
      "Iteration 542, loss = 0.01102446\n",
      "Iteration 543, loss = 0.01090766\n",
      "Iteration 544, loss = 0.01087769\n",
      "Iteration 545, loss = 0.01075139\n",
      "Iteration 546, loss = 0.01068746\n",
      "Iteration 547, loss = 0.01059511\n",
      "Iteration 548, loss = 0.01052186\n",
      "Iteration 549, loss = 0.01048078\n",
      "Iteration 550, loss = 0.01043870\n",
      "Iteration 551, loss = 0.01041504\n",
      "Iteration 552, loss = 0.01032627\n",
      "Iteration 553, loss = 0.01023239\n",
      "Iteration 554, loss = 0.01018600\n",
      "Iteration 555, loss = 0.01008314\n",
      "Iteration 556, loss = 0.01000762\n",
      "Iteration 557, loss = 0.00993848\n",
      "Iteration 558, loss = 0.00987643\n",
      "Iteration 559, loss = 0.00990328\n",
      "Iteration 560, loss = 0.00986357\n",
      "Iteration 561, loss = 0.00983686\n",
      "Iteration 562, loss = 0.00961910\n",
      "Iteration 563, loss = 0.00963841\n",
      "Iteration 564, loss = 0.00950222\n",
      "Iteration 565, loss = 0.00944949\n",
      "Iteration 566, loss = 0.00949368\n",
      "Iteration 567, loss = 0.00932716\n",
      "Iteration 568, loss = 0.00930042\n",
      "Iteration 569, loss = 0.00920434\n",
      "Iteration 570, loss = 0.00914585\n",
      "Iteration 571, loss = 0.00907624\n",
      "Iteration 572, loss = 0.00907302\n",
      "Iteration 573, loss = 0.00901379\n",
      "Iteration 574, loss = 0.00894642\n",
      "Iteration 575, loss = 0.00885122\n",
      "Iteration 576, loss = 0.00880002\n",
      "Iteration 577, loss = 0.00875165\n",
      "Iteration 578, loss = 0.00870159\n",
      "Iteration 579, loss = 0.00864161\n",
      "Iteration 580, loss = 0.00861746\n",
      "Iteration 581, loss = 0.00854659\n",
      "Iteration 582, loss = 0.00847241\n",
      "Iteration 583, loss = 0.00844580\n",
      "Iteration 584, loss = 0.00835445\n",
      "Iteration 585, loss = 0.00831971\n",
      "Iteration 586, loss = 0.00827269\n",
      "Iteration 587, loss = 0.00823687\n",
      "Iteration 588, loss = 0.00815787\n",
      "Iteration 589, loss = 0.00808865\n",
      "Iteration 590, loss = 0.00806832\n",
      "Iteration 591, loss = 0.00800302\n",
      "Iteration 592, loss = 0.00801639\n",
      "Iteration 593, loss = 0.00793707\n",
      "Iteration 594, loss = 0.00788085\n",
      "Iteration 595, loss = 0.00781810\n",
      "Iteration 596, loss = 0.00775312\n",
      "Iteration 597, loss = 0.00774378\n",
      "Iteration 598, loss = 0.00772539\n",
      "Iteration 599, loss = 0.00761200\n",
      "Iteration 600, loss = 0.00757705\n",
      "Iteration 601, loss = 0.00754973\n",
      "Iteration 602, loss = 0.00749491\n",
      "Iteration 603, loss = 0.00744638\n",
      "Iteration 604, loss = 0.00741926\n",
      "Iteration 605, loss = 0.00734551\n",
      "Iteration 606, loss = 0.00732058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 607, loss = 0.00727523\n",
      "Iteration 608, loss = 0.00722297\n",
      "Iteration 609, loss = 0.00717292\n",
      "Iteration 610, loss = 0.00714242\n",
      "Iteration 611, loss = 0.00711250\n",
      "Iteration 612, loss = 0.00706088\n",
      "Iteration 613, loss = 0.00703353\n",
      "Iteration 614, loss = 0.00696588\n",
      "Iteration 615, loss = 0.00693914\n",
      "Iteration 616, loss = 0.00689620\n",
      "Iteration 617, loss = 0.00684538\n",
      "Iteration 618, loss = 0.00681718\n",
      "Iteration 619, loss = 0.00681555\n",
      "Iteration 620, loss = 0.00673640\n",
      "Iteration 621, loss = 0.00669884\n",
      "Iteration 622, loss = 0.00665479\n",
      "Iteration 623, loss = 0.00662974\n",
      "Iteration 624, loss = 0.00659360\n",
      "Iteration 625, loss = 0.00655478\n",
      "Iteration 626, loss = 0.00650125\n",
      "Iteration 627, loss = 0.00646033\n",
      "Iteration 628, loss = 0.00644821\n",
      "Iteration 629, loss = 0.00638910\n",
      "Iteration 630, loss = 0.00636470\n",
      "Iteration 631, loss = 0.00632651\n",
      "Iteration 632, loss = 0.00626193\n",
      "Iteration 633, loss = 0.00627044\n",
      "Iteration 634, loss = 0.00626663\n",
      "Iteration 635, loss = 0.00618551\n",
      "Iteration 636, loss = 0.00613836\n",
      "Iteration 637, loss = 0.00611365\n",
      "Iteration 638, loss = 0.00608619\n",
      "Iteration 639, loss = 0.00606613\n",
      "Iteration 640, loss = 0.00599642\n",
      "Iteration 641, loss = 0.00597228\n",
      "Iteration 642, loss = 0.00593450\n",
      "Iteration 643, loss = 0.00590575\n",
      "Iteration 644, loss = 0.00590660\n",
      "Iteration 645, loss = 0.00585185\n",
      "Iteration 646, loss = 0.00582566\n",
      "Iteration 647, loss = 0.00577777\n",
      "Iteration 648, loss = 0.00575466\n",
      "Iteration 649, loss = 0.00570767\n",
      "Iteration 650, loss = 0.00567899\n",
      "Iteration 651, loss = 0.00569081\n",
      "Iteration 652, loss = 0.00562420\n",
      "Iteration 653, loss = 0.00560062\n",
      "Iteration 654, loss = 0.00555539\n",
      "Iteration 655, loss = 0.00553345\n",
      "Iteration 656, loss = 0.00549367\n",
      "Iteration 657, loss = 0.00546851\n",
      "Iteration 658, loss = 0.00544529\n",
      "Iteration 659, loss = 0.00541482\n",
      "Iteration 660, loss = 0.00538566\n",
      "Iteration 661, loss = 0.00535060\n",
      "Iteration 662, loss = 0.00534173\n",
      "Iteration 663, loss = 0.00531210\n",
      "Iteration 664, loss = 0.00530601\n",
      "Iteration 665, loss = 0.00522693\n",
      "Iteration 666, loss = 0.00529669\n",
      "Iteration 667, loss = 0.00519877\n",
      "Iteration 668, loss = 0.00516742\n",
      "Iteration 669, loss = 0.00514164\n",
      "Iteration 670, loss = 0.00512333\n",
      "Iteration 671, loss = 0.00508194\n",
      "Iteration 672, loss = 0.00504871\n",
      "Iteration 673, loss = 0.00502604\n",
      "Iteration 674, loss = 0.00500177\n",
      "Iteration 675, loss = 0.00497143\n",
      "Iteration 676, loss = 0.00495152\n",
      "Iteration 677, loss = 0.00491732\n",
      "Iteration 678, loss = 0.00491032\n",
      "Iteration 679, loss = 0.00487322\n",
      "Iteration 680, loss = 0.00485780\n",
      "Iteration 681, loss = 0.00482556\n",
      "Iteration 682, loss = 0.00481132\n",
      "Iteration 683, loss = 0.00480021\n",
      "Iteration 684, loss = 0.00475644\n",
      "Iteration 685, loss = 0.00473561\n",
      "Iteration 686, loss = 0.00469843\n",
      "Iteration 687, loss = 0.00468667\n",
      "Iteration 688, loss = 0.00468865\n",
      "Iteration 689, loss = 0.00464016\n",
      "Iteration 690, loss = 0.00467480\n",
      "Iteration 691, loss = 0.00460022\n",
      "Iteration 692, loss = 0.00456820\n",
      "Iteration 693, loss = 0.00454043\n",
      "Iteration 694, loss = 0.00454077\n",
      "Iteration 695, loss = 0.00449992\n",
      "Iteration 696, loss = 0.00448486\n",
      "Iteration 697, loss = 0.00445468\n",
      "Iteration 698, loss = 0.00443125\n",
      "Iteration 699, loss = 0.00442378\n",
      "Iteration 700, loss = 0.00439488\n",
      "Iteration 701, loss = 0.00437467\n",
      "Iteration 702, loss = 0.00434755\n",
      "Iteration 703, loss = 0.00432791\n",
      "Iteration 704, loss = 0.00430547\n",
      "Iteration 705, loss = 0.00428832\n",
      "Iteration 706, loss = 0.00426164\n",
      "Iteration 707, loss = 0.00425287\n",
      "Iteration 708, loss = 0.00424019\n",
      "Iteration 709, loss = 0.00420550\n",
      "Iteration 710, loss = 0.00418862\n",
      "Iteration 711, loss = 0.00417766\n",
      "Iteration 712, loss = 0.00414481\n",
      "Iteration 713, loss = 0.00413258\n",
      "Iteration 714, loss = 0.00419134\n",
      "Iteration 715, loss = 0.00408873\n",
      "Iteration 716, loss = 0.00406982\n",
      "Iteration 717, loss = 0.00405405\n",
      "Iteration 718, loss = 0.00404258\n",
      "Iteration 719, loss = 0.00401628\n",
      "Iteration 720, loss = 0.00400007\n",
      "Iteration 721, loss = 0.00397854\n",
      "Iteration 722, loss = 0.00396291\n",
      "Iteration 723, loss = 0.00394384\n",
      "Iteration 724, loss = 0.00395934\n",
      "Iteration 725, loss = 0.00391498\n",
      "Iteration 726, loss = 0.00389057\n",
      "Iteration 727, loss = 0.00387977\n",
      "Iteration 728, loss = 0.00384452\n",
      "Iteration 729, loss = 0.00383937\n",
      "Iteration 730, loss = 0.00381435\n",
      "Iteration 731, loss = 0.00380117\n",
      "Iteration 732, loss = 0.00378396\n",
      "Iteration 733, loss = 0.00376772\n",
      "Iteration 734, loss = 0.00375755\n",
      "Iteration 735, loss = 0.00374046\n",
      "Iteration 736, loss = 0.00371882\n",
      "Iteration 737, loss = 0.00371008\n",
      "Iteration 738, loss = 0.00368067\n",
      "Iteration 739, loss = 0.00367031\n",
      "Iteration 740, loss = 0.00366026\n",
      "Iteration 741, loss = 0.00363539\n",
      "Iteration 742, loss = 0.00362510\n",
      "Iteration 743, loss = 0.00360344\n",
      "Iteration 744, loss = 0.00359010\n",
      "Iteration 745, loss = 0.00357761\n",
      "Iteration 746, loss = 0.00357259\n",
      "Iteration 747, loss = 0.00354217\n",
      "Iteration 748, loss = 0.00352689\n",
      "Iteration 749, loss = 0.00351411\n",
      "Iteration 750, loss = 0.00350420\n",
      "Iteration 751, loss = 0.00352241\n",
      "Iteration 752, loss = 0.00350472\n",
      "Iteration 753, loss = 0.00346201\n",
      "Iteration 754, loss = 0.00344429\n",
      "Iteration 755, loss = 0.00342336\n",
      "Iteration 756, loss = 0.00341622\n",
      "Iteration 757, loss = 0.00339463\n",
      "Iteration 758, loss = 0.00339337\n",
      "Iteration 759, loss = 0.00337623\n",
      "Iteration 760, loss = 0.00337810\n",
      "Iteration 761, loss = 0.00334734\n",
      "Iteration 762, loss = 0.00332780\n",
      "Iteration 763, loss = 0.00331700\n",
      "Iteration 764, loss = 0.00331898\n",
      "Iteration 765, loss = 0.00328187\n",
      "Iteration 766, loss = 0.00327033\n",
      "Iteration 767, loss = 0.00326064\n",
      "Iteration 768, loss = 0.00325138\n",
      "Iteration 769, loss = 0.00323977\n",
      "Iteration 770, loss = 0.00322079\n",
      "Iteration 771, loss = 0.00320615\n",
      "Iteration 772, loss = 0.00320287\n",
      "Iteration 773, loss = 0.00318213\n",
      "Iteration 774, loss = 0.00317432\n",
      "Iteration 775, loss = 0.00315474\n",
      "Iteration 776, loss = 0.00313940\n",
      "Iteration 777, loss = 0.00313380\n",
      "Iteration 778, loss = 0.00312322\n",
      "Iteration 779, loss = 0.00310219\n",
      "Iteration 780, loss = 0.00309306\n",
      "Iteration 781, loss = 0.00308908\n",
      "Iteration 782, loss = 0.00307310\n",
      "Iteration 783, loss = 0.00305689\n",
      "Iteration 784, loss = 0.00304705\n",
      "Iteration 785, loss = 0.00304638\n",
      "Iteration 786, loss = 0.00301634\n",
      "Iteration 787, loss = 0.00301394\n",
      "Iteration 788, loss = 0.00299490\n",
      "Iteration 789, loss = 0.00299331\n",
      "Iteration 790, loss = 0.00297713\n",
      "Iteration 791, loss = 0.00296234\n",
      "Iteration 792, loss = 0.00295316\n",
      "Iteration 793, loss = 0.00294459\n",
      "Iteration 794, loss = 0.00292723\n",
      "Iteration 795, loss = 0.00292118\n",
      "Iteration 796, loss = 0.00290329\n",
      "Iteration 797, loss = 0.00289234\n",
      "Iteration 798, loss = 0.00288434\n",
      "Iteration 799, loss = 0.00287293\n",
      "Iteration 800, loss = 0.00285952\n",
      "Iteration 801, loss = 0.00284941\n",
      "Iteration 802, loss = 0.00284120\n",
      "Iteration 803, loss = 0.00284125\n",
      "Iteration 804, loss = 0.00282943\n",
      "Iteration 805, loss = 0.00281438\n",
      "Iteration 806, loss = 0.00279874\n",
      "Iteration 807, loss = 0.00279360\n",
      "Iteration 808, loss = 0.00278147\n",
      "Iteration 809, loss = 0.00276982\n",
      "Iteration 810, loss = 0.00275803\n",
      "Iteration 811, loss = 0.00274944\n",
      "Iteration 812, loss = 0.00274658\n",
      "Iteration 813, loss = 0.00273033\n",
      "Iteration 814, loss = 0.00272141\n",
      "Iteration 815, loss = 0.00271428\n",
      "Iteration 816, loss = 0.00269745\n",
      "Iteration 817, loss = 0.00269081\n",
      "Iteration 818, loss = 0.00267737\n",
      "Iteration 819, loss = 0.00266729\n",
      "Iteration 820, loss = 0.00266206\n",
      "Iteration 821, loss = 0.00264840\n",
      "Iteration 822, loss = 0.00264830\n",
      "Iteration 823, loss = 0.00262332\n",
      "Iteration 824, loss = 0.00262031\n",
      "Iteration 825, loss = 0.00261952\n",
      "Iteration 826, loss = 0.00260409\n",
      "Iteration 827, loss = 0.00259400\n",
      "Iteration 828, loss = 0.00258419\n",
      "Iteration 829, loss = 0.00257733\n",
      "Iteration 830, loss = 0.00256148\n",
      "Iteration 831, loss = 0.00255728\n",
      "Iteration 832, loss = 0.00255370\n",
      "Iteration 833, loss = 0.00254164\n",
      "Iteration 834, loss = 0.00253055\n",
      "Iteration 835, loss = 0.00252268\n",
      "Iteration 836, loss = 0.00250912\n",
      "Iteration 837, loss = 0.00250526\n",
      "Iteration 838, loss = 0.00249279\n",
      "Iteration 839, loss = 0.00248466\n",
      "Iteration 840, loss = 0.00247843\n",
      "Iteration 841, loss = 0.00248439\n",
      "Iteration 842, loss = 0.00245822\n",
      "Iteration 843, loss = 0.00246099\n",
      "Iteration 844, loss = 0.00244486\n",
      "Iteration 845, loss = 0.00244456\n",
      "Iteration 846, loss = 0.00242955\n",
      "Iteration 847, loss = 0.00242704\n",
      "Iteration 848, loss = 0.00241049\n",
      "Iteration 849, loss = 0.00240466\n",
      "Iteration 850, loss = 0.00240437\n",
      "Iteration 851, loss = 0.00238284\n",
      "Iteration 852, loss = 0.00238500\n",
      "Iteration 853, loss = 0.00237045\n",
      "Iteration 854, loss = 0.00236393\n",
      "Iteration 855, loss = 0.00235695\n",
      "Iteration 856, loss = 0.00234676\n",
      "Iteration 857, loss = 0.00233871\n",
      "Iteration 858, loss = 0.00233331\n",
      "Iteration 859, loss = 0.00232376\n",
      "Iteration 860, loss = 0.00231795\n",
      "Iteration 861, loss = 0.00230929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 862, loss = 0.00230970\n",
      "Iteration 863, loss = 0.00229625\n",
      "Iteration 864, loss = 0.00228782\n",
      "Iteration 865, loss = 0.00227877\n",
      "Iteration 866, loss = 0.00227301\n",
      "Iteration 867, loss = 0.00226861\n",
      "Iteration 868, loss = 0.00225422\n",
      "Iteration 869, loss = 0.00224846\n",
      "Iteration 870, loss = 0.00224132\n",
      "Iteration 871, loss = 0.00223605\n",
      "Iteration 872, loss = 0.00222927\n",
      "Iteration 873, loss = 0.00222004\n",
      "Iteration 874, loss = 0.00222315\n",
      "Iteration 875, loss = 0.00220552\n",
      "Iteration 876, loss = 0.00220187\n",
      "Iteration 877, loss = 0.00219750\n",
      "Iteration 878, loss = 0.00218895\n",
      "Iteration 879, loss = 0.00217538\n",
      "Iteration 880, loss = 0.00217289\n",
      "Iteration 881, loss = 0.00216437\n",
      "Iteration 882, loss = 0.00215863\n",
      "Iteration 883, loss = 0.00215796\n",
      "Iteration 884, loss = 0.00214063\n",
      "Iteration 885, loss = 0.00213574\n",
      "Iteration 886, loss = 0.00213381\n",
      "Iteration 887, loss = 0.00212608\n",
      "Iteration 888, loss = 0.00211897\n",
      "Iteration 889, loss = 0.00210980\n",
      "Iteration 890, loss = 0.00210407\n",
      "Iteration 891, loss = 0.00209633\n",
      "Iteration 892, loss = 0.00209247\n",
      "Iteration 893, loss = 0.00208518\n",
      "Iteration 894, loss = 0.00208028\n",
      "Iteration 895, loss = 0.00207297\n",
      "Iteration 896, loss = 0.00206655\n",
      "Iteration 897, loss = 0.00206364\n",
      "Iteration 898, loss = 0.00206031\n",
      "Iteration 899, loss = 0.00204845\n",
      "Iteration 900, loss = 0.00204245\n",
      "Iteration 901, loss = 0.00203665\n",
      "Iteration 902, loss = 0.00203156\n",
      "Iteration 903, loss = 0.00202479\n",
      "Iteration 904, loss = 0.00201874\n",
      "Iteration 905, loss = 0.00201147\n",
      "Iteration 906, loss = 0.00201515\n",
      "Iteration 907, loss = 0.00199764\n",
      "Iteration 908, loss = 0.00199471\n",
      "Iteration 909, loss = 0.00198995\n",
      "Iteration 910, loss = 0.00198026\n",
      "Iteration 911, loss = 0.00197548\n",
      "Iteration 912, loss = 0.00197036\n",
      "Iteration 913, loss = 0.00196487\n",
      "Iteration 914, loss = 0.00195658\n",
      "Iteration 915, loss = 0.00195381\n",
      "Iteration 916, loss = 0.00194871\n",
      "Iteration 917, loss = 0.00194112\n",
      "Iteration 918, loss = 0.00193629\n",
      "Iteration 919, loss = 0.00192911\n",
      "Iteration 920, loss = 0.00192328\n",
      "Iteration 921, loss = 0.00191799\n",
      "Iteration 922, loss = 0.00191365\n",
      "Iteration 923, loss = 0.00190632\n",
      "Iteration 924, loss = 0.00190291\n",
      "Iteration 925, loss = 0.00189919\n",
      "Iteration 926, loss = 0.00189644\n",
      "Iteration 927, loss = 0.00189312\n",
      "Iteration 928, loss = 0.00188150\n",
      "Iteration 929, loss = 0.00187491\n",
      "Iteration 930, loss = 0.00187050\n",
      "Iteration 931, loss = 0.00186708\n",
      "Iteration 932, loss = 0.00185911\n",
      "Iteration 933, loss = 0.00185709\n",
      "Iteration 934, loss = 0.00185218\n",
      "Iteration 935, loss = 0.00184235\n",
      "Iteration 936, loss = 0.00183749\n",
      "Iteration 937, loss = 0.00183561\n",
      "Iteration 938, loss = 0.00182757\n",
      "Iteration 939, loss = 0.00182333\n",
      "Iteration 940, loss = 0.00181762\n",
      "Iteration 941, loss = 0.00181218\n",
      "Iteration 942, loss = 0.00180705\n",
      "Iteration 943, loss = 0.00180606\n",
      "Iteration 944, loss = 0.00180065\n",
      "Iteration 945, loss = 0.00179428\n",
      "Iteration 946, loss = 0.00179171\n",
      "Iteration 947, loss = 0.00178430\n",
      "Iteration 948, loss = 0.00177765\n",
      "Iteration 949, loss = 0.00177334\n",
      "Iteration 950, loss = 0.00177029\n",
      "Iteration 951, loss = 0.00176395\n",
      "Iteration 952, loss = 0.00175783\n",
      "Iteration 953, loss = 0.00175337\n",
      "Iteration 954, loss = 0.00175196\n",
      "Iteration 955, loss = 0.00174505\n",
      "Iteration 956, loss = 0.00174169\n",
      "Iteration 957, loss = 0.00173413\n",
      "Iteration 958, loss = 0.00173086\n",
      "Iteration 959, loss = 0.00172535\n",
      "Iteration 960, loss = 0.00172227\n",
      "Iteration 961, loss = 0.00171786\n",
      "Iteration 962, loss = 0.00171164\n",
      "Iteration 963, loss = 0.00170771\n",
      "Iteration 964, loss = 0.00170296\n",
      "Iteration 965, loss = 0.00169748\n",
      "Iteration 966, loss = 0.00169460\n",
      "Iteration 967, loss = 0.00169222\n",
      "Iteration 968, loss = 0.00168540\n",
      "Iteration 969, loss = 0.00168407\n",
      "Iteration 970, loss = 0.00167814\n",
      "Iteration 971, loss = 0.00167531\n",
      "Iteration 972, loss = 0.00167018\n",
      "Iteration 973, loss = 0.00166157\n",
      "Iteration 974, loss = 0.00165925\n",
      "Iteration 975, loss = 0.00165390\n",
      "Iteration 976, loss = 0.00165168\n",
      "Iteration 977, loss = 0.00164629\n",
      "Iteration 978, loss = 0.00164171\n",
      "Iteration 979, loss = 0.00163724\n",
      "Iteration 980, loss = 0.00163543\n",
      "Iteration 981, loss = 0.00162823\n",
      "Iteration 982, loss = 0.00162643\n",
      "Iteration 983, loss = 0.00162338\n",
      "Iteration 984, loss = 0.00161678\n",
      "Iteration 985, loss = 0.00161284\n",
      "Iteration 986, loss = 0.00160865\n",
      "Iteration 987, loss = 0.00160447\n",
      "Iteration 988, loss = 0.00159980\n",
      "Iteration 989, loss = 0.00159807\n",
      "Iteration 990, loss = 0.00159315\n",
      "Iteration 991, loss = 0.00159281\n",
      "Iteration 992, loss = 0.00158411\n",
      "Iteration 993, loss = 0.00158239\n",
      "Iteration 994, loss = 0.00157702\n",
      "Iteration 995, loss = 0.00157261\n",
      "Iteration 996, loss = 0.00156669\n",
      "Iteration 997, loss = 0.00156400\n",
      "Iteration 998, loss = 0.00156203\n",
      "Iteration 999, loss = 0.00155668\n",
      "Iteration 1000, loss = 0.00155675\n",
      "Attribute -> Pronic\n",
      "Accuracy -> 0.998046875\n"
     ]
    }
   ],
   "source": [
    "filename = \"Data.csv\"\n",
    "data = pd.read_csv(filename)\n",
    "X = data[data.columns[1:]]\n",
    "Y = data[data.columns[-1]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size= 0.25, random_state=27)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=1000, alpha=0.0001,\n",
    "         solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print('Accuracy ->',accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multioutput target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-35c90b2c5566>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    971\u001b[0m         \"\"\"\n\u001b[0;32m    972\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[1;32m--> 973\u001b[1;33m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    329\u001b[0m                              hidden_layer_sizes)\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_type_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'multioutput'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_type_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             raise ValueError(\"Multioutput target data is not supported with \"\n\u001b[0m\u001b[0;32m    279\u001b[0m                              \"label binarization\")\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
